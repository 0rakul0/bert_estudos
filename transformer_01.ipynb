{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:32:30.989965800Z",
     "start_time": "2023-08-04T23:32:26.082913600Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras_preprocessing.sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "from util.limpeza import limpeza\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## base de dados"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "833dbfedbffe79b8"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "with open('./tradutor/europarl-v7.pt-en.en', mode='r', encoding='utf-8') as f:\n",
    "    europarl_en = f.read()\n",
    "\n",
    "with open('./tradutor/europarl-v7.pt-en.pt', mode='r', encoding='utf-8') as f:\n",
    "    europarl_pt = f.read()\n",
    "\n",
    "corpus_en = europarl_en\n",
    "corpus_pt = europarl_pt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:32:34.519394400Z",
     "start_time": "2023-08-04T23:32:30.998964200Z"
    }
   },
   "id": "5718d7a47866c261"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960408\n",
      "1960408\n"
     ]
    }
   ],
   "source": [
    "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
    "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
    "corpus_en = re.sub(r\" +\", ' ', corpus_en)\n",
    "corpus_en = corpus_en.split('\\n')\n",
    "print(len(corpus_en))\n",
    "\n",
    "corpus_pt = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_pt)\n",
    "corpus_pt = re.sub(r\".\\$\\$\\$\", '', corpus_pt)\n",
    "corpus_pt = re.sub(r\" +\", ' ', corpus_pt)\n",
    "corpus_pt = corpus_pt.split('\\n')\n",
    "print(len(corpus_pt))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:33:20.825090500Z",
     "start_time": "2023-08-04T23:32:34.523395Z"
    }
   },
   "id": "f836d9f13b0a06b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## tokenizer "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a3c67a36eed6c94"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#### tokenizer ####\n",
    "# build_tokenize_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_en, target_vocab_size=2**13)\n",
    "# build_tokenize_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_pt, target_vocab_size=2**13)\n",
    "#\n",
    "# build_tokenize_en.save_to_file(\"./vocab_tokenizer_en\")\n",
    "# build_tokenize_pt.save_to_file(\"./vocab_tokenizer_pt\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:33:20.826092Z",
     "start_time": "2023-08-04T23:33:20.789164900Z"
    }
   },
   "id": "5008fd5e3b4eb788"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carregando o vocab\n",
      "vocab carregado\n"
     ]
    }
   ],
   "source": [
    "print(\"carregando o vocab\")\n",
    "\n",
    "tokenize_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file('./vocabs/vocab_tokenizer_en')\n",
    "tokenize_pt = tfds.deprecated.text.SubwordTextEncoder.load_from_file('./vocabs/vocab_tokenizer_pt')\n",
    "\n",
    "print(\"vocab carregado\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:33:21.055384100Z",
     "start_time": "2023-08-04T23:33:20.801681600Z"
    }
   },
   "id": "554c40da9c214b9"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8191, 3014, 55, 159, 233, 75, 107, 497, 7980, 403, 7980, 1744, 260, 12, 120, 2, 87, 11, 137, 8, 9, 7, 858, 193, 3, 16, 457, 7981, 8192]\n",
      "[8116, 25, 475, 459, 498, 75, 1336, 2831, 6, 443, 242, 1, 4, 2615, 7892, 3, 992, 23, 3863, 134, 21, 234, 26, 961, 8, 282, 8, 7227, 3260, 20, 599, 66, 7957, 75, 4638, 419, 2, 28, 1689, 7070, 6, 10, 207, 2, 147, 5, 62, 10, 147, 3, 243, 2383, 1804, 1, 251, 27, 14, 484, 7, 1768, 1929, 2, 1689, 10, 62, 1696, 1, 7958, 75, 484, 7, 1429, 2, 788, 15, 7568, 19, 1551, 34, 5623, 1, 7959, 75, 5373, 2, 2948, 4443, 7892, 26, 317, 15, 711, 5220, 9, 26, 599, 1, 7960, 75, 2731, 2, 28, 7301, 226, 1, 7961, 75, 4026, 5, 7, 457, 2, 2918, 2453, 1965, 23, 733, 8, 7227, 3260, 10, 62, 2021, 200, 743, 5047, 488, 4625, 7957, 7906, 8117]\n"
     ]
    }
   ],
   "source": [
    "vocab_size_en = tokenize_en.vocab_size + 2\n",
    "inputs_en = [[vocab_size_en - 2] + tokenize_en.encode(sentence) + [vocab_size_en - 1] for sentence in corpus_en]\n",
    "print(inputs_en[random.randint(0, len(inputs_en) - 1)])\n",
    "\n",
    "vocab_size_pt = tokenize_pt.vocab_size + 2\n",
    "outputs_pt = [[vocab_size_pt - 2] + tokenize_pt.encode(sentence) + [vocab_size_pt - 1] for sentence in corpus_pt]\n",
    "print(outputs_pt[random.randint(0, len(outputs_pt) - 1)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:36:50.510826400Z",
     "start_time": "2023-08-04T23:33:20.942804600Z"
    }
   },
   "id": "7bb477517089a15c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## etapa de melhora do processamento"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33c4591c5184da6e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etapa de melhora do processamento\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "o Objetivo Ã© tirar frases com mais de 15 palavras\n",
    "\"\"\"\n",
    "\n",
    "max_length = 15\n",
    "idx_to_remove = [count for count, sent in enumerate(inputs_en) if len(sent) > max_length]\n",
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs_en[idx]\n",
    "    del outputs_pt[idx]\n",
    "\n",
    "idx_to_remove = [count for count, sent in enumerate(outputs_pt) if len(sent) > max_length]\n",
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs_en[idx]\n",
    "    del outputs_pt[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:40:15.618518400Z",
     "start_time": "2023-08-04T23:36:50.507829900Z"
    }
   },
   "id": "ec7dc30c49908921"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanhos das entradas e saidas\n",
      "208990\n",
      "208990\n"
     ]
    }
   ],
   "source": [
    "#### tamanho total\n",
    "print(\"tamanhos das entradas e saidas\")\n",
    "print(len(inputs_en))\n",
    "print(len(outputs_pt))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:40:15.638516800Z",
     "start_time": "2023-08-04T23:40:15.621519900Z"
    }
   },
   "id": "a8da197dd66ad15d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## padding "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23ed87bd4d48ca47"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8191   11  157    4  348   66  968   18  270   16 4663 7981 8192    0\n",
      "    0]\n",
      "[8116 7179   19  182 1566 7969 7905   34 1907 7906 8117    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "inputs_en = keras_preprocessing.sequence.pad_sequences(inputs_en, value=0, padding='post', maxlen=max_length)\n",
    "outputs_pt = keras_preprocessing.sequence.pad_sequences(outputs_pt, value=0, padding='post', maxlen=max_length)\n",
    "\n",
    "print(inputs_en[random.randint(0, len(inputs_en) - 1)])\n",
    "print(outputs_pt[random.randint(0, len(outputs_pt) - 1)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:40:16.879547300Z",
     "start_time": "2023-08-04T23:40:15.676520600Z"
    }
   },
   "id": "6a5f5c4bde739bdb"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 20000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:40:16.938256600Z",
     "start_time": "2023-08-04T23:40:16.880547700Z"
    }
   },
   "id": "9e6f81c7dc39ceda"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 15), dtype=tf.int32, name=None), TensorSpec(shape=(None, 15), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((inputs_en, outputs_pt))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T23:40:37.659506700Z",
     "start_time": "2023-08-04T23:40:37.540021500Z"
    }
   },
   "id": "b71e26cc51ce8a14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "64566bd1853e4fce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
