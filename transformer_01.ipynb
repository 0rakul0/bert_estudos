{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:10:47.853748600Z",
     "start_time": "2023-08-06T02:10:47.697745300Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import zipfile\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## base de dados"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "833dbfedbffe79b8"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "with open('./tradutor/europarl-v7.pt-en.en', mode='r', encoding='utf-8') as f:\n",
    "    europarl_en = f.read()\n",
    "\n",
    "with open('./tradutor/europarl-v7.pt-en.pt', mode='r', encoding='utf-8') as f:\n",
    "    europarl_pt = f.read()\n",
    "\n",
    "corpus_en = europarl_en\n",
    "corpus_pt = europarl_pt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:10:53.833577300Z",
     "start_time": "2023-08-06T02:10:47.714756300Z"
    }
   },
   "id": "5718d7a47866c261"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1960408\n",
      "1960408\n"
     ]
    }
   ],
   "source": [
    "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
    "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
    "corpus_en = re.sub(r\" +\", ' ', corpus_en)\n",
    "corpus_en = corpus_en.split('\\n')\n",
    "print(len(corpus_en))\n",
    "\n",
    "corpus_pt = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_pt)\n",
    "corpus_pt = re.sub(r\".\\$\\$\\$\", '', corpus_pt)\n",
    "corpus_pt = re.sub(r\" +\", ' ', corpus_pt)\n",
    "corpus_pt = corpus_pt.split('\\n')\n",
    "print(len(corpus_pt))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:11:41.511773600Z",
     "start_time": "2023-08-06T02:10:53.836576700Z"
    }
   },
   "id": "f836d9f13b0a06b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## tokenizer "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a3c67a36eed6c94"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "#### tokenizer ####\n",
    "build_tokenize_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_en, target_vocab_size=2**13)\n",
    "build_tokenize_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_pt, target_vocab_size=2**13)\n",
    "\n",
    "build_tokenize_en.save_to_file(\"./vocabs/vocab_tokenizer_en\")\n",
    "build_tokenize_pt.save_to_file(\"./vocabs/vocab_tokenizer_pt\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:22:19.208110100Z",
     "start_time": "2023-08-06T02:11:41.536379500Z"
    }
   },
   "id": "5008fd5e3b4eb788"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carregando o vocab\n",
      "vocab carregado\n"
     ]
    }
   ],
   "source": [
    "print(\"carregando o vocab\")\n",
    "\n",
    "tokenize_en = tfds.deprecated.text.SubwordTextEncoder.load_from_file('./vocabs/vocab_tokenizer_en')\n",
    "tokenize_pt = tfds.deprecated.text.SubwordTextEncoder.load_from_file('./vocabs/vocab_tokenizer_pt')\n",
    "\n",
    "print(\"vocab carregado\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:22:24.447694600Z",
     "start_time": "2023-08-06T02:22:19.211108800Z"
    }
   },
   "id": "554c40da9c214b9"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8191, 133, 9, 19, 226, 87, 60, 26, 14, 23, 1450, 7981, 8192]\n",
      "[8116, 30, 46, 1263, 3283, 7905, 163, 7905, 5142, 1, 1143, 1, 6, 1263, 687, 464, 7906, 8117]\n"
     ]
    }
   ],
   "source": [
    "vocab_size_en = tokenize_en.vocab_size + 2\n",
    "inputs_en = [[vocab_size_en - 2] + tokenize_en.encode(sentence) + [vocab_size_en - 1] for sentence in corpus_en]\n",
    "print(inputs_en[random.randint(0, len(inputs_en) - 1)])\n",
    "\n",
    "vocab_size_pt = tokenize_pt.vocab_size + 2\n",
    "outputs_pt = [[vocab_size_pt - 2] + tokenize_pt.encode(sentence) + [vocab_size_pt - 1] for sentence in corpus_pt]\n",
    "print(outputs_pt[random.randint(0, len(outputs_pt) - 1)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:25:57.844960700Z",
     "start_time": "2023-08-06T02:22:24.540697800Z"
    }
   },
   "id": "7bb477517089a15c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## etapa de melhora do processamento"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33c4591c5184da6e"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "o Objetivo é tirar frases com mais de 15 palavras\n",
    "\n",
    "mas para o treinamento total melhor é por mais\n",
    "\"\"\"\n",
    "\n",
    "max_length = 40\n",
    "idx_to_remove = [count for count, sent in enumerate(inputs_en) if len(sent) > max_length]\n",
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs_en[idx]\n",
    "    del outputs_pt[idx]\n",
    "\n",
    "idx_to_remove = [count for count, sent in enumerate(outputs_pt) if len(sent) > max_length]\n",
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs_en[idx]\n",
    "    del outputs_pt[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:51.349200300Z",
     "start_time": "2023-08-06T02:25:57.847961600Z"
    }
   },
   "id": "ec7dc30c49908921"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanhos das entradas e saidas\n",
      "1239460\n",
      "1239460\n"
     ]
    }
   ],
   "source": [
    "#### tamanho total\n",
    "print(\"tamanhos das entradas e saidas\")\n",
    "print(len(inputs_en))\n",
    "print(len(outputs_pt))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:51.364221800Z",
     "start_time": "2023-08-06T02:36:51.350199900Z"
    }
   },
   "id": "a8da197dd66ad15d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## padding "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23ed87bd4d48ca47"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8191, 61, 1983, 107, 118, 753, 7981, 8192]\n",
      "[8116, 6809, 515, 134, 145, 14, 247, 2481, 66, 214, 947, 24, 4762, 2489, 7906, 8117]\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs_en, value=0, padding='post', maxlen=max_length)\n",
    "outputs= tf.keras.preprocessing.sequence.pad_sequences(outputs_pt, value=0, padding='post', maxlen=max_length)\n",
    "\n",
    "print(inputs_en[random.randint(0, len(inputs_en) - 1)])\n",
    "print(outputs_pt[random.randint(0, len(outputs_pt) - 1)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.317651800Z",
     "start_time": "2023-08-06T02:36:51.379140200Z"
    }
   },
   "id": "6a5f5c4bde739bdb"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 20000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.363663500Z",
     "start_time": "2023-08-06T02:36:59.319652200Z"
    }
   },
   "id": "9e6f81c7dc39ceda"
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.474664900Z",
     "start_time": "2023-08-06T02:36:59.378666700Z"
    }
   },
   "id": "b71e26cc51ce8a14"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# contruindo o modelo\n",
    "\n",
    "## Positional encoding:\n",
    "\n",
    "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
    "<br>\n",
    "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45864680d6564483"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angles = 1 / np.power(10000., (2*(i // 2)) / np.float32(d_model))\n",
    "        return pos * angles # (seq_lenght, d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_lenght = inputs.shape.as_list()[-2]\n",
    "        d_model = inputs.shape.as_list()[-1]\n",
    "        angles = self.get_angles(np.arange(seq_lenght)[:, np.newaxis],\n",
    "                                 np.arange(d_model)[np.newaxis, :], d_model)\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "        pos_encoding = angles[np.newaxis, ...]\n",
    "        return inputs + tf.cast(pos_encoding, tf.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.491666600Z",
     "start_time": "2023-08-06T02:36:59.483666Z"
    }
   },
   "id": "466fdcf26fda9fd8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "scaled"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff0b97f0cf70065c"
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    "    product = tf.matmul(queries, keys, transpose_b=True)\n",
    "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_product += (mask * -1e9) # 0.0000000001\n",
    "\n",
    "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
    "    return attention"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.523667700Z",
     "start_time": "2023-08-06T02:36:59.494666400Z"
    }
   },
   "id": "51f872ed9f0b0f67"
  },
  {
   "cell_type": "markdown",
   "source": [
    "MultiHeadAttention"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22d6127e114d53dc"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "\n",
    "    def __init__(self, nb_proj):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.nb_proj = nb_proj\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.nb_proj == 0\n",
    "\n",
    "        self.d_proj = self.d_model // self.nb_proj\n",
    "\n",
    "        self.query_lin = layers.Dense(units = self.d_model)\n",
    "        self.key_lin = layers.Dense(units = self.d_model)\n",
    "        self.value_lin = layers.Dense(units = self.d_model)\n",
    "\n",
    "        self.final_lin = layers.Dense(units = self.d_model)\n",
    "\n",
    "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_lenght, d_model)\n",
    "        shape = (batch_size, -1, self.nb_proj, self.d_proj)\n",
    "        splited_inputs = tf.reshape(inputs, shape = shape) # (batch_size, seq_lenght, nb_proj, d_proj)\n",
    "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_lenght, d_proj)\n",
    "\n",
    "    def call(self, queries, keys, values, mask):\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "\n",
    "        queries = self.query_lin(queries)\n",
    "        keys = self.key_lin(keys)\n",
    "        values = self.value_lin(values)\n",
    "\n",
    "        queries = self.split_proj(queries, batch_size)\n",
    "        keys = self.split_proj(keys, batch_size)\n",
    "        values = self.split_proj(values, batch_size)\n",
    "\n",
    "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
    "\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = tf.reshape(attention, shape=(batch_size, -1, self.d_model))\n",
    "\n",
    "        outputs = self.final_lin(concat_attention)\n",
    "\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.523667700Z",
     "start_time": "2023-08-06T02:36:59.514668Z"
    }
   },
   "id": "2c3e95a4bb292096"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Encoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3d2453a19a02649"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6) # 0.0000001\n",
    "\n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units, activation='relu')\n",
    "        self.dense_2 = layers.Dense(units=self.d_model, activation='relu')\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, mask, training):\n",
    "        attention = self.multi_head_attention(inputs, inputs, inputs, mask)\n",
    "        attention = self.dropout_1(attention, training = training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "\n",
    "        outputs = self.dense_1(attention)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_2(outputs, training=training)\n",
    "        outputs = self.norm_2(outputs + attention)\n",
    "\n",
    "        return outputs\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.541664800Z",
     "start_time": "2023-08-06T02:36:59.529665200Z"
    }
   },
   "id": "766b135c645216bf"
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"encoder\"):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "        self.nb_layers = nb_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        self.enc_layers = [EncoderLayer(FFN_units, nb_proj, dropout_rate) for _ in range(nb_layers)]\n",
    "\n",
    "\n",
    "    def call(self, inputs, mask, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "\n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.enc_layers[i](outputs, mask, training)\n",
    "\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.584663900Z",
     "start_time": "2023-08-06T02:36:59.544666100Z"
    }
   },
   "id": "ab1fc856ad735ccb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "decoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0ab351846fe6a6"
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "\n",
    "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dense_1 = layers.Dense(units = self.FFN_units, activation='relu')\n",
    "        self.dense_2 = layers.Dense(units = self.d_model, activation='relu')\n",
    "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        attention = self.multi_head_attention_1(inputs, inputs, inputs, mask_1)\n",
    "        attention = self.dropout_1(attention, training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "\n",
    "        attention_2 = self.multi_head_attention_2(attention, enc_outputs, enc_outputs, mask_2)\n",
    "        attention_2 = self.dropout_2(attention_2, training)\n",
    "        attention_2 = self.norm_2(attention_2 + attention)\n",
    "\n",
    "        outputs = self.dense_1(attention_2)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_3(outputs, training)\n",
    "        outputs = self.norm_3(outputs + attention_2)\n",
    "\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.585666900Z",
     "start_time": "2023-08-06T02:36:59.561666700Z"
    }
   },
   "id": "cf8b8ef8c444d549"
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"decoder\"):\n",
    "        super(Decoder, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.nb_layers = nb_layers\n",
    "\n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(FFN_units, nb_proj, dropout_rate) for i in range(nb_layers)]\n",
    "\n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "\n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.dec_layers[i](outputs, enc_outputs, mask_1, mask_2, training)\n",
    "\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.587666300Z",
     "start_time": "2023-08-06T02:36:59.575667200Z"
    }
   },
   "id": "6eb9887093846800"
  },
  {
   "cell_type": "markdown",
   "source": [
    "transforme"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "947afb2097ff1ac0"
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size_enc,\n",
    "                 vocab_size_dec,\n",
    "                 d_model,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 name=\"transformer\"):\n",
    "        super(Transformer, self).__init__(name=name)\n",
    "\n",
    "        self.encoder = Encoder(nb_layers, FFN_units, nb_proj, dropout_rate,\n",
    "                               vocab_size_enc, d_model)\n",
    "        self.decoder = Decoder(nb_layers, FFN_units, nb_proj, dropout_rate,\n",
    "                               vocab_size_dec, d_model)\n",
    "        self.last_linear = layers.Dense(units=vocab_size_dec, name='lin_output')\n",
    "\n",
    "    def create_padding_mask(self, seq): # (batch_size, seq_length) -> (batch_size, nb_proj, seq_lenght, d_proj)\n",
    "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    def create_look_ahead_mask(self, seq):\n",
    "        seq_len = tf.shape(seq)[1]\n",
    "        look_ahed_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return look_ahed_mask\n",
    "\n",
    "    def call(self, enc_inputs, dec_inputs, training):\n",
    "        enc_mask = self.create_padding_mask(enc_inputs)\n",
    "        dec_mask_1 = tf.maximum(self.create_padding_mask(dec_inputs), self.create_look_ahead_mask(dec_inputs))\n",
    "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
    "\n",
    "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
    "        dec_outputs = self.decoder(dec_inputs, enc_outputs, dec_mask_1, dec_mask_2, training)\n",
    "\n",
    "        outputs = self.last_linear(dec_outputs)\n",
    "\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.602664600Z",
     "start_time": "2023-08-06T02:36:59.593665200Z"
    }
   },
   "id": "96194b928d26fc7d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "restart daqui"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd2aabba96531d62"
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "d_model = 512 # 512\n",
    "nb_layers = 6 # 6\n",
    "ffn_units = 1024 # 2048\n",
    "nb_proj = 8 # 8\n",
    "dropout_rate = 0.1 # 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.651697200Z",
     "start_time": "2023-08-06T02:36:59.606665400Z"
    }
   },
   "id": "3cc92339c239bfe3"
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "transformer = Transformer(vocab_size_enc=vocab_size_en,\n",
    "                          vocab_size_dec=vocab_size_pt,\n",
    "                          d_model=d_model,\n",
    "                          nb_layers=nb_layers,\n",
    "                          FFN_units=ffn_units,\n",
    "                          nb_proj=nb_proj,\n",
    "                          dropout_rate=dropout_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.697650800Z",
     "start_time": "2023-08-06T02:36:59.652696700Z"
    }
   },
   "id": "391a259e6ea6ac52"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction='none')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.714650800Z",
     "start_time": "2023-08-06T02:36:59.699648800Z"
    }
   },
   "id": "cf5cb71c82bb1625"
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "def loss_function(target, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "    loss_ = loss_object(target, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.742649700Z",
     "start_time": "2023-08-06T02:36:59.717650200Z"
    }
   },
   "id": "70b8842460ecb763"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.787200Z",
     "start_time": "2023-08-06T02:36:59.739652800Z"
    }
   },
   "id": "8f27013e996523a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "customSchedule"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1231ca90f7c0cd79"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.792204600Z",
     "start_time": "2023-08-06T02:36:59.779203200Z"
    }
   },
   "id": "8cdf47ebcbae038e"
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "learing_rate = CustomSchedule(d_model=d_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.807864300Z",
     "start_time": "2023-08-06T02:36:59.793205100Z"
    }
   },
   "id": "f5173872b53af919"
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learing_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.845040Z",
     "start_time": "2023-08-06T02:36:59.809866300Z"
    }
   },
   "id": "7317d2e249c73146"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "checkpoint = './tradutor_chek/'\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optmizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint, max_to_keep=5)\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('checkpoint restalrado')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.846038100Z",
     "start_time": "2023-08-06T02:36:59.826026500Z"
    }
   },
   "id": "b6045215f15908e1"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "epochs = 40"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-06T02:36:59.855820Z",
     "start_time": "2023-08-06T02:36:59.841042700Z"
    }
   },
   "id": "3bd92967ec93f04c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start or epoch 1\n",
      "Epoch 1 Batch 0 Loss 5.3673 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 5.2041 Accuracy 0.0137\n",
      "Epoch 1 Batch 100 Loss 5.0733 Accuracy 0.0219\n",
      "Epoch 1 Batch 150 Loss 4.9641 Accuracy 0.0259\n",
      "Epoch 1 Batch 200 Loss 4.8503 Accuracy 0.0307\n",
      "Epoch 1 Batch 250 Loss 4.7242 Accuracy 0.0364\n",
      "Epoch 1 Batch 300 Loss 4.6118 Accuracy 0.0417\n",
      "Epoch 1 Batch 350 Loss 4.5139 Accuracy 0.0458\n",
      "Epoch 1 Batch 400 Loss 4.4456 Accuracy 0.0491\n",
      "Epoch 1 Batch 450 Loss 4.3876 Accuracy 0.0520\n",
      "Epoch 1 Batch 500 Loss 4.3320 Accuracy 0.0551\n",
      "Epoch 1 Batch 550 Loss 4.2755 Accuracy 0.0583\n",
      "Epoch 1 Batch 600 Loss 4.2259 Accuracy 0.0616\n",
      "Epoch 1 Batch 650 Loss 4.1789 Accuracy 0.0649\n",
      "Epoch 1 Batch 700 Loss 4.1324 Accuracy 0.0680\n",
      "Epoch 1 Batch 750 Loss 4.0911 Accuracy 0.0710\n",
      "Epoch 1 Batch 800 Loss 4.0478 Accuracy 0.0738\n",
      "Epoch 1 Batch 850 Loss 4.0064 Accuracy 0.0763\n",
      "Epoch 1 Batch 900 Loss 3.9663 Accuracy 0.0787\n",
      "Epoch 1 Batch 950 Loss 3.9249 Accuracy 0.0810\n",
      "Epoch 1 Batch 1000 Loss 3.8888 Accuracy 0.0833\n",
      "Epoch 1 Batch 1050 Loss 3.8540 Accuracy 0.0854\n",
      "Epoch 1 Batch 1100 Loss 3.8198 Accuracy 0.0874\n",
      "Epoch 1 Batch 1150 Loss 3.7862 Accuracy 0.0893\n",
      "Epoch 1 Batch 1200 Loss 3.7541 Accuracy 0.0912\n",
      "Epoch 1 Batch 1250 Loss 3.7235 Accuracy 0.0930\n",
      "Epoch 1 Batch 1300 Loss 3.6945 Accuracy 0.0948\n",
      "Epoch 1 Batch 1350 Loss 3.6662 Accuracy 0.0966\n",
      "Epoch 1 Batch 1400 Loss 3.6394 Accuracy 0.0983\n",
      "Epoch 1 Batch 1450 Loss 3.6125 Accuracy 0.1000\n",
      "Epoch 1 Batch 1500 Loss 3.5863 Accuracy 0.1016\n",
      "Epoch 1 Batch 1550 Loss 3.5609 Accuracy 0.1031\n",
      "Epoch 1 Batch 1600 Loss 3.5384 Accuracy 0.1047\n",
      "Epoch 1 Batch 1650 Loss 3.5134 Accuracy 0.1062\n",
      "Epoch 1 Batch 1700 Loss 3.4912 Accuracy 0.1079\n",
      "Epoch 1 Batch 1750 Loss 3.4695 Accuracy 0.1094\n",
      "Epoch 1 Batch 1800 Loss 3.4477 Accuracy 0.1109\n",
      "Epoch 1 Batch 1850 Loss 3.4254 Accuracy 0.1123\n",
      "Epoch 1 Batch 1900 Loss 3.4043 Accuracy 0.1137\n",
      "Epoch 1 Batch 1950 Loss 3.3831 Accuracy 0.1150\n",
      "Epoch 1 Batch 2000 Loss 3.3644 Accuracy 0.1164\n",
      "Epoch 1 Batch 2050 Loss 3.3455 Accuracy 0.1177\n",
      "Epoch 1 Batch 2100 Loss 3.3271 Accuracy 0.1189\n",
      "Epoch 1 Batch 2150 Loss 3.3089 Accuracy 0.1202\n",
      "Epoch 1 Batch 2200 Loss 3.2921 Accuracy 0.1214\n",
      "Epoch 1 Batch 2250 Loss 3.2751 Accuracy 0.1226\n",
      "Epoch 1 Batch 2300 Loss 3.2592 Accuracy 0.1238\n",
      "Epoch 1 Batch 2350 Loss 3.2430 Accuracy 0.1250\n",
      "Epoch 1 Batch 2400 Loss 3.2277 Accuracy 0.1261\n",
      "Epoch 1 Batch 2450 Loss 3.2123 Accuracy 0.1273\n",
      "Epoch 1 Batch 2500 Loss 3.1982 Accuracy 0.1284\n",
      "Epoch 1 Batch 2550 Loss 3.1830 Accuracy 0.1295\n",
      "Epoch 1 Batch 2600 Loss 3.1686 Accuracy 0.1306\n",
      "Epoch 1 Batch 2650 Loss 3.1541 Accuracy 0.1317\n",
      "Epoch 1 Batch 2700 Loss 3.1411 Accuracy 0.1328\n",
      "Epoch 1 Batch 2750 Loss 3.1278 Accuracy 0.1338\n",
      "Epoch 1 Batch 2800 Loss 3.1146 Accuracy 0.1349\n",
      "Epoch 1 Batch 2850 Loss 3.1017 Accuracy 0.1360\n",
      "Epoch 1 Batch 2900 Loss 3.0885 Accuracy 0.1371\n",
      "Epoch 1 Batch 2950 Loss 3.0750 Accuracy 0.1380\n",
      "Epoch 1 Batch 3000 Loss 3.0620 Accuracy 0.1391\n",
      "Epoch 1 Batch 3050 Loss 3.0493 Accuracy 0.1401\n",
      "Epoch 1 Batch 3100 Loss 3.0368 Accuracy 0.1410\n",
      "Epoch 1 Batch 3150 Loss 3.0252 Accuracy 0.1420\n",
      "Epoch 1 Batch 3200 Loss 3.0134 Accuracy 0.1429\n",
      "Epoch 1 Batch 3250 Loss 3.0014 Accuracy 0.1438\n",
      "Epoch 1 Batch 3300 Loss 2.9897 Accuracy 0.1448\n",
      "Epoch 1 Batch 3350 Loss 2.9782 Accuracy 0.1456\n",
      "Epoch 1 Batch 3400 Loss 2.9668 Accuracy 0.1465\n",
      "Epoch 1 Batch 3450 Loss 2.9559 Accuracy 0.1473\n",
      "Epoch 1 Batch 3500 Loss 2.9450 Accuracy 0.1482\n",
      "Epoch 1 Batch 3550 Loss 2.9340 Accuracy 0.1490\n",
      "Epoch 1 Batch 3600 Loss 2.9236 Accuracy 0.1499\n",
      "Epoch 1 Batch 3650 Loss 2.9127 Accuracy 0.1507\n",
      "Epoch 1 Batch 3700 Loss 2.9022 Accuracy 0.1514\n",
      "Epoch 1 Batch 3750 Loss 2.8925 Accuracy 0.1522\n",
      "Epoch 1 Batch 3800 Loss 2.8830 Accuracy 0.1530\n",
      "Epoch 1 Batch 3850 Loss 2.8737 Accuracy 0.1538\n",
      "Epoch 1 Batch 3900 Loss 2.8645 Accuracy 0.1545\n",
      "Epoch 1 Batch 3950 Loss 2.8555 Accuracy 0.1553\n",
      "Epoch 1 Batch 4000 Loss 2.8466 Accuracy 0.1560\n",
      "Epoch 1 Batch 4050 Loss 2.8385 Accuracy 0.1567\n",
      "Epoch 1 Batch 4100 Loss 2.8305 Accuracy 0.1574\n",
      "Epoch 1 Batch 4150 Loss 2.8223 Accuracy 0.1582\n",
      "Epoch 1 Batch 4200 Loss 2.8142 Accuracy 0.1589\n",
      "Epoch 1 Batch 4250 Loss 2.8064 Accuracy 0.1596\n",
      "Epoch 1 Batch 4300 Loss 2.7983 Accuracy 0.1603\n",
      "Epoch 1 Batch 4350 Loss 2.7907 Accuracy 0.1611\n",
      "Epoch 1 Batch 4400 Loss 2.7831 Accuracy 0.1618\n",
      "Epoch 1 Batch 4450 Loss 2.7756 Accuracy 0.1625\n",
      "Epoch 1 Batch 4500 Loss 2.7682 Accuracy 0.1632\n",
      "Epoch 1 Batch 4550 Loss 2.7608 Accuracy 0.1639\n",
      "Epoch 1 Batch 4600 Loss 2.7535 Accuracy 0.1646\n",
      "Epoch 1 Batch 4650 Loss 2.7463 Accuracy 0.1652\n",
      "Epoch 1 Batch 4700 Loss 2.7386 Accuracy 0.1659\n",
      "Epoch 1 Batch 4750 Loss 2.7313 Accuracy 0.1665\n",
      "Epoch 1 Batch 4800 Loss 2.7241 Accuracy 0.1671\n",
      "Epoch 1 Batch 4850 Loss 2.7171 Accuracy 0.1678\n",
      "Epoch 1 Batch 4900 Loss 2.7106 Accuracy 0.1684\n",
      "Epoch 1 Batch 4950 Loss 2.7038 Accuracy 0.1690\n",
      "Epoch 1 Batch 5000 Loss 2.6970 Accuracy 0.1696\n",
      "Epoch 1 Batch 5050 Loss 2.6905 Accuracy 0.1703\n",
      "Epoch 1 Batch 5100 Loss 2.6842 Accuracy 0.1709\n",
      "Epoch 1 Batch 5150 Loss 2.6776 Accuracy 0.1715\n",
      "Epoch 1 Batch 5200 Loss 2.6715 Accuracy 0.1722\n",
      "Epoch 1 Batch 5250 Loss 2.6654 Accuracy 0.1728\n",
      "Epoch 1 Batch 5300 Loss 2.6592 Accuracy 0.1734\n",
      "Epoch 1 Batch 5350 Loss 2.6532 Accuracy 0.1740\n",
      "Epoch 1 Batch 5400 Loss 2.6470 Accuracy 0.1746\n",
      "Epoch 1 Batch 5450 Loss 2.6409 Accuracy 0.1752\n",
      "Epoch 1 Batch 5500 Loss 2.6351 Accuracy 0.1758\n",
      "Epoch 1 Batch 5550 Loss 2.6292 Accuracy 0.1764\n",
      "Epoch 1 Batch 5600 Loss 2.6234 Accuracy 0.1770\n",
      "Epoch 1 Batch 5650 Loss 2.6180 Accuracy 0.1777\n",
      "Epoch 1 Batch 5700 Loss 2.6124 Accuracy 0.1783\n",
      "Epoch 1 Batch 5750 Loss 2.6069 Accuracy 0.1788\n",
      "Epoch 1 Batch 5800 Loss 2.6011 Accuracy 0.1794\n",
      "Epoch 1 Batch 5850 Loss 2.5955 Accuracy 0.1799\n",
      "Epoch 1 Batch 5900 Loss 2.5901 Accuracy 0.1805\n",
      "Epoch 1 Batch 5950 Loss 2.5849 Accuracy 0.1811\n",
      "Epoch 1 Batch 6000 Loss 2.5798 Accuracy 0.1816\n",
      "Epoch 1 Batch 6050 Loss 2.5747 Accuracy 0.1822\n",
      "Epoch 1 Batch 6100 Loss 2.5697 Accuracy 0.1827\n",
      "Epoch 1 Batch 6150 Loss 2.5647 Accuracy 0.1832\n",
      "Epoch 1 Batch 6200 Loss 2.5596 Accuracy 0.1837\n",
      "Epoch 1 Batch 6250 Loss 2.5550 Accuracy 0.1842\n",
      "Epoch 1 Batch 6300 Loss 2.5502 Accuracy 0.1848\n",
      "Epoch 1 Batch 6350 Loss 2.5453 Accuracy 0.1853\n",
      "Epoch 1 Batch 6400 Loss 2.5404 Accuracy 0.1858\n",
      "Epoch 1 Batch 6450 Loss 2.5357 Accuracy 0.1863\n",
      "Epoch 1 Batch 6500 Loss 2.5309 Accuracy 0.1868\n",
      "Epoch 1 Batch 6550 Loss 2.5264 Accuracy 0.1873\n",
      "Epoch 1 Batch 6600 Loss 2.5217 Accuracy 0.1878\n",
      "Epoch 1 Batch 6650 Loss 2.5170 Accuracy 0.1882\n",
      "Epoch 1 Batch 6700 Loss 2.5124 Accuracy 0.1887\n",
      "Epoch 1 Batch 6750 Loss 2.5079 Accuracy 0.1891\n",
      "Epoch 1 Batch 6800 Loss 2.5035 Accuracy 0.1896\n",
      "Epoch 1 Batch 6850 Loss 2.4993 Accuracy 0.1901\n",
      "Epoch 1 Batch 6900 Loss 2.4950 Accuracy 0.1905\n",
      "Epoch 1 Batch 6950 Loss 2.4905 Accuracy 0.1910\n",
      "Epoch 1 Batch 7000 Loss 2.4862 Accuracy 0.1915\n",
      "Epoch 1 Batch 7050 Loss 2.4817 Accuracy 0.1919\n",
      "Epoch 1 Batch 7100 Loss 2.4773 Accuracy 0.1924\n",
      "Epoch 1 Batch 7150 Loss 2.4731 Accuracy 0.1928\n",
      "Epoch 1 Batch 7200 Loss 2.4687 Accuracy 0.1932\n",
      "Epoch 1 Batch 7250 Loss 2.4645 Accuracy 0.1937\n",
      "Epoch 1 Batch 7300 Loss 2.4604 Accuracy 0.1941\n",
      "Epoch 1 Batch 7350 Loss 2.4561 Accuracy 0.1945\n",
      "Epoch 1 Batch 7400 Loss 2.4521 Accuracy 0.1949\n",
      "Epoch 1 Batch 7450 Loss 2.4482 Accuracy 0.1953\n",
      "Epoch 1 Batch 7500 Loss 2.4442 Accuracy 0.1957\n",
      "Epoch 1 Batch 7550 Loss 2.4401 Accuracy 0.1961\n",
      "Epoch 1 Batch 7600 Loss 2.4358 Accuracy 0.1965\n",
      "Epoch 1 Batch 7650 Loss 2.4318 Accuracy 0.1969\n",
      "Epoch 1 Batch 7700 Loss 2.4276 Accuracy 0.1973\n",
      "Epoch 1 Batch 7750 Loss 2.4233 Accuracy 0.1977\n",
      "Epoch 1 Batch 7800 Loss 2.4193 Accuracy 0.1980\n",
      "Epoch 1 Batch 7850 Loss 2.4150 Accuracy 0.1984\n",
      "Epoch 1 Batch 7900 Loss 2.4108 Accuracy 0.1987\n",
      "Epoch 1 Batch 7950 Loss 2.4068 Accuracy 0.1991\n",
      "Epoch 1 Batch 8000 Loss 2.4027 Accuracy 0.1995\n",
      "Epoch 1 Batch 8050 Loss 2.3986 Accuracy 0.1998\n",
      "Epoch 1 Batch 8100 Loss 2.3945 Accuracy 0.2001\n",
      "Epoch 1 Batch 8150 Loss 2.3906 Accuracy 0.2005\n",
      "Epoch 1 Batch 8200 Loss 2.3867 Accuracy 0.2008\n",
      "Epoch 1 Batch 8250 Loss 2.3828 Accuracy 0.2012\n",
      "Epoch 1 Batch 8300 Loss 2.3788 Accuracy 0.2015\n",
      "Epoch 1 Batch 8350 Loss 2.3752 Accuracy 0.2018\n",
      "Epoch 1 Batch 8400 Loss 2.3717 Accuracy 0.2022\n",
      "Epoch 1 Batch 8450 Loss 2.3680 Accuracy 0.2025\n",
      "Epoch 1 Batch 8500 Loss 2.3644 Accuracy 0.2028\n",
      "Epoch 1 Batch 8550 Loss 2.3608 Accuracy 0.2031\n",
      "Epoch 1 Batch 8600 Loss 2.3573 Accuracy 0.2035\n",
      "Epoch 1 Batch 8650 Loss 2.3538 Accuracy 0.2038\n",
      "Epoch 1 Batch 8700 Loss 2.3504 Accuracy 0.2041\n",
      "Epoch 1 Batch 8750 Loss 2.3473 Accuracy 0.2045\n",
      "Epoch 1 Batch 8800 Loss 2.3438 Accuracy 0.2048\n",
      "Epoch 1 Batch 8850 Loss 2.3402 Accuracy 0.2051\n",
      "Epoch 1 Batch 8900 Loss 2.3369 Accuracy 0.2054\n",
      "Epoch 1 Batch 8950 Loss 2.3335 Accuracy 0.2058\n",
      "Epoch 1 Batch 9000 Loss 2.3303 Accuracy 0.2061\n",
      "Epoch 1 Batch 9050 Loss 2.3271 Accuracy 0.2064\n",
      "Epoch 1 Batch 9100 Loss 2.3240 Accuracy 0.2067\n",
      "Epoch 1 Batch 9150 Loss 2.3207 Accuracy 0.2070\n",
      "Epoch 1 Batch 9200 Loss 2.3173 Accuracy 0.2074\n",
      "Epoch 1 Batch 9250 Loss 2.3141 Accuracy 0.2077\n",
      "Epoch 1 Batch 9300 Loss 2.3108 Accuracy 0.2080\n",
      "Epoch 1 Batch 9350 Loss 2.3077 Accuracy 0.2083\n",
      "Epoch 1 Batch 9400 Loss 2.3045 Accuracy 0.2086\n",
      "Epoch 1 Batch 9450 Loss 2.3014 Accuracy 0.2089\n",
      "Epoch 1 Batch 9500 Loss 2.2985 Accuracy 0.2093\n",
      "Epoch 1 Batch 9550 Loss 2.2955 Accuracy 0.2096\n",
      "Epoch 1 Batch 9600 Loss 2.2926 Accuracy 0.2099\n",
      "Epoch 1 Batch 9650 Loss 2.2896 Accuracy 0.2102\n",
      "Epoch 1 Batch 9700 Loss 2.2867 Accuracy 0.2105\n",
      "Epoch 1 Batch 9750 Loss 2.2838 Accuracy 0.2108\n",
      "Epoch 1 Batch 9800 Loss 2.2808 Accuracy 0.2111\n",
      "Epoch 1 Batch 9850 Loss 2.2779 Accuracy 0.2115\n",
      "Epoch 1 Batch 9900 Loss 2.2750 Accuracy 0.2118\n",
      "Epoch 1 Batch 9950 Loss 2.2721 Accuracy 0.2121\n",
      "Epoch 1 Batch 10000 Loss 2.2693 Accuracy 0.2124\n",
      "Epoch 1 Batch 10050 Loss 2.2664 Accuracy 0.2126\n",
      "Epoch 1 Batch 10100 Loss 2.2635 Accuracy 0.2129\n",
      "Epoch 1 Batch 10150 Loss 2.2607 Accuracy 0.2132\n",
      "Epoch 1 Batch 10200 Loss 2.2578 Accuracy 0.2135\n",
      "Epoch 1 Batch 10250 Loss 2.2551 Accuracy 0.2138\n",
      "Epoch 1 Batch 10300 Loss 2.2524 Accuracy 0.2140\n",
      "Epoch 1 Batch 10350 Loss 2.2497 Accuracy 0.2143\n",
      "Epoch 1 Batch 10400 Loss 2.2469 Accuracy 0.2146\n",
      "Epoch 1 Batch 10450 Loss 2.2443 Accuracy 0.2149\n",
      "Epoch 1 Batch 10500 Loss 2.2418 Accuracy 0.2151\n",
      "Epoch 1 Batch 10550 Loss 2.2392 Accuracy 0.2154\n",
      "Epoch 1 Batch 10600 Loss 2.2365 Accuracy 0.2157\n",
      "Epoch 1 Batch 10650 Loss 2.2338 Accuracy 0.2159\n",
      "Epoch 1 Batch 10700 Loss 2.2312 Accuracy 0.2162\n",
      "Epoch 1 Batch 10750 Loss 2.2287 Accuracy 0.2165\n",
      "Epoch 1 Batch 10800 Loss 2.2259 Accuracy 0.2167\n",
      "Epoch 1 Batch 10850 Loss 2.2234 Accuracy 0.2170\n",
      "Epoch 1 Batch 10900 Loss 2.2208 Accuracy 0.2173\n",
      "Epoch 1 Batch 10950 Loss 2.2183 Accuracy 0.2175\n",
      "Epoch 1 Batch 11000 Loss 2.2157 Accuracy 0.2178\n",
      "Epoch 1 Batch 11050 Loss 2.2134 Accuracy 0.2181\n",
      "Epoch 1 Batch 11100 Loss 2.2110 Accuracy 0.2183\n",
      "Epoch 1 Batch 11150 Loss 2.2086 Accuracy 0.2186\n",
      "Epoch 1 Batch 11200 Loss 2.2062 Accuracy 0.2189\n",
      "Epoch 1 Batch 11250 Loss 2.2038 Accuracy 0.2191\n",
      "Epoch 1 Batch 11300 Loss 2.2014 Accuracy 0.2194\n",
      "Epoch 1 Batch 11350 Loss 2.1991 Accuracy 0.2197\n",
      "Epoch 1 Batch 11400 Loss 2.1967 Accuracy 0.2199\n",
      "Epoch 1 Batch 11450 Loss 2.1943 Accuracy 0.2202\n",
      "Epoch 1 Batch 11500 Loss 2.1919 Accuracy 0.2204\n",
      "Epoch 1 Batch 11550 Loss 2.1896 Accuracy 0.2206\n",
      "Epoch 1 Batch 11600 Loss 2.1871 Accuracy 0.2209\n",
      "Epoch 1 Batch 11650 Loss 2.1847 Accuracy 0.2211\n",
      "Epoch 1 Batch 11700 Loss 2.1822 Accuracy 0.2213\n",
      "Epoch 1 Batch 11750 Loss 2.1797 Accuracy 0.2215\n",
      "Epoch 1 Batch 11800 Loss 2.1773 Accuracy 0.2218\n",
      "Epoch 1 Batch 11850 Loss 2.1748 Accuracy 0.2220\n",
      "Epoch 1 Batch 11900 Loss 2.1726 Accuracy 0.2222\n",
      "Epoch 1 Batch 11950 Loss 2.1703 Accuracy 0.2225\n",
      "Epoch 1 Batch 12000 Loss 2.1679 Accuracy 0.2227\n",
      "Epoch 1 Batch 12050 Loss 2.1656 Accuracy 0.2229\n",
      "Epoch 1 Batch 12100 Loss 2.1633 Accuracy 0.2232\n",
      "Epoch 1 Batch 12150 Loss 2.1610 Accuracy 0.2234\n",
      "Epoch 1 Batch 12200 Loss 2.1587 Accuracy 0.2236\n",
      "Epoch 1 Batch 12250 Loss 2.1565 Accuracy 0.2238\n",
      "Epoch 1 Batch 12300 Loss 2.1543 Accuracy 0.2241\n",
      "Epoch 1 Batch 12350 Loss 2.1522 Accuracy 0.2243\n",
      "Epoch 1 Batch 12400 Loss 2.1502 Accuracy 0.2245\n",
      "Epoch 1 Batch 12450 Loss 2.1482 Accuracy 0.2248\n",
      "Epoch 1 Batch 12500 Loss 2.1460 Accuracy 0.2250\n",
      "Epoch 1 Batch 12550 Loss 2.1439 Accuracy 0.2252\n",
      "Epoch 1 Batch 12600 Loss 2.1418 Accuracy 0.2255\n",
      "Epoch 1 Batch 12650 Loss 2.1398 Accuracy 0.2257\n",
      "Epoch 1 Batch 12700 Loss 2.1377 Accuracy 0.2259\n",
      "Epoch 1 Batch 12750 Loss 2.1356 Accuracy 0.2261\n",
      "Epoch 1 Batch 12800 Loss 2.1335 Accuracy 0.2264\n",
      "Epoch 1 Batch 12850 Loss 2.1315 Accuracy 0.2266\n",
      "Epoch 1 Batch 12900 Loss 2.1294 Accuracy 0.2268\n",
      "Epoch 1 Batch 12950 Loss 2.1275 Accuracy 0.2271\n",
      "Epoch 1 Batch 13000 Loss 2.1255 Accuracy 0.2273\n",
      "Epoch 1 Batch 13050 Loss 2.1236 Accuracy 0.2275\n",
      "Epoch 1 Batch 13100 Loss 2.1216 Accuracy 0.2278\n",
      "Epoch 1 Batch 13150 Loss 2.1197 Accuracy 0.2280\n",
      "Epoch 1 Batch 13200 Loss 2.1178 Accuracy 0.2282\n",
      "Epoch 1 Batch 13250 Loss 2.1159 Accuracy 0.2285\n",
      "Epoch 1 Batch 13300 Loss 2.1139 Accuracy 0.2287\n",
      "Epoch 1 Batch 13350 Loss 2.1121 Accuracy 0.2289\n",
      "Epoch 1 Batch 13400 Loss 2.1101 Accuracy 0.2291\n",
      "Epoch 1 Batch 13450 Loss 2.1083 Accuracy 0.2294\n",
      "Epoch 1 Batch 13500 Loss 2.1065 Accuracy 0.2296\n",
      "Epoch 1 Batch 13550 Loss 2.1047 Accuracy 0.2298\n",
      "Epoch 1 Batch 13600 Loss 2.1030 Accuracy 0.2300\n",
      "Epoch 1 Batch 13650 Loss 2.1012 Accuracy 0.2303\n",
      "Epoch 1 Batch 13700 Loss 2.0994 Accuracy 0.2305\n",
      "Epoch 1 Batch 13750 Loss 2.0975 Accuracy 0.2307\n",
      "Epoch 1 Batch 13800 Loss 2.0956 Accuracy 0.2309\n",
      "Epoch 1 Batch 13850 Loss 2.0939 Accuracy 0.2311\n",
      "Epoch 1 Batch 13900 Loss 2.0922 Accuracy 0.2314\n",
      "Epoch 1 Batch 13950 Loss 2.0904 Accuracy 0.2316\n",
      "Epoch 1 Batch 14000 Loss 2.0886 Accuracy 0.2318\n",
      "Epoch 1 Batch 14050 Loss 2.0868 Accuracy 0.2320\n",
      "Epoch 1 Batch 14100 Loss 2.0850 Accuracy 0.2322\n",
      "Epoch 1 Batch 14150 Loss 2.0833 Accuracy 0.2325\n",
      "Epoch 1 Batch 14200 Loss 2.0815 Accuracy 0.2327\n",
      "Epoch 1 Batch 14250 Loss 2.0798 Accuracy 0.2329\n",
      "Epoch 1 Batch 14300 Loss 2.0780 Accuracy 0.2331\n",
      "Epoch 1 Batch 14350 Loss 2.0763 Accuracy 0.2333\n",
      "Epoch 1 Batch 14400 Loss 2.0746 Accuracy 0.2335\n",
      "Epoch 1 Batch 14450 Loss 2.0730 Accuracy 0.2337\n",
      "Epoch 1 Batch 14500 Loss 2.0714 Accuracy 0.2340\n",
      "Epoch 1 Batch 14550 Loss 2.0698 Accuracy 0.2342\n",
      "Epoch 1 Batch 14600 Loss 2.0681 Accuracy 0.2344\n",
      "Epoch 1 Batch 14650 Loss 2.0664 Accuracy 0.2346\n",
      "Epoch 1 Batch 14700 Loss 2.0647 Accuracy 0.2348\n",
      "Epoch 1 Batch 14750 Loss 2.0632 Accuracy 0.2349\n",
      "Epoch 1 Batch 14800 Loss 2.0619 Accuracy 0.2351\n",
      "Epoch 1 Batch 14850 Loss 2.0606 Accuracy 0.2352\n",
      "Epoch 1 Batch 14900 Loss 2.0595 Accuracy 0.2353\n",
      "Epoch 1 Batch 14950 Loss 2.0583 Accuracy 0.2355\n",
      "Epoch 1 Batch 15000 Loss 2.0572 Accuracy 0.2356\n",
      "Epoch 1 Batch 15050 Loss 2.0561 Accuracy 0.2357\n",
      "Epoch 1 Batch 15100 Loss 2.0550 Accuracy 0.2358\n",
      "Epoch 1 Batch 15150 Loss 2.0539 Accuracy 0.2359\n",
      "Epoch 1 Batch 15200 Loss 2.0529 Accuracy 0.2360\n",
      "Epoch 1 Batch 15250 Loss 2.0519 Accuracy 0.2361\n",
      "Epoch 1 Batch 15300 Loss 2.0509 Accuracy 0.2362\n",
      "Epoch 1 Batch 15350 Loss 2.0499 Accuracy 0.2363\n",
      "Epoch 1 Batch 15400 Loss 2.0489 Accuracy 0.2364\n",
      "Epoch 1 Batch 15450 Loss 2.0478 Accuracy 0.2365\n",
      "Epoch 1 Batch 15500 Loss 2.0468 Accuracy 0.2366\n",
      "Epoch 1 Batch 15550 Loss 2.0458 Accuracy 0.2367\n",
      "Epoch 1 Batch 15600 Loss 2.0448 Accuracy 0.2368\n",
      "Epoch 1 Batch 15650 Loss 2.0439 Accuracy 0.2368\n",
      "Epoch 1 Batch 15700 Loss 2.0429 Accuracy 0.2369\n",
      "Epoch 1 Batch 15750 Loss 2.0419 Accuracy 0.2370\n",
      "Epoch 1 Batch 15800 Loss 2.0409 Accuracy 0.2371\n",
      "Epoch 1 Batch 15850 Loss 2.0399 Accuracy 0.2372\n",
      "Epoch 1 Batch 15900 Loss 2.0389 Accuracy 0.2372\n",
      "Epoch 1 Batch 15950 Loss 2.0378 Accuracy 0.2373\n",
      "Epoch 1 Batch 16000 Loss 2.0369 Accuracy 0.2374\n",
      "Epoch 1 Batch 16050 Loss 2.0359 Accuracy 0.2375\n",
      "Epoch 1 Batch 16100 Loss 2.0349 Accuracy 0.2376\n",
      "Epoch 1 Batch 16150 Loss 2.0339 Accuracy 0.2377\n",
      "Epoch 1 Batch 16200 Loss 2.0329 Accuracy 0.2377\n",
      "Epoch 1 Batch 16250 Loss 2.0320 Accuracy 0.2378\n",
      "Epoch 1 Batch 16300 Loss 2.0310 Accuracy 0.2379\n",
      "Epoch 1 Batch 16350 Loss 2.0300 Accuracy 0.2380\n",
      "Epoch 1 Batch 16400 Loss 2.0290 Accuracy 0.2381\n",
      "Epoch 1 Batch 16450 Loss 2.0280 Accuracy 0.2382\n",
      "Epoch 1 Batch 16500 Loss 2.0270 Accuracy 0.2383\n",
      "Epoch 1 Batch 16550 Loss 2.0260 Accuracy 0.2383\n",
      "Epoch 1 Batch 16600 Loss 2.0251 Accuracy 0.2384\n",
      "Epoch 1 Batch 16650 Loss 2.0240 Accuracy 0.2385\n",
      "Epoch 1 Batch 16700 Loss 2.0231 Accuracy 0.2386\n",
      "Epoch 1 Batch 16750 Loss 2.0221 Accuracy 0.2387\n",
      "Epoch 1 Batch 16800 Loss 2.0211 Accuracy 0.2388\n",
      "Epoch 1 Batch 16850 Loss 2.0201 Accuracy 0.2389\n",
      "Epoch 1 Batch 16900 Loss 2.0192 Accuracy 0.2390\n",
      "Epoch 1 Batch 16950 Loss 2.0182 Accuracy 0.2391\n",
      "Epoch 1 Batch 17000 Loss 2.0172 Accuracy 0.2392\n",
      "Epoch 1 Batch 17050 Loss 2.0162 Accuracy 0.2393\n",
      "Epoch 1 Batch 17100 Loss 2.0152 Accuracy 0.2394\n",
      "Epoch 1 Batch 17150 Loss 2.0142 Accuracy 0.2395\n",
      "Epoch 1 Batch 17200 Loss 2.0132 Accuracy 0.2396\n",
      "Epoch 1 Batch 17250 Loss 2.0122 Accuracy 0.2397\n",
      "Epoch 1 Batch 17300 Loss 2.0112 Accuracy 0.2398\n",
      "Epoch 1 Batch 17350 Loss 2.0102 Accuracy 0.2399\n",
      "Epoch 1 Batch 17400 Loss 2.0092 Accuracy 0.2400\n",
      "Epoch 1 Batch 17450 Loss 2.0080 Accuracy 0.2401\n",
      "Epoch 1 Batch 17500 Loss 2.0071 Accuracy 0.2401\n",
      "Epoch 1 Batch 17550 Loss 2.0061 Accuracy 0.2402\n",
      "Epoch 1 Batch 17600 Loss 2.0052 Accuracy 0.2403\n",
      "Epoch 1 Batch 17650 Loss 2.0043 Accuracy 0.2404\n",
      "Epoch 1 Batch 17700 Loss 2.0034 Accuracy 0.2405\n",
      "Epoch 1 Batch 17750 Loss 2.0025 Accuracy 0.2406\n",
      "Epoch 1 Batch 17800 Loss 2.0017 Accuracy 0.2407\n",
      "Epoch 1 Batch 17850 Loss 2.0008 Accuracy 0.2408\n",
      "Epoch 1 Batch 17900 Loss 2.0000 Accuracy 0.2409\n",
      "Epoch 1 Batch 17950 Loss 1.9992 Accuracy 0.2409\n",
      "Epoch 1 Batch 18000 Loss 1.9983 Accuracy 0.2410\n",
      "Epoch 1 Batch 18050 Loss 1.9975 Accuracy 0.2411\n",
      "Epoch 1 Batch 18100 Loss 1.9967 Accuracy 0.2412\n",
      "Epoch 1 Batch 18150 Loss 1.9958 Accuracy 0.2413\n",
      "Epoch 1 Batch 18200 Loss 1.9950 Accuracy 0.2414\n",
      "Epoch 1 Batch 18250 Loss 1.9941 Accuracy 0.2414\n",
      "Epoch 1 Batch 18300 Loss 1.9932 Accuracy 0.2415\n",
      "Epoch 1 Batch 18350 Loss 1.9923 Accuracy 0.2416\n",
      "Epoch 1 Batch 18400 Loss 1.9915 Accuracy 0.2417\n",
      "Epoch 1 Batch 18450 Loss 1.9906 Accuracy 0.2418\n",
      "Epoch 1 Batch 18500 Loss 1.9897 Accuracy 0.2419\n",
      "Epoch 1 Batch 18550 Loss 1.9888 Accuracy 0.2420\n",
      "Epoch 1 Batch 18600 Loss 1.9878 Accuracy 0.2420\n",
      "Epoch 1 Batch 18650 Loss 1.9869 Accuracy 0.2421\n",
      "Epoch 1 Batch 18700 Loss 1.9861 Accuracy 0.2422\n",
      "Epoch 1 Batch 18750 Loss 1.9852 Accuracy 0.2422\n",
      "Epoch 1 Batch 18800 Loss 1.9843 Accuracy 0.2423\n",
      "Epoch 1 Batch 18850 Loss 1.9835 Accuracy 0.2424\n",
      "Epoch 1 Batch 18900 Loss 1.9826 Accuracy 0.2425\n",
      "Epoch 1 Batch 18950 Loss 1.9817 Accuracy 0.2425\n",
      "Epoch 1 Batch 19000 Loss 1.9809 Accuracy 0.2426\n",
      "Epoch 1 Batch 19050 Loss 1.9800 Accuracy 0.2427\n",
      "Epoch 1 Batch 19100 Loss 1.9791 Accuracy 0.2427\n",
      "Epoch 1 Batch 19150 Loss 1.9782 Accuracy 0.2428\n",
      "Epoch 1 Batch 19200 Loss 1.9773 Accuracy 0.2429\n",
      "Epoch 1 Batch 19250 Loss 1.9765 Accuracy 0.2430\n",
      "Epoch 1 Batch 19300 Loss 1.9756 Accuracy 0.2430\n",
      "Epoch 1 Batch 19350 Loss 1.9747 Accuracy 0.2431\n",
      "Saving checkpoint for epoch 1 at ./tradutor_chek/ckpt-1\n",
      "Time taken for 1 epoch 15683.21042394638 secs\n",
      "\n",
      "Start or epoch 2\n",
      "Epoch 2 Batch 0 Loss 1.6798 Accuracy 0.2736\n",
      "Epoch 2 Batch 50 Loss 1.7057 Accuracy 0.2698\n",
      "Epoch 2 Batch 100 Loss 1.6769 Accuracy 0.2691\n",
      "Epoch 2 Batch 150 Loss 1.6709 Accuracy 0.2686\n",
      "Epoch 2 Batch 200 Loss 1.6654 Accuracy 0.2688\n",
      "Epoch 2 Batch 250 Loss 1.6725 Accuracy 0.2696\n",
      "Epoch 2 Batch 300 Loss 1.6677 Accuracy 0.2700\n",
      "Epoch 2 Batch 350 Loss 1.6687 Accuracy 0.2705\n",
      "Epoch 2 Batch 400 Loss 1.6685 Accuracy 0.2708\n",
      "Epoch 2 Batch 450 Loss 1.6681 Accuracy 0.2712\n",
      "Epoch 2 Batch 500 Loss 1.6686 Accuracy 0.2710\n",
      "Epoch 2 Batch 550 Loss 1.6687 Accuracy 0.2713\n",
      "Epoch 2 Batch 600 Loss 1.6700 Accuracy 0.2715\n",
      "Epoch 2 Batch 650 Loss 1.6705 Accuracy 0.2716\n",
      "Epoch 2 Batch 700 Loss 1.6683 Accuracy 0.2714\n",
      "Epoch 2 Batch 750 Loss 1.6685 Accuracy 0.2715\n",
      "Epoch 2 Batch 800 Loss 1.6701 Accuracy 0.2718\n",
      "Epoch 2 Batch 850 Loss 1.6697 Accuracy 0.2720\n",
      "Epoch 2 Batch 900 Loss 1.6694 Accuracy 0.2722\n",
      "Epoch 2 Batch 950 Loss 1.6691 Accuracy 0.2724\n",
      "Epoch 2 Batch 1000 Loss 1.6675 Accuracy 0.2726\n",
      "Epoch 2 Batch 1050 Loss 1.6658 Accuracy 0.2727\n",
      "Epoch 2 Batch 1100 Loss 1.6639 Accuracy 0.2726\n",
      "Epoch 2 Batch 1150 Loss 1.6624 Accuracy 0.2727\n",
      "Epoch 2 Batch 1200 Loss 1.6612 Accuracy 0.2729\n",
      "Epoch 2 Batch 1250 Loss 1.6608 Accuracy 0.2731\n",
      "Epoch 2 Batch 1300 Loss 1.6591 Accuracy 0.2731\n",
      "Epoch 2 Batch 1350 Loss 1.6584 Accuracy 0.2732\n",
      "Epoch 2 Batch 1400 Loss 1.6572 Accuracy 0.2733\n",
      "Epoch 2 Batch 1450 Loss 1.6570 Accuracy 0.2734\n",
      "Epoch 2 Batch 1500 Loss 1.6563 Accuracy 0.2734\n",
      "Epoch 2 Batch 1550 Loss 1.6553 Accuracy 0.2734\n",
      "Epoch 2 Batch 1600 Loss 1.6551 Accuracy 0.2735\n",
      "Epoch 2 Batch 1650 Loss 1.6548 Accuracy 0.2735\n",
      "Epoch 2 Batch 1700 Loss 1.6551 Accuracy 0.2736\n",
      "Epoch 2 Batch 1750 Loss 1.6543 Accuracy 0.2738\n",
      "Epoch 2 Batch 1800 Loss 1.6539 Accuracy 0.2739\n",
      "Epoch 2 Batch 1850 Loss 1.6541 Accuracy 0.2741\n",
      "Epoch 2 Batch 1900 Loss 1.6538 Accuracy 0.2742\n",
      "Epoch 2 Batch 1950 Loss 1.6526 Accuracy 0.2742\n",
      "Epoch 2 Batch 2000 Loss 1.6522 Accuracy 0.2744\n",
      "Epoch 2 Batch 2050 Loss 1.6511 Accuracy 0.2745\n",
      "Epoch 2 Batch 2100 Loss 1.6507 Accuracy 0.2745\n",
      "Epoch 2 Batch 2150 Loss 1.6504 Accuracy 0.2746\n",
      "Epoch 2 Batch 2200 Loss 1.6502 Accuracy 0.2748\n",
      "Epoch 2 Batch 2250 Loss 1.6495 Accuracy 0.2748\n",
      "Epoch 2 Batch 2300 Loss 1.6495 Accuracy 0.2749\n",
      "Epoch 2 Batch 2350 Loss 1.6492 Accuracy 0.2750\n",
      "Epoch 2 Batch 2400 Loss 1.6490 Accuracy 0.2750\n",
      "Epoch 2 Batch 2450 Loss 1.6486 Accuracy 0.2751\n",
      "Epoch 2 Batch 2500 Loss 1.6485 Accuracy 0.2752\n",
      "Epoch 2 Batch 2550 Loss 1.6475 Accuracy 0.2753\n",
      "Epoch 2 Batch 2600 Loss 1.6472 Accuracy 0.2755\n",
      "Epoch 2 Batch 2650 Loss 1.6466 Accuracy 0.2756\n",
      "Epoch 2 Batch 2700 Loss 1.6463 Accuracy 0.2758\n",
      "Epoch 2 Batch 2750 Loss 1.6465 Accuracy 0.2760\n",
      "Epoch 2 Batch 2800 Loss 1.6453 Accuracy 0.2761\n",
      "Epoch 2 Batch 2850 Loss 1.6442 Accuracy 0.2763\n",
      "Epoch 2 Batch 2900 Loss 1.6439 Accuracy 0.2765\n",
      "Epoch 2 Batch 2950 Loss 1.6434 Accuracy 0.2766\n",
      "Epoch 2 Batch 3000 Loss 1.6425 Accuracy 0.2768\n",
      "Epoch 2 Batch 3050 Loss 1.6418 Accuracy 0.2770\n",
      "Epoch 2 Batch 3100 Loss 1.6408 Accuracy 0.2771\n",
      "Epoch 2 Batch 3150 Loss 1.6402 Accuracy 0.2772\n",
      "Epoch 2 Batch 3200 Loss 1.6392 Accuracy 0.2774\n",
      "Epoch 2 Batch 3250 Loss 1.6383 Accuracy 0.2775\n",
      "Epoch 2 Batch 3300 Loss 1.6371 Accuracy 0.2776\n",
      "Epoch 2 Batch 3350 Loss 1.6360 Accuracy 0.2776\n",
      "Epoch 2 Batch 3400 Loss 1.6349 Accuracy 0.2778\n",
      "Epoch 2 Batch 3450 Loss 1.6341 Accuracy 0.2779\n",
      "Epoch 2 Batch 3500 Loss 1.6327 Accuracy 0.2780\n",
      "Epoch 2 Batch 3550 Loss 1.6316 Accuracy 0.2781\n",
      "Epoch 2 Batch 3600 Loss 1.6305 Accuracy 0.2781\n",
      "Epoch 2 Batch 3650 Loss 1.6292 Accuracy 0.2782\n",
      "Epoch 2 Batch 3700 Loss 1.6280 Accuracy 0.2783\n",
      "Epoch 2 Batch 3750 Loss 1.6270 Accuracy 0.2784\n",
      "Epoch 2 Batch 3800 Loss 1.6260 Accuracy 0.2786\n",
      "Epoch 2 Batch 3850 Loss 1.6246 Accuracy 0.2786\n",
      "Epoch 2 Batch 3900 Loss 1.6237 Accuracy 0.2787\n",
      "Epoch 2 Batch 3950 Loss 1.6231 Accuracy 0.2788\n",
      "Epoch 2 Batch 4000 Loss 1.6225 Accuracy 0.2789\n",
      "Epoch 2 Batch 4050 Loss 1.6221 Accuracy 0.2790\n",
      "Epoch 2 Batch 4100 Loss 1.6216 Accuracy 0.2792\n",
      "Epoch 2 Batch 4150 Loss 1.6210 Accuracy 0.2792\n",
      "Epoch 2 Batch 4200 Loss 1.6203 Accuracy 0.2793\n",
      "Epoch 2 Batch 4250 Loss 1.6198 Accuracy 0.2795\n",
      "Epoch 2 Batch 4300 Loss 1.6193 Accuracy 0.2796\n",
      "Epoch 2 Batch 4350 Loss 1.6190 Accuracy 0.2797\n",
      "Epoch 2 Batch 4400 Loss 1.6186 Accuracy 0.2799\n",
      "Epoch 2 Batch 4450 Loss 1.6180 Accuracy 0.2800\n",
      "Epoch 2 Batch 4500 Loss 1.6178 Accuracy 0.2801\n",
      "Epoch 2 Batch 4550 Loss 1.6173 Accuracy 0.2803\n",
      "Epoch 2 Batch 4600 Loss 1.6165 Accuracy 0.2804\n",
      "Epoch 2 Batch 4650 Loss 1.6159 Accuracy 0.2805\n",
      "Epoch 2 Batch 4700 Loss 1.6151 Accuracy 0.2806\n",
      "Epoch 2 Batch 4750 Loss 1.6149 Accuracy 0.2808\n",
      "Epoch 2 Batch 4800 Loss 1.6145 Accuracy 0.2809\n",
      "Epoch 2 Batch 4850 Loss 1.6141 Accuracy 0.2810\n",
      "Epoch 2 Batch 4900 Loss 1.6137 Accuracy 0.2811\n",
      "Epoch 2 Batch 4950 Loss 1.6136 Accuracy 0.2812\n",
      "Epoch 2 Batch 5000 Loss 1.6135 Accuracy 0.2813\n",
      "Epoch 2 Batch 5050 Loss 1.6131 Accuracy 0.2814\n",
      "Epoch 2 Batch 5100 Loss 1.6129 Accuracy 0.2815\n",
      "Epoch 2 Batch 5150 Loss 1.6126 Accuracy 0.2817\n",
      "Epoch 2 Batch 5200 Loss 1.6121 Accuracy 0.2818\n",
      "Epoch 2 Batch 5250 Loss 1.6118 Accuracy 0.2819\n",
      "Epoch 2 Batch 5300 Loss 1.6114 Accuracy 0.2820\n",
      "Epoch 2 Batch 5350 Loss 1.6111 Accuracy 0.2821\n",
      "Epoch 2 Batch 5400 Loss 1.6105 Accuracy 0.2823\n",
      "Epoch 2 Batch 5450 Loss 1.6102 Accuracy 0.2824\n",
      "Epoch 2 Batch 5500 Loss 1.6098 Accuracy 0.2826\n",
      "Epoch 2 Batch 5550 Loss 1.6096 Accuracy 0.2827\n",
      "Epoch 2 Batch 5600 Loss 1.6093 Accuracy 0.2829\n",
      "Epoch 2 Batch 5650 Loss 1.6089 Accuracy 0.2830\n",
      "Epoch 2 Batch 5700 Loss 1.6085 Accuracy 0.2832\n",
      "Epoch 2 Batch 5750 Loss 1.6082 Accuracy 0.2833\n",
      "Epoch 2 Batch 5800 Loss 1.6077 Accuracy 0.2834\n",
      "Epoch 2 Batch 5850 Loss 1.6074 Accuracy 0.2836\n",
      "Epoch 2 Batch 5900 Loss 1.6069 Accuracy 0.2836\n",
      "Epoch 2 Batch 5950 Loss 1.6065 Accuracy 0.2838\n",
      "Epoch 2 Batch 6000 Loss 1.6063 Accuracy 0.2839\n",
      "Epoch 2 Batch 6050 Loss 1.6060 Accuracy 0.2840\n",
      "Epoch 2 Batch 6100 Loss 1.6059 Accuracy 0.2841\n",
      "Epoch 2 Batch 6150 Loss 1.6055 Accuracy 0.2842\n",
      "Epoch 2 Batch 6200 Loss 1.6055 Accuracy 0.2843\n",
      "Epoch 2 Batch 6250 Loss 1.6054 Accuracy 0.2844\n",
      "Epoch 2 Batch 6300 Loss 1.6051 Accuracy 0.2845\n",
      "Epoch 2 Batch 6350 Loss 1.6049 Accuracy 0.2846\n",
      "Epoch 2 Batch 6400 Loss 1.6046 Accuracy 0.2847\n",
      "Epoch 2 Batch 6450 Loss 1.6044 Accuracy 0.2849\n",
      "Epoch 2 Batch 6500 Loss 1.6042 Accuracy 0.2850\n",
      "Epoch 2 Batch 6550 Loss 1.6040 Accuracy 0.2851\n",
      "Epoch 2 Batch 6600 Loss 1.6035 Accuracy 0.2852\n",
      "Epoch 2 Batch 6650 Loss 1.6030 Accuracy 0.2853\n",
      "Epoch 2 Batch 6700 Loss 1.6027 Accuracy 0.2854\n",
      "Epoch 2 Batch 6750 Loss 1.6025 Accuracy 0.2855\n",
      "Epoch 2 Batch 6800 Loss 1.6022 Accuracy 0.2856\n",
      "Epoch 2 Batch 6850 Loss 1.6019 Accuracy 0.2856\n",
      "Epoch 2 Batch 6900 Loss 1.6016 Accuracy 0.2857\n",
      "Epoch 2 Batch 6950 Loss 1.6013 Accuracy 0.2858\n",
      "Epoch 2 Batch 7000 Loss 1.6009 Accuracy 0.2859\n",
      "Epoch 2 Batch 7050 Loss 1.6005 Accuracy 0.2860\n",
      "Epoch 2 Batch 7100 Loss 1.6002 Accuracy 0.2861\n",
      "Epoch 2 Batch 7150 Loss 1.5996 Accuracy 0.2862\n",
      "Epoch 2 Batch 7200 Loss 1.5992 Accuracy 0.2862\n",
      "Epoch 2 Batch 7250 Loss 1.5988 Accuracy 0.2864\n",
      "Epoch 2 Batch 7300 Loss 1.5984 Accuracy 0.2864\n",
      "Epoch 2 Batch 7350 Loss 1.5980 Accuracy 0.2865\n",
      "Epoch 2 Batch 7400 Loss 1.5975 Accuracy 0.2866\n",
      "Epoch 2 Batch 7450 Loss 1.5971 Accuracy 0.2867\n",
      "Epoch 2 Batch 7500 Loss 1.5965 Accuracy 0.2867\n",
      "Epoch 2 Batch 7550 Loss 1.5960 Accuracy 0.2868\n",
      "Epoch 2 Batch 7600 Loss 1.5955 Accuracy 0.2869\n",
      "Epoch 2 Batch 7650 Loss 1.5949 Accuracy 0.2870\n",
      "Epoch 2 Batch 7700 Loss 1.5945 Accuracy 0.2870\n",
      "Epoch 2 Batch 7750 Loss 1.5939 Accuracy 0.2871\n",
      "Epoch 2 Batch 7800 Loss 1.5933 Accuracy 0.2872\n",
      "Epoch 2 Batch 7850 Loss 1.5924 Accuracy 0.2872\n",
      "Epoch 2 Batch 7900 Loss 1.5915 Accuracy 0.2872\n",
      "Epoch 2 Batch 7950 Loss 1.5908 Accuracy 0.2873\n",
      "Epoch 2 Batch 8000 Loss 1.5901 Accuracy 0.2874\n",
      "Epoch 2 Batch 8050 Loss 1.5893 Accuracy 0.2874\n",
      "Epoch 2 Batch 8100 Loss 1.5885 Accuracy 0.2874\n",
      "Epoch 2 Batch 8150 Loss 1.5876 Accuracy 0.2875\n",
      "Epoch 2 Batch 8200 Loss 1.5869 Accuracy 0.2875\n",
      "Epoch 2 Batch 8250 Loss 1.5861 Accuracy 0.2875\n",
      "Epoch 2 Batch 8300 Loss 1.5854 Accuracy 0.2876\n",
      "Epoch 2 Batch 8350 Loss 1.5848 Accuracy 0.2876\n",
      "Epoch 2 Batch 8400 Loss 1.5842 Accuracy 0.2876\n",
      "Epoch 2 Batch 8450 Loss 1.5837 Accuracy 0.2877\n",
      "Epoch 2 Batch 8500 Loss 1.5831 Accuracy 0.2877\n",
      "Epoch 2 Batch 8550 Loss 1.5825 Accuracy 0.2878\n",
      "Epoch 2 Batch 8600 Loss 1.5820 Accuracy 0.2879\n",
      "Epoch 2 Batch 8650 Loss 1.5815 Accuracy 0.2879\n",
      "Epoch 2 Batch 8700 Loss 1.5811 Accuracy 0.2880\n",
      "Epoch 2 Batch 8750 Loss 1.5808 Accuracy 0.2880\n",
      "Epoch 2 Batch 8800 Loss 1.5803 Accuracy 0.2881\n",
      "Epoch 2 Batch 8850 Loss 1.5797 Accuracy 0.2881\n",
      "Epoch 2 Batch 8900 Loss 1.5791 Accuracy 0.2882\n",
      "Epoch 2 Batch 8950 Loss 1.5785 Accuracy 0.2882\n",
      "Epoch 2 Batch 9000 Loss 1.5780 Accuracy 0.2883\n",
      "Epoch 2 Batch 9050 Loss 1.5776 Accuracy 0.2883\n",
      "Epoch 2 Batch 9100 Loss 1.5771 Accuracy 0.2884\n",
      "Epoch 2 Batch 9150 Loss 1.5767 Accuracy 0.2885\n",
      "Epoch 2 Batch 9200 Loss 1.5762 Accuracy 0.2885\n",
      "Epoch 2 Batch 9250 Loss 1.5757 Accuracy 0.2886\n",
      "Epoch 2 Batch 9300 Loss 1.5751 Accuracy 0.2887\n",
      "Epoch 2 Batch 9350 Loss 1.5746 Accuracy 0.2888\n",
      "Epoch 2 Batch 9400 Loss 1.5740 Accuracy 0.2888\n",
      "Epoch 2 Batch 9450 Loss 1.5735 Accuracy 0.2889\n",
      "Epoch 2 Batch 9500 Loss 1.5729 Accuracy 0.2890\n",
      "Epoch 2 Batch 9550 Loss 1.5724 Accuracy 0.2891\n",
      "Epoch 2 Batch 9600 Loss 1.5719 Accuracy 0.2892\n",
      "Epoch 2 Batch 9650 Loss 1.5714 Accuracy 0.2893\n",
      "Epoch 2 Batch 9700 Loss 1.5710 Accuracy 0.2893\n",
      "Epoch 2 Batch 9750 Loss 1.5704 Accuracy 0.2894\n",
      "Epoch 2 Batch 9800 Loss 1.5699 Accuracy 0.2895\n",
      "Epoch 2 Batch 9850 Loss 1.5695 Accuracy 0.2896\n",
      "Epoch 2 Batch 9900 Loss 1.5689 Accuracy 0.2896\n",
      "Epoch 2 Batch 9950 Loss 1.5684 Accuracy 0.2897\n",
      "Epoch 2 Batch 10000 Loss 1.5678 Accuracy 0.2898\n",
      "Epoch 2 Batch 10050 Loss 1.5674 Accuracy 0.2898\n",
      "Epoch 2 Batch 10100 Loss 1.5669 Accuracy 0.2899\n",
      "Epoch 2 Batch 10150 Loss 1.5664 Accuracy 0.2899\n",
      "Epoch 2 Batch 10200 Loss 1.5660 Accuracy 0.2900\n",
      "Epoch 2 Batch 10250 Loss 1.5656 Accuracy 0.2900\n",
      "Epoch 2 Batch 10300 Loss 1.5650 Accuracy 0.2901\n",
      "Epoch 2 Batch 10350 Loss 1.5646 Accuracy 0.2902\n",
      "Epoch 2 Batch 10400 Loss 1.5641 Accuracy 0.2902\n",
      "Epoch 2 Batch 10450 Loss 1.5637 Accuracy 0.2903\n",
      "Epoch 2 Batch 10500 Loss 1.5632 Accuracy 0.2903\n",
      "Epoch 2 Batch 10550 Loss 1.5628 Accuracy 0.2904\n",
      "Epoch 2 Batch 10600 Loss 1.5623 Accuracy 0.2904\n",
      "Epoch 2 Batch 10650 Loss 1.5619 Accuracy 0.2905\n",
      "Epoch 2 Batch 10700 Loss 1.5613 Accuracy 0.2906\n",
      "Epoch 2 Batch 10750 Loss 1.5609 Accuracy 0.2906\n",
      "Epoch 2 Batch 10800 Loss 1.5603 Accuracy 0.2907\n",
      "Epoch 2 Batch 10850 Loss 1.5599 Accuracy 0.2907\n",
      "Epoch 2 Batch 10900 Loss 1.5594 Accuracy 0.2908\n",
      "Epoch 2 Batch 10950 Loss 1.5589 Accuracy 0.2909\n",
      "Epoch 2 Batch 11000 Loss 1.5585 Accuracy 0.2910\n",
      "Epoch 2 Batch 11050 Loss 1.5580 Accuracy 0.2910\n",
      "Epoch 2 Batch 11100 Loss 1.5576 Accuracy 0.2911\n",
      "Epoch 2 Batch 11150 Loss 1.5572 Accuracy 0.2912\n",
      "Epoch 2 Batch 11200 Loss 1.5568 Accuracy 0.2913\n",
      "Epoch 2 Batch 11250 Loss 1.5564 Accuracy 0.2914\n",
      "Epoch 2 Batch 11300 Loss 1.5561 Accuracy 0.2914\n",
      "Epoch 2 Batch 11350 Loss 1.5557 Accuracy 0.2915\n",
      "Epoch 2 Batch 11400 Loss 1.5554 Accuracy 0.2916\n",
      "Epoch 2 Batch 11450 Loss 1.5549 Accuracy 0.2917\n",
      "Epoch 2 Batch 11500 Loss 1.5544 Accuracy 0.2917\n",
      "Epoch 2 Batch 11550 Loss 1.5538 Accuracy 0.2917\n",
      "Epoch 2 Batch 11600 Loss 1.5533 Accuracy 0.2918\n",
      "Epoch 2 Batch 11650 Loss 1.5527 Accuracy 0.2918\n",
      "Epoch 2 Batch 11700 Loss 1.5522 Accuracy 0.2919\n",
      "Epoch 2 Batch 11750 Loss 1.5517 Accuracy 0.2919\n",
      "Epoch 2 Batch 11800 Loss 1.5511 Accuracy 0.2920\n",
      "Epoch 2 Batch 11850 Loss 1.5505 Accuracy 0.2920\n",
      "Epoch 2 Batch 11900 Loss 1.5500 Accuracy 0.2921\n",
      "Epoch 2 Batch 11950 Loss 1.5494 Accuracy 0.2921\n",
      "Epoch 2 Batch 12000 Loss 1.5488 Accuracy 0.2922\n",
      "Epoch 2 Batch 12050 Loss 1.5483 Accuracy 0.2923\n",
      "Epoch 2 Batch 12100 Loss 1.5479 Accuracy 0.2923\n",
      "Epoch 2 Batch 12150 Loss 1.5474 Accuracy 0.2924\n",
      "Epoch 2 Batch 12200 Loss 1.5468 Accuracy 0.2925\n",
      "Epoch 2 Batch 12250 Loss 1.5464 Accuracy 0.2925\n",
      "Epoch 2 Batch 12300 Loss 1.5459 Accuracy 0.2926\n",
      "Epoch 2 Batch 12350 Loss 1.5455 Accuracy 0.2926\n",
      "Epoch 2 Batch 12400 Loss 1.5451 Accuracy 0.2927\n",
      "Epoch 2 Batch 12450 Loss 1.5446 Accuracy 0.2927\n",
      "Epoch 2 Batch 12500 Loss 1.5441 Accuracy 0.2928\n",
      "Epoch 2 Batch 12550 Loss 1.5436 Accuracy 0.2928\n",
      "Epoch 2 Batch 12600 Loss 1.5432 Accuracy 0.2929\n",
      "Epoch 2 Batch 12650 Loss 1.5427 Accuracy 0.2930\n",
      "Epoch 2 Batch 12700 Loss 1.5423 Accuracy 0.2931\n",
      "Epoch 2 Batch 12750 Loss 1.5419 Accuracy 0.2931\n",
      "Epoch 2 Batch 12800 Loss 1.5415 Accuracy 0.2932\n",
      "Epoch 2 Batch 12850 Loss 1.5412 Accuracy 0.2933\n",
      "Epoch 2 Batch 12900 Loss 1.5408 Accuracy 0.2934\n",
      "Epoch 2 Batch 12950 Loss 1.5404 Accuracy 0.2934\n",
      "Epoch 2 Batch 13000 Loss 1.5400 Accuracy 0.2935\n",
      "Epoch 2 Batch 13050 Loss 1.5396 Accuracy 0.2936\n",
      "Epoch 2 Batch 13100 Loss 1.5392 Accuracy 0.2937\n",
      "Epoch 2 Batch 13150 Loss 1.5388 Accuracy 0.2938\n",
      "Epoch 2 Batch 13200 Loss 1.5385 Accuracy 0.2938\n",
      "Epoch 2 Batch 13250 Loss 1.5380 Accuracy 0.2939\n",
      "Epoch 2 Batch 13300 Loss 1.5376 Accuracy 0.2940\n",
      "Epoch 2 Batch 13350 Loss 1.5372 Accuracy 0.2941\n",
      "Epoch 2 Batch 13400 Loss 1.5368 Accuracy 0.2942\n",
      "Epoch 2 Batch 13450 Loss 1.5364 Accuracy 0.2942\n",
      "Epoch 2 Batch 13500 Loss 1.5360 Accuracy 0.2943\n",
      "Epoch 2 Batch 13550 Loss 1.5357 Accuracy 0.2944\n",
      "Epoch 2 Batch 13600 Loss 1.5353 Accuracy 0.2945\n",
      "Epoch 2 Batch 13650 Loss 1.5350 Accuracy 0.2946\n",
      "Epoch 2 Batch 13700 Loss 1.5346 Accuracy 0.2946\n",
      "Epoch 2 Batch 13750 Loss 1.5343 Accuracy 0.2947\n",
      "Epoch 2 Batch 13800 Loss 1.5339 Accuracy 0.2948\n",
      "Epoch 2 Batch 13850 Loss 1.5335 Accuracy 0.2949\n",
      "Epoch 2 Batch 13900 Loss 1.5332 Accuracy 0.2950\n",
      "Epoch 2 Batch 13950 Loss 1.5328 Accuracy 0.2951\n",
      "Epoch 2 Batch 14000 Loss 1.5325 Accuracy 0.2951\n",
      "Epoch 2 Batch 14050 Loss 1.5321 Accuracy 0.2952\n",
      "Epoch 2 Batch 14100 Loss 1.5317 Accuracy 0.2953\n",
      "Epoch 2 Batch 14150 Loss 1.5313 Accuracy 0.2954\n",
      "Epoch 2 Batch 14200 Loss 1.5309 Accuracy 0.2955\n",
      "Epoch 2 Batch 14250 Loss 1.5305 Accuracy 0.2955\n",
      "Epoch 2 Batch 14300 Loss 1.5301 Accuracy 0.2956\n",
      "Epoch 2 Batch 14350 Loss 1.5298 Accuracy 0.2957\n",
      "Epoch 2 Batch 14400 Loss 1.5294 Accuracy 0.2958\n",
      "Epoch 2 Batch 14450 Loss 1.5291 Accuracy 0.2958\n",
      "Epoch 2 Batch 14500 Loss 1.5287 Accuracy 0.2959\n",
      "Epoch 2 Batch 14550 Loss 1.5283 Accuracy 0.2960\n",
      "Epoch 2 Batch 14600 Loss 1.5279 Accuracy 0.2961\n",
      "Epoch 2 Batch 14650 Loss 1.5275 Accuracy 0.2961\n",
      "Epoch 2 Batch 14700 Loss 1.5272 Accuracy 0.2962\n",
      "Epoch 2 Batch 14750 Loss 1.5270 Accuracy 0.2962\n",
      "Epoch 2 Batch 14800 Loss 1.5269 Accuracy 0.2962\n",
      "Epoch 2 Batch 14850 Loss 1.5269 Accuracy 0.2963\n",
      "Epoch 2 Batch 14900 Loss 1.5269 Accuracy 0.2962\n",
      "Epoch 2 Batch 14950 Loss 1.5270 Accuracy 0.2962\n",
      "Epoch 2 Batch 15000 Loss 1.5271 Accuracy 0.2962\n",
      "Epoch 2 Batch 15050 Loss 1.5273 Accuracy 0.2962\n",
      "Epoch 2 Batch 15100 Loss 1.5274 Accuracy 0.2962\n",
      "Epoch 2 Batch 15150 Loss 1.5276 Accuracy 0.2961\n",
      "Epoch 2 Batch 15200 Loss 1.5277 Accuracy 0.2961\n",
      "Epoch 2 Batch 15250 Loss 1.5278 Accuracy 0.2961\n",
      "Epoch 2 Batch 15300 Loss 1.5281 Accuracy 0.2961\n",
      "Epoch 2 Batch 15350 Loss 1.5283 Accuracy 0.2960\n",
      "Epoch 2 Batch 15400 Loss 1.5286 Accuracy 0.2960\n",
      "Epoch 2 Batch 15450 Loss 1.5289 Accuracy 0.2960\n",
      "Epoch 2 Batch 15500 Loss 1.5291 Accuracy 0.2960\n",
      "Epoch 2 Batch 15550 Loss 1.5292 Accuracy 0.2959\n",
      "Epoch 2 Batch 15600 Loss 1.5294 Accuracy 0.2959\n",
      "Epoch 2 Batch 15650 Loss 1.5295 Accuracy 0.2958\n",
      "Epoch 2 Batch 15700 Loss 1.5297 Accuracy 0.2958\n",
      "Epoch 2 Batch 15750 Loss 1.5299 Accuracy 0.2958\n",
      "Epoch 2 Batch 15800 Loss 1.5301 Accuracy 0.2957\n",
      "Epoch 2 Batch 15850 Loss 1.5302 Accuracy 0.2957\n",
      "Epoch 2 Batch 15900 Loss 1.5303 Accuracy 0.2957\n",
      "Epoch 2 Batch 15950 Loss 1.5305 Accuracy 0.2956\n",
      "Epoch 2 Batch 16000 Loss 1.5306 Accuracy 0.2956\n",
      "Epoch 2 Batch 16050 Loss 1.5307 Accuracy 0.2956\n",
      "Epoch 2 Batch 16100 Loss 1.5309 Accuracy 0.2955\n",
      "Epoch 2 Batch 16150 Loss 1.5310 Accuracy 0.2955\n",
      "Epoch 2 Batch 16200 Loss 1.5312 Accuracy 0.2955\n",
      "Epoch 2 Batch 16250 Loss 1.5313 Accuracy 0.2954\n",
      "Epoch 2 Batch 16300 Loss 1.5314 Accuracy 0.2954\n",
      "Epoch 2 Batch 16350 Loss 1.5315 Accuracy 0.2954\n",
      "Epoch 2 Batch 16400 Loss 1.5315 Accuracy 0.2953\n",
      "Epoch 2 Batch 16450 Loss 1.5317 Accuracy 0.2953\n",
      "Epoch 2 Batch 16500 Loss 1.5318 Accuracy 0.2953\n",
      "Epoch 2 Batch 16550 Loss 1.5319 Accuracy 0.2953\n",
      "Epoch 2 Batch 16600 Loss 1.5319 Accuracy 0.2952\n",
      "Epoch 2 Batch 16650 Loss 1.5320 Accuracy 0.2952\n",
      "Epoch 2 Batch 16700 Loss 1.5319 Accuracy 0.2952\n",
      "Epoch 2 Batch 16750 Loss 1.5320 Accuracy 0.2952\n",
      "Epoch 2 Batch 16800 Loss 1.5321 Accuracy 0.2952\n",
      "Epoch 2 Batch 16850 Loss 1.5321 Accuracy 0.2951\n",
      "Epoch 2 Batch 16900 Loss 1.5322 Accuracy 0.2951\n",
      "Epoch 2 Batch 16950 Loss 1.5322 Accuracy 0.2951\n",
      "Epoch 2 Batch 17000 Loss 1.5323 Accuracy 0.2951\n",
      "Epoch 2 Batch 17050 Loss 1.5323 Accuracy 0.2951\n",
      "Epoch 2 Batch 17100 Loss 1.5323 Accuracy 0.2951\n",
      "Epoch 2 Batch 17150 Loss 1.5324 Accuracy 0.2951\n",
      "Epoch 2 Batch 17200 Loss 1.5324 Accuracy 0.2951\n",
      "Epoch 2 Batch 17250 Loss 1.5325 Accuracy 0.2951\n",
      "Epoch 2 Batch 17300 Loss 1.5326 Accuracy 0.2951\n",
      "Epoch 2 Batch 17350 Loss 1.5326 Accuracy 0.2950\n",
      "Epoch 2 Batch 17400 Loss 1.5326 Accuracy 0.2950\n",
      "Epoch 2 Batch 17450 Loss 1.5326 Accuracy 0.2950\n",
      "Epoch 2 Batch 17500 Loss 1.5326 Accuracy 0.2950\n",
      "Epoch 2 Batch 17550 Loss 1.5327 Accuracy 0.2950\n",
      "Epoch 2 Batch 17600 Loss 1.5328 Accuracy 0.2950\n",
      "Epoch 2 Batch 17650 Loss 1.5328 Accuracy 0.2950\n",
      "Epoch 2 Batch 17700 Loss 1.5329 Accuracy 0.2950\n",
      "Epoch 2 Batch 17750 Loss 1.5330 Accuracy 0.2950\n",
      "Epoch 2 Batch 17800 Loss 1.5331 Accuracy 0.2950\n",
      "Epoch 2 Batch 17850 Loss 1.5332 Accuracy 0.2949\n",
      "Epoch 2 Batch 17900 Loss 1.5333 Accuracy 0.2949\n",
      "Epoch 2 Batch 17950 Loss 1.5333 Accuracy 0.2949\n",
      "Epoch 2 Batch 18000 Loss 1.5333 Accuracy 0.2949\n",
      "Epoch 2 Batch 18050 Loss 1.5334 Accuracy 0.2949\n",
      "Epoch 2 Batch 18100 Loss 1.5336 Accuracy 0.2949\n",
      "Epoch 2 Batch 18150 Loss 1.5338 Accuracy 0.2949\n",
      "Epoch 2 Batch 18200 Loss 1.5338 Accuracy 0.2948\n",
      "Epoch 2 Batch 18250 Loss 1.5338 Accuracy 0.2948\n",
      "Epoch 2 Batch 18300 Loss 1.5338 Accuracy 0.2948\n",
      "Epoch 2 Batch 18350 Loss 1.5339 Accuracy 0.2948\n",
      "Epoch 2 Batch 18400 Loss 1.5339 Accuracy 0.2948\n",
      "Epoch 2 Batch 18450 Loss 1.5339 Accuracy 0.2948\n",
      "Epoch 2 Batch 18500 Loss 1.5339 Accuracy 0.2947\n",
      "Epoch 2 Batch 18550 Loss 1.5340 Accuracy 0.2947\n",
      "Epoch 2 Batch 18600 Loss 1.5340 Accuracy 0.2947\n",
      "Epoch 2 Batch 18650 Loss 1.5340 Accuracy 0.2947\n",
      "Epoch 2 Batch 18700 Loss 1.5341 Accuracy 0.2947\n",
      "Epoch 2 Batch 18750 Loss 1.5341 Accuracy 0.2946\n",
      "Epoch 2 Batch 18800 Loss 1.5342 Accuracy 0.2946\n",
      "Epoch 2 Batch 18850 Loss 1.5341 Accuracy 0.2946\n",
      "Epoch 2 Batch 18900 Loss 1.5341 Accuracy 0.2946\n",
      "Epoch 2 Batch 18950 Loss 1.5341 Accuracy 0.2946\n",
      "Epoch 2 Batch 19000 Loss 1.5341 Accuracy 0.2945\n",
      "Epoch 2 Batch 19050 Loss 1.5340 Accuracy 0.2945\n",
      "Epoch 2 Batch 19100 Loss 1.5340 Accuracy 0.2945\n",
      "Epoch 2 Batch 19150 Loss 1.5339 Accuracy 0.2945\n",
      "Epoch 2 Batch 19200 Loss 1.5338 Accuracy 0.2945\n",
      "Epoch 2 Batch 19250 Loss 1.5338 Accuracy 0.2944\n",
      "Epoch 2 Batch 19300 Loss 1.5338 Accuracy 0.2944\n",
      "Epoch 2 Batch 19350 Loss 1.5337 Accuracy 0.2944\n",
      "Saving checkpoint for epoch 2 at ./tradutor_chek/ckpt-2\n",
      "Time taken for 1 epoch 15744.153058052063 secs\n",
      "\n",
      "Start or epoch 3\n",
      "Epoch 3 Batch 0 Loss 1.5972 Accuracy 0.2768\n",
      "Epoch 3 Batch 50 Loss 1.5608 Accuracy 0.2813\n",
      "Epoch 3 Batch 100 Loss 1.5598 Accuracy 0.2849\n",
      "Epoch 3 Batch 150 Loss 1.5553 Accuracy 0.2857\n",
      "Epoch 3 Batch 200 Loss 1.5519 Accuracy 0.2864\n",
      "Epoch 3 Batch 250 Loss 1.5432 Accuracy 0.2864\n",
      "Epoch 3 Batch 300 Loss 1.5450 Accuracy 0.2875\n",
      "Epoch 3 Batch 350 Loss 1.5410 Accuracy 0.2875\n",
      "Epoch 3 Batch 400 Loss 1.5411 Accuracy 0.2879\n",
      "Epoch 3 Batch 450 Loss 1.5410 Accuracy 0.2881\n",
      "Epoch 3 Batch 500 Loss 1.5443 Accuracy 0.2887\n",
      "Epoch 3 Batch 550 Loss 1.5462 Accuracy 0.2891\n",
      "Epoch 3 Batch 600 Loss 1.5472 Accuracy 0.2892\n",
      "Epoch 3 Batch 650 Loss 1.5475 Accuracy 0.2892\n",
      "Epoch 3 Batch 700 Loss 1.5463 Accuracy 0.2892\n",
      "Epoch 3 Batch 750 Loss 1.5479 Accuracy 0.2891\n",
      "Epoch 3 Batch 800 Loss 1.5469 Accuracy 0.2891\n",
      "Epoch 3 Batch 850 Loss 1.5468 Accuracy 0.2892\n",
      "Epoch 3 Batch 900 Loss 1.5473 Accuracy 0.2894\n",
      "Epoch 3 Batch 950 Loss 1.5472 Accuracy 0.2894\n",
      "Epoch 3 Batch 1000 Loss 1.5476 Accuracy 0.2893\n",
      "Epoch 3 Batch 1050 Loss 1.5470 Accuracy 0.2893\n",
      "Epoch 3 Batch 1100 Loss 1.5455 Accuracy 0.2894\n",
      "Epoch 3 Batch 1150 Loss 1.5450 Accuracy 0.2894\n",
      "Epoch 3 Batch 1200 Loss 1.5434 Accuracy 0.2895\n",
      "Epoch 3 Batch 1250 Loss 1.5426 Accuracy 0.2896\n",
      "Epoch 3 Batch 1300 Loss 1.5428 Accuracy 0.2897\n",
      "Epoch 3 Batch 1350 Loss 1.5426 Accuracy 0.2897\n",
      "Epoch 3 Batch 1400 Loss 1.5428 Accuracy 0.2897\n",
      "Epoch 3 Batch 1450 Loss 1.5419 Accuracy 0.2898\n",
      "Epoch 3 Batch 1500 Loss 1.5409 Accuracy 0.2898\n",
      "Epoch 3 Batch 1550 Loss 1.5405 Accuracy 0.2899\n",
      "Epoch 3 Batch 1600 Loss 1.5396 Accuracy 0.2898\n",
      "Epoch 3 Batch 1650 Loss 1.5396 Accuracy 0.2899\n",
      "Epoch 3 Batch 1700 Loss 1.5399 Accuracy 0.2901\n",
      "Epoch 3 Batch 1750 Loss 1.5393 Accuracy 0.2902\n",
      "Epoch 3 Batch 1800 Loss 1.5394 Accuracy 0.2904\n",
      "Epoch 3 Batch 1850 Loss 1.5388 Accuracy 0.2905\n",
      "Epoch 3 Batch 1900 Loss 1.5388 Accuracy 0.2906\n",
      "Epoch 3 Batch 1950 Loss 1.5380 Accuracy 0.2905\n",
      "Epoch 3 Batch 2000 Loss 1.5366 Accuracy 0.2906\n",
      "Epoch 3 Batch 2050 Loss 1.5359 Accuracy 0.2907\n",
      "Epoch 3 Batch 2100 Loss 1.5348 Accuracy 0.2907\n",
      "Epoch 3 Batch 2150 Loss 1.5345 Accuracy 0.2907\n",
      "Epoch 3 Batch 2200 Loss 1.5351 Accuracy 0.2908\n",
      "Epoch 3 Batch 2250 Loss 1.5350 Accuracy 0.2909\n",
      "Epoch 3 Batch 2300 Loss 1.5348 Accuracy 0.2910\n",
      "Epoch 3 Batch 2350 Loss 1.5353 Accuracy 0.2911\n",
      "Epoch 3 Batch 2400 Loss 1.5346 Accuracy 0.2911\n",
      "Epoch 3 Batch 2450 Loss 1.5348 Accuracy 0.2912\n",
      "Epoch 3 Batch 2500 Loss 1.5353 Accuracy 0.2913\n",
      "Epoch 3 Batch 2550 Loss 1.5350 Accuracy 0.2914\n",
      "Epoch 3 Batch 2600 Loss 1.5345 Accuracy 0.2915\n",
      "Epoch 3 Batch 2650 Loss 1.5338 Accuracy 0.2916\n",
      "Epoch 3 Batch 2700 Loss 1.5334 Accuracy 0.2918\n",
      "Epoch 3 Batch 2750 Loss 1.5335 Accuracy 0.2920\n",
      "Epoch 3 Batch 2800 Loss 1.5332 Accuracy 0.2923\n",
      "Epoch 3 Batch 2850 Loss 1.5327 Accuracy 0.2924\n",
      "Epoch 3 Batch 2900 Loss 1.5320 Accuracy 0.2926\n",
      "Epoch 3 Batch 2950 Loss 1.5312 Accuracy 0.2927\n",
      "Epoch 3 Batch 3000 Loss 1.5302 Accuracy 0.2928\n",
      "Epoch 3 Batch 3050 Loss 1.5294 Accuracy 0.2930\n",
      "Epoch 3 Batch 3100 Loss 1.5289 Accuracy 0.2932\n",
      "Epoch 3 Batch 3150 Loss 1.5277 Accuracy 0.2933\n",
      "Epoch 3 Batch 3200 Loss 1.5268 Accuracy 0.2934\n",
      "Epoch 3 Batch 3250 Loss 1.5261 Accuracy 0.2936\n",
      "Epoch 3 Batch 3300 Loss 1.5250 Accuracy 0.2937\n",
      "Epoch 3 Batch 3350 Loss 1.5242 Accuracy 0.2938\n",
      "Epoch 3 Batch 3400 Loss 1.5231 Accuracy 0.2939\n",
      "Epoch 3 Batch 3450 Loss 1.5220 Accuracy 0.2939\n",
      "Epoch 3 Batch 3500 Loss 1.5210 Accuracy 0.2940\n",
      "Epoch 3 Batch 3550 Loss 1.5197 Accuracy 0.2940\n",
      "Epoch 3 Batch 3600 Loss 1.5186 Accuracy 0.2941\n",
      "Epoch 3 Batch 3650 Loss 1.5175 Accuracy 0.2942\n",
      "Epoch 3 Batch 3700 Loss 1.5167 Accuracy 0.2943\n",
      "Epoch 3 Batch 3750 Loss 1.5159 Accuracy 0.2944\n",
      "Epoch 3 Batch 3800 Loss 1.5146 Accuracy 0.2945\n",
      "Epoch 3 Batch 3850 Loss 1.5136 Accuracy 0.2946\n",
      "Epoch 3 Batch 3900 Loss 1.5128 Accuracy 0.2947\n",
      "Epoch 3 Batch 3950 Loss 1.5122 Accuracy 0.2947\n",
      "Epoch 3 Batch 4000 Loss 1.5118 Accuracy 0.2949\n",
      "Epoch 3 Batch 4050 Loss 1.5114 Accuracy 0.2950\n",
      "Epoch 3 Batch 4100 Loss 1.5110 Accuracy 0.2952\n",
      "Epoch 3 Batch 4150 Loss 1.5107 Accuracy 0.2953\n",
      "Epoch 3 Batch 4200 Loss 1.5101 Accuracy 0.2954\n",
      "Epoch 3 Batch 4250 Loss 1.5099 Accuracy 0.2955\n",
      "Epoch 3 Batch 4300 Loss 1.5096 Accuracy 0.2956\n",
      "Epoch 3 Batch 4350 Loss 1.5092 Accuracy 0.2958\n",
      "Epoch 3 Batch 4400 Loss 1.5088 Accuracy 0.2959\n",
      "Epoch 3 Batch 4450 Loss 1.5086 Accuracy 0.2960\n",
      "Epoch 3 Batch 4500 Loss 1.5083 Accuracy 0.2961\n",
      "Epoch 3 Batch 4550 Loss 1.5080 Accuracy 0.2962\n",
      "Epoch 3 Batch 4600 Loss 1.5073 Accuracy 0.2963\n",
      "Epoch 3 Batch 4650 Loss 1.5068 Accuracy 0.2965\n",
      "Epoch 3 Batch 4700 Loss 1.5062 Accuracy 0.2966\n",
      "Epoch 3 Batch 4750 Loss 1.5059 Accuracy 0.2967\n",
      "Epoch 3 Batch 4800 Loss 1.5057 Accuracy 0.2968\n",
      "Epoch 3 Batch 4850 Loss 1.5055 Accuracy 0.2969\n",
      "Epoch 3 Batch 4900 Loss 1.5051 Accuracy 0.2970\n",
      "Epoch 3 Batch 4950 Loss 1.5051 Accuracy 0.2971\n",
      "Epoch 3 Batch 5000 Loss 1.5049 Accuracy 0.2972\n",
      "Epoch 3 Batch 5050 Loss 1.5045 Accuracy 0.2973\n",
      "Epoch 3 Batch 5100 Loss 1.5043 Accuracy 0.2974\n",
      "Epoch 3 Batch 5150 Loss 1.5040 Accuracy 0.2975\n",
      "Epoch 3 Batch 5200 Loss 1.5038 Accuracy 0.2976\n",
      "Epoch 3 Batch 5250 Loss 1.5036 Accuracy 0.2977\n",
      "Epoch 3 Batch 5300 Loss 1.5030 Accuracy 0.2978\n",
      "Epoch 3 Batch 5350 Loss 1.5026 Accuracy 0.2979\n",
      "Epoch 3 Batch 5400 Loss 1.5025 Accuracy 0.2981\n",
      "Epoch 3 Batch 5450 Loss 1.5021 Accuracy 0.2982\n",
      "Epoch 3 Batch 5500 Loss 1.5018 Accuracy 0.2983\n",
      "Epoch 3 Batch 5550 Loss 1.5015 Accuracy 0.2985\n",
      "Epoch 3 Batch 5600 Loss 1.5012 Accuracy 0.2986\n",
      "Epoch 3 Batch 5650 Loss 1.5011 Accuracy 0.2987\n",
      "Epoch 3 Batch 5700 Loss 1.5009 Accuracy 0.2989\n",
      "Epoch 3 Batch 5750 Loss 1.5005 Accuracy 0.2990\n",
      "Epoch 3 Batch 5800 Loss 1.5001 Accuracy 0.2991\n",
      "Epoch 3 Batch 5850 Loss 1.4997 Accuracy 0.2992\n",
      "Epoch 3 Batch 5900 Loss 1.4995 Accuracy 0.2993\n",
      "Epoch 3 Batch 5950 Loss 1.4994 Accuracy 0.2994\n",
      "Epoch 3 Batch 6000 Loss 1.4993 Accuracy 0.2996\n",
      "Epoch 3 Batch 6050 Loss 1.4994 Accuracy 0.2997\n",
      "Epoch 3 Batch 6100 Loss 1.4993 Accuracy 0.2998\n",
      "Epoch 3 Batch 6150 Loss 1.4990 Accuracy 0.2999\n",
      "Epoch 3 Batch 6200 Loss 1.4988 Accuracy 0.3000\n",
      "Epoch 3 Batch 6250 Loss 1.4988 Accuracy 0.3001\n",
      "Epoch 3 Batch 6300 Loss 1.4988 Accuracy 0.3002\n",
      "Epoch 3 Batch 6350 Loss 1.4984 Accuracy 0.3003\n",
      "Epoch 3 Batch 6400 Loss 1.4983 Accuracy 0.3004\n",
      "Epoch 3 Batch 6450 Loss 1.4981 Accuracy 0.3005\n",
      "Epoch 3 Batch 6500 Loss 1.4978 Accuracy 0.3005\n",
      "Epoch 3 Batch 6550 Loss 1.4976 Accuracy 0.3006\n",
      "Epoch 3 Batch 6600 Loss 1.4974 Accuracy 0.3008\n",
      "Epoch 3 Batch 6650 Loss 1.4971 Accuracy 0.3009\n",
      "Epoch 3 Batch 6700 Loss 1.4968 Accuracy 0.3009\n",
      "Epoch 3 Batch 6750 Loss 1.4966 Accuracy 0.3010\n",
      "Epoch 3 Batch 6800 Loss 1.4963 Accuracy 0.3011\n",
      "Epoch 3 Batch 6850 Loss 1.4962 Accuracy 0.3012\n",
      "Epoch 3 Batch 6900 Loss 1.4961 Accuracy 0.3013\n",
      "Epoch 3 Batch 6950 Loss 1.4958 Accuracy 0.3013\n",
      "Epoch 3 Batch 7000 Loss 1.4954 Accuracy 0.3014\n",
      "Epoch 3 Batch 7050 Loss 1.4950 Accuracy 0.3015\n",
      "Epoch 3 Batch 7100 Loss 1.4948 Accuracy 0.3016\n",
      "Epoch 3 Batch 7150 Loss 1.4945 Accuracy 0.3017\n",
      "Epoch 3 Batch 7200 Loss 1.4941 Accuracy 0.3018\n",
      "Epoch 3 Batch 7250 Loss 1.4937 Accuracy 0.3018\n",
      "Epoch 3 Batch 7300 Loss 1.4934 Accuracy 0.3019\n",
      "Epoch 3 Batch 7350 Loss 1.4930 Accuracy 0.3020\n",
      "Epoch 3 Batch 7400 Loss 1.4927 Accuracy 0.3020\n",
      "Epoch 3 Batch 7450 Loss 1.4925 Accuracy 0.3021\n",
      "Epoch 3 Batch 7500 Loss 1.4920 Accuracy 0.3021\n",
      "Epoch 3 Batch 7550 Loss 1.4915 Accuracy 0.3022\n",
      "Epoch 3 Batch 7600 Loss 1.4910 Accuracy 0.3023\n",
      "Epoch 3 Batch 7650 Loss 1.4904 Accuracy 0.3024\n",
      "Epoch 3 Batch 7700 Loss 1.4898 Accuracy 0.3024\n",
      "Epoch 3 Batch 7750 Loss 1.4893 Accuracy 0.3024\n",
      "Epoch 3 Batch 7800 Loss 1.4887 Accuracy 0.3025\n",
      "Epoch 3 Batch 7850 Loss 1.4882 Accuracy 0.3026\n",
      "Epoch 3 Batch 7900 Loss 1.4876 Accuracy 0.3026\n",
      "Epoch 3 Batch 7950 Loss 1.4870 Accuracy 0.3027\n",
      "Epoch 3 Batch 8000 Loss 1.4863 Accuracy 0.3027\n",
      "Epoch 3 Batch 8050 Loss 1.4856 Accuracy 0.3027\n",
      "Epoch 3 Batch 8100 Loss 1.4851 Accuracy 0.3028\n",
      "Epoch 3 Batch 8150 Loss 1.4846 Accuracy 0.3028\n",
      "Epoch 3 Batch 8200 Loss 1.4839 Accuracy 0.3028\n",
      "Epoch 3 Batch 8250 Loss 1.4832 Accuracy 0.3028\n",
      "Epoch 3 Batch 8300 Loss 1.4825 Accuracy 0.3028\n",
      "Epoch 3 Batch 8350 Loss 1.4818 Accuracy 0.3028\n",
      "Epoch 3 Batch 8400 Loss 1.4813 Accuracy 0.3029\n",
      "Epoch 3 Batch 8450 Loss 1.4809 Accuracy 0.3029\n",
      "Epoch 3 Batch 8500 Loss 1.4804 Accuracy 0.3030\n",
      "Epoch 3 Batch 8550 Loss 1.4798 Accuracy 0.3030\n",
      "Epoch 3 Batch 8600 Loss 1.4795 Accuracy 0.3031\n",
      "Epoch 3 Batch 8650 Loss 1.4790 Accuracy 0.3031\n",
      "Epoch 3 Batch 8700 Loss 1.4787 Accuracy 0.3031\n",
      "Epoch 3 Batch 8750 Loss 1.4784 Accuracy 0.3032\n",
      "Epoch 3 Batch 8800 Loss 1.4779 Accuracy 0.3033\n",
      "Epoch 3 Batch 8850 Loss 1.4775 Accuracy 0.3033\n",
      "Epoch 3 Batch 8900 Loss 1.4770 Accuracy 0.3033\n",
      "Epoch 3 Batch 8950 Loss 1.4766 Accuracy 0.3034\n",
      "Epoch 3 Batch 9000 Loss 1.4763 Accuracy 0.3034\n",
      "Epoch 3 Batch 9050 Loss 1.4760 Accuracy 0.3035\n",
      "Epoch 3 Batch 9100 Loss 1.4757 Accuracy 0.3036\n",
      "Epoch 3 Batch 9150 Loss 1.4752 Accuracy 0.3036\n",
      "Epoch 3 Batch 9200 Loss 1.4747 Accuracy 0.3037\n",
      "Epoch 3 Batch 9250 Loss 1.4743 Accuracy 0.3037\n",
      "Epoch 3 Batch 9300 Loss 1.4739 Accuracy 0.3038\n",
      "Epoch 3 Batch 9350 Loss 1.4735 Accuracy 0.3038\n",
      "Epoch 3 Batch 9400 Loss 1.4730 Accuracy 0.3039\n",
      "Epoch 3 Batch 9450 Loss 1.4724 Accuracy 0.3040\n",
      "Epoch 3 Batch 9500 Loss 1.4720 Accuracy 0.3040\n",
      "Epoch 3 Batch 9550 Loss 1.4715 Accuracy 0.3041\n",
      "Epoch 3 Batch 9600 Loss 1.4711 Accuracy 0.3041\n",
      "Epoch 3 Batch 9650 Loss 1.4708 Accuracy 0.3042\n",
      "Epoch 3 Batch 9700 Loss 1.4704 Accuracy 0.3043\n",
      "Epoch 3 Batch 9750 Loss 1.4700 Accuracy 0.3044\n",
      "Epoch 3 Batch 9800 Loss 1.4695 Accuracy 0.3044\n",
      "Epoch 3 Batch 9850 Loss 1.4691 Accuracy 0.3045\n",
      "Epoch 3 Batch 9900 Loss 1.4688 Accuracy 0.3046\n",
      "Epoch 3 Batch 9950 Loss 1.4684 Accuracy 0.3046\n",
      "Epoch 3 Batch 10000 Loss 1.4678 Accuracy 0.3047\n",
      "Epoch 3 Batch 10050 Loss 1.4672 Accuracy 0.3047\n",
      "Epoch 3 Batch 10100 Loss 1.4668 Accuracy 0.3048\n",
      "Epoch 3 Batch 10150 Loss 1.4665 Accuracy 0.3048\n",
      "Epoch 3 Batch 10200 Loss 1.4661 Accuracy 0.3049\n",
      "Epoch 3 Batch 10250 Loss 1.4657 Accuracy 0.3049\n",
      "Epoch 3 Batch 10300 Loss 1.4653 Accuracy 0.3050\n",
      "Epoch 3 Batch 10350 Loss 1.4650 Accuracy 0.3050\n",
      "Epoch 3 Batch 10400 Loss 1.4646 Accuracy 0.3051\n",
      "Epoch 3 Batch 10450 Loss 1.4643 Accuracy 0.3051\n",
      "Epoch 3 Batch 10500 Loss 1.4639 Accuracy 0.3052\n",
      "Epoch 3 Batch 10550 Loss 1.4638 Accuracy 0.3052\n",
      "Epoch 3 Batch 10600 Loss 1.4634 Accuracy 0.3053\n",
      "Epoch 3 Batch 10650 Loss 1.4630 Accuracy 0.3053\n",
      "Epoch 3 Batch 10700 Loss 1.4626 Accuracy 0.3054\n",
      "Epoch 3 Batch 10750 Loss 1.4621 Accuracy 0.3054\n",
      "Epoch 3 Batch 10800 Loss 1.4617 Accuracy 0.3055\n",
      "Epoch 3 Batch 10850 Loss 1.4613 Accuracy 0.3055\n",
      "Epoch 3 Batch 10900 Loss 1.4609 Accuracy 0.3056\n",
      "Epoch 3 Batch 10950 Loss 1.4605 Accuracy 0.3056\n",
      "Epoch 3 Batch 11000 Loss 1.4601 Accuracy 0.3057\n",
      "Epoch 3 Batch 11050 Loss 1.4598 Accuracy 0.3057\n",
      "Epoch 3 Batch 11100 Loss 1.4594 Accuracy 0.3058\n",
      "Epoch 3 Batch 11150 Loss 1.4592 Accuracy 0.3059\n",
      "Epoch 3 Batch 11200 Loss 1.4588 Accuracy 0.3059\n",
      "Epoch 3 Batch 11250 Loss 1.4584 Accuracy 0.3060\n",
      "Epoch 3 Batch 11300 Loss 1.4581 Accuracy 0.3061\n",
      "Epoch 3 Batch 11350 Loss 1.4577 Accuracy 0.3061\n",
      "Epoch 3 Batch 11400 Loss 1.4575 Accuracy 0.3062\n",
      "Epoch 3 Batch 11450 Loss 1.4571 Accuracy 0.3062\n",
      "Epoch 3 Batch 11500 Loss 1.4567 Accuracy 0.3063\n",
      "Epoch 3 Batch 11550 Loss 1.4563 Accuracy 0.3063\n",
      "Epoch 3 Batch 11600 Loss 1.4558 Accuracy 0.3064\n",
      "Epoch 3 Batch 11650 Loss 1.4553 Accuracy 0.3064\n",
      "Epoch 3 Batch 11700 Loss 1.4548 Accuracy 0.3064\n",
      "Epoch 3 Batch 11750 Loss 1.4543 Accuracy 0.3065\n",
      "Epoch 3 Batch 11800 Loss 1.4540 Accuracy 0.3065\n",
      "Epoch 3 Batch 11850 Loss 1.4535 Accuracy 0.3066\n",
      "Epoch 3 Batch 11900 Loss 1.4531 Accuracy 0.3066\n",
      "Epoch 3 Batch 11950 Loss 1.4526 Accuracy 0.3067\n",
      "Epoch 3 Batch 12000 Loss 1.4522 Accuracy 0.3067\n",
      "Epoch 3 Batch 12050 Loss 1.4517 Accuracy 0.3068\n",
      "Epoch 3 Batch 12100 Loss 1.4512 Accuracy 0.3068\n",
      "Epoch 3 Batch 12150 Loss 1.4509 Accuracy 0.3069\n",
      "Epoch 3 Batch 12200 Loss 1.4504 Accuracy 0.3069\n",
      "Epoch 3 Batch 12250 Loss 1.4499 Accuracy 0.3070\n",
      "Epoch 3 Batch 12300 Loss 1.4495 Accuracy 0.3070\n",
      "Epoch 3 Batch 12350 Loss 1.4491 Accuracy 0.3071\n",
      "Epoch 3 Batch 12400 Loss 1.4488 Accuracy 0.3071\n",
      "Epoch 3 Batch 12450 Loss 1.4485 Accuracy 0.3072\n",
      "Epoch 3 Batch 12500 Loss 1.4481 Accuracy 0.3072\n",
      "Epoch 3 Batch 12550 Loss 1.4478 Accuracy 0.3073\n",
      "Epoch 3 Batch 12600 Loss 1.4474 Accuracy 0.3073\n",
      "Epoch 3 Batch 12650 Loss 1.4470 Accuracy 0.3074\n",
      "Epoch 3 Batch 12700 Loss 1.4468 Accuracy 0.3074\n",
      "Epoch 3 Batch 12750 Loss 1.4465 Accuracy 0.3075\n",
      "Epoch 3 Batch 12800 Loss 1.4461 Accuracy 0.3076\n",
      "Epoch 3 Batch 12850 Loss 1.4458 Accuracy 0.3076\n",
      "Epoch 3 Batch 12900 Loss 1.4454 Accuracy 0.3077\n",
      "Epoch 3 Batch 12950 Loss 1.4452 Accuracy 0.3078\n",
      "Epoch 3 Batch 13000 Loss 1.4449 Accuracy 0.3078\n",
      "Epoch 3 Batch 13050 Loss 1.4446 Accuracy 0.3079\n",
      "Epoch 3 Batch 13100 Loss 1.4442 Accuracy 0.3080\n",
      "Epoch 3 Batch 13150 Loss 1.4439 Accuracy 0.3080\n",
      "Epoch 3 Batch 13200 Loss 1.4435 Accuracy 0.3081\n",
      "Epoch 3 Batch 13250 Loss 1.4432 Accuracy 0.3082\n",
      "Epoch 3 Batch 13300 Loss 1.4430 Accuracy 0.3082\n",
      "Epoch 3 Batch 13350 Loss 1.4426 Accuracy 0.3083\n",
      "Epoch 3 Batch 13400 Loss 1.4423 Accuracy 0.3084\n",
      "Epoch 3 Batch 13450 Loss 1.4420 Accuracy 0.3085\n",
      "Epoch 3 Batch 13500 Loss 1.4417 Accuracy 0.3085\n",
      "Epoch 3 Batch 13550 Loss 1.4414 Accuracy 0.3086\n",
      "Epoch 3 Batch 13600 Loss 1.4411 Accuracy 0.3087\n",
      "Epoch 3 Batch 13650 Loss 1.4408 Accuracy 0.3087\n",
      "Epoch 3 Batch 13700 Loss 1.4405 Accuracy 0.3088\n",
      "Epoch 3 Batch 13750 Loss 1.4402 Accuracy 0.3089\n",
      "Epoch 3 Batch 13800 Loss 1.4399 Accuracy 0.3090\n",
      "Epoch 3 Batch 13850 Loss 1.4397 Accuracy 0.3090\n",
      "Epoch 3 Batch 13900 Loss 1.4394 Accuracy 0.3091\n",
      "Epoch 3 Batch 13950 Loss 1.4391 Accuracy 0.3092\n",
      "Epoch 3 Batch 14000 Loss 1.4388 Accuracy 0.3092\n",
      "Epoch 3 Batch 14050 Loss 1.4385 Accuracy 0.3093\n",
      "Epoch 3 Batch 14100 Loss 1.4381 Accuracy 0.3094\n",
      "Epoch 3 Batch 14150 Loss 1.4378 Accuracy 0.3095\n",
      "Epoch 3 Batch 14200 Loss 1.4376 Accuracy 0.3095\n",
      "Epoch 3 Batch 14250 Loss 1.4374 Accuracy 0.3096\n",
      "Epoch 3 Batch 14300 Loss 1.4370 Accuracy 0.3097\n",
      "Epoch 3 Batch 14350 Loss 1.4367 Accuracy 0.3097\n",
      "Epoch 3 Batch 14400 Loss 1.4363 Accuracy 0.3098\n",
      "Epoch 3 Batch 14450 Loss 1.4360 Accuracy 0.3099\n",
      "Epoch 3 Batch 14500 Loss 1.4357 Accuracy 0.3099\n",
      "Epoch 3 Batch 14550 Loss 1.4355 Accuracy 0.3100\n",
      "Epoch 3 Batch 14600 Loss 1.4351 Accuracy 0.3101\n",
      "Epoch 3 Batch 14650 Loss 1.4349 Accuracy 0.3101\n",
      "Epoch 3 Batch 14700 Loss 1.4346 Accuracy 0.3102\n",
      "Epoch 3 Batch 14750 Loss 1.4345 Accuracy 0.3102\n",
      "Epoch 3 Batch 14800 Loss 1.4345 Accuracy 0.3102\n",
      "Epoch 3 Batch 14850 Loss 1.4345 Accuracy 0.3102\n",
      "Epoch 3 Batch 14900 Loss 1.4346 Accuracy 0.3102\n",
      "Epoch 3 Batch 14950 Loss 1.4349 Accuracy 0.3102\n",
      "Epoch 3 Batch 15000 Loss 1.4351 Accuracy 0.3102\n",
      "Epoch 3 Batch 15050 Loss 1.4353 Accuracy 0.3101\n",
      "Epoch 3 Batch 15100 Loss 1.4355 Accuracy 0.3101\n",
      "Epoch 3 Batch 15150 Loss 1.4357 Accuracy 0.3100\n",
      "Epoch 3 Batch 15200 Loss 1.4361 Accuracy 0.3100\n",
      "Epoch 3 Batch 15250 Loss 1.4362 Accuracy 0.3099\n",
      "Epoch 3 Batch 15300 Loss 1.4365 Accuracy 0.3099\n",
      "Epoch 3 Batch 15350 Loss 1.4368 Accuracy 0.3099\n",
      "Epoch 3 Batch 15400 Loss 1.4370 Accuracy 0.3099\n",
      "Epoch 3 Batch 15450 Loss 1.4373 Accuracy 0.3098\n",
      "Epoch 3 Batch 15500 Loss 1.4376 Accuracy 0.3098\n",
      "Epoch 3 Batch 15550 Loss 1.4379 Accuracy 0.3097\n",
      "Epoch 3 Batch 15600 Loss 1.4381 Accuracy 0.3097\n",
      "Epoch 3 Batch 15650 Loss 1.4384 Accuracy 0.3096\n",
      "Epoch 3 Batch 15700 Loss 1.4387 Accuracy 0.3096\n",
      "Epoch 3 Batch 15750 Loss 1.4390 Accuracy 0.3095\n",
      "Epoch 3 Batch 15800 Loss 1.4392 Accuracy 0.3095\n",
      "Epoch 3 Batch 15850 Loss 1.4395 Accuracy 0.3094\n",
      "Epoch 3 Batch 15900 Loss 1.4396 Accuracy 0.3094\n",
      "Epoch 3 Batch 15950 Loss 1.4399 Accuracy 0.3093\n",
      "Epoch 3 Batch 16000 Loss 1.4400 Accuracy 0.3093\n",
      "Epoch 3 Batch 16050 Loss 1.4403 Accuracy 0.3092\n",
      "Epoch 3 Batch 16100 Loss 1.4405 Accuracy 0.3092\n",
      "Epoch 3 Batch 16150 Loss 1.4406 Accuracy 0.3091\n",
      "Epoch 3 Batch 16200 Loss 1.4409 Accuracy 0.3091\n",
      "Epoch 3 Batch 16250 Loss 1.4410 Accuracy 0.3090\n",
      "Epoch 3 Batch 16300 Loss 1.4414 Accuracy 0.3090\n",
      "Epoch 3 Batch 16350 Loss 1.4415 Accuracy 0.3090\n",
      "Epoch 3 Batch 16400 Loss 1.4417 Accuracy 0.3089\n",
      "Epoch 3 Batch 16450 Loss 1.4419 Accuracy 0.3089\n",
      "Epoch 3 Batch 16500 Loss 1.4422 Accuracy 0.3088\n",
      "Epoch 3 Batch 16550 Loss 1.4423 Accuracy 0.3088\n",
      "Epoch 3 Batch 16600 Loss 1.4424 Accuracy 0.3088\n",
      "Epoch 3 Batch 16650 Loss 1.4426 Accuracy 0.3087\n",
      "Epoch 3 Batch 16700 Loss 1.4427 Accuracy 0.3087\n",
      "Epoch 3 Batch 16750 Loss 1.4429 Accuracy 0.3087\n",
      "Epoch 3 Batch 16800 Loss 1.4430 Accuracy 0.3086\n",
      "Epoch 3 Batch 16850 Loss 1.4432 Accuracy 0.3086\n",
      "Epoch 3 Batch 16900 Loss 1.4432 Accuracy 0.3086\n",
      "Epoch 3 Batch 16950 Loss 1.4433 Accuracy 0.3086\n",
      "Epoch 3 Batch 17000 Loss 1.4435 Accuracy 0.3085\n",
      "Epoch 3 Batch 17050 Loss 1.4436 Accuracy 0.3085\n",
      "Epoch 3 Batch 17100 Loss 1.4437 Accuracy 0.3085\n",
      "Epoch 3 Batch 17150 Loss 1.4438 Accuracy 0.3084\n",
      "Epoch 3 Batch 17200 Loss 1.4439 Accuracy 0.3084\n",
      "Epoch 3 Batch 17250 Loss 1.4441 Accuracy 0.3084\n",
      "Epoch 3 Batch 17300 Loss 1.4441 Accuracy 0.3084\n",
      "Epoch 3 Batch 17350 Loss 1.4442 Accuracy 0.3084\n",
      "Epoch 3 Batch 17400 Loss 1.4443 Accuracy 0.3083\n",
      "Epoch 3 Batch 17450 Loss 1.4443 Accuracy 0.3083\n",
      "Epoch 3 Batch 17500 Loss 1.4445 Accuracy 0.3083\n",
      "Epoch 3 Batch 17550 Loss 1.4446 Accuracy 0.3083\n",
      "Epoch 3 Batch 17600 Loss 1.4447 Accuracy 0.3083\n",
      "Epoch 3 Batch 17650 Loss 1.4448 Accuracy 0.3082\n",
      "Epoch 3 Batch 17700 Loss 1.4449 Accuracy 0.3082\n",
      "Epoch 3 Batch 17750 Loss 1.4450 Accuracy 0.3082\n",
      "Epoch 3 Batch 17800 Loss 1.4451 Accuracy 0.3082\n",
      "Epoch 3 Batch 17850 Loss 1.4454 Accuracy 0.3081\n",
      "Epoch 3 Batch 17900 Loss 1.4455 Accuracy 0.3081\n",
      "Epoch 3 Batch 17950 Loss 1.4457 Accuracy 0.3081\n",
      "Epoch 3 Batch 18000 Loss 1.4458 Accuracy 0.3081\n",
      "Epoch 3 Batch 18050 Loss 1.4459 Accuracy 0.3080\n",
      "Epoch 3 Batch 18100 Loss 1.4461 Accuracy 0.3080\n",
      "Epoch 3 Batch 18150 Loss 1.4463 Accuracy 0.3080\n",
      "Epoch 3 Batch 18200 Loss 1.4465 Accuracy 0.3079\n",
      "Epoch 3 Batch 18250 Loss 1.4466 Accuracy 0.3079\n",
      "Epoch 3 Batch 18300 Loss 1.4468 Accuracy 0.3079\n",
      "Epoch 3 Batch 18350 Loss 1.4468 Accuracy 0.3079\n",
      "Epoch 3 Batch 18400 Loss 1.4469 Accuracy 0.3078\n",
      "Epoch 3 Batch 18450 Loss 1.4470 Accuracy 0.3078\n",
      "Epoch 3 Batch 18500 Loss 1.4471 Accuracy 0.3078\n",
      "Epoch 3 Batch 18550 Loss 1.4472 Accuracy 0.3078\n",
      "Epoch 3 Batch 18600 Loss 1.4473 Accuracy 0.3077\n",
      "Epoch 3 Batch 18650 Loss 1.4474 Accuracy 0.3077\n",
      "Epoch 3 Batch 18700 Loss 1.4475 Accuracy 0.3077\n",
      "Epoch 3 Batch 18750 Loss 1.4476 Accuracy 0.3076\n",
      "Epoch 3 Batch 18800 Loss 1.4477 Accuracy 0.3076\n",
      "Epoch 3 Batch 18850 Loss 1.4478 Accuracy 0.3076\n",
      "Epoch 3 Batch 18900 Loss 1.4479 Accuracy 0.3075\n",
      "Epoch 3 Batch 18950 Loss 1.4479 Accuracy 0.3075\n",
      "Epoch 3 Batch 19000 Loss 1.4480 Accuracy 0.3075\n",
      "Epoch 3 Batch 19050 Loss 1.4481 Accuracy 0.3075\n",
      "Epoch 3 Batch 19100 Loss 1.4481 Accuracy 0.3074\n",
      "Epoch 3 Batch 19150 Loss 1.4482 Accuracy 0.3074\n",
      "Epoch 3 Batch 19200 Loss 1.4481 Accuracy 0.3074\n",
      "Epoch 3 Batch 19250 Loss 1.4481 Accuracy 0.3073\n",
      "Epoch 3 Batch 19300 Loss 1.4482 Accuracy 0.3073\n",
      "Epoch 3 Batch 19350 Loss 1.4482 Accuracy 0.3073\n",
      "Saving checkpoint for epoch 3 at ./tradutor_chek/ckpt-3\n",
      "Time taken for 1 epoch 15863.585892438889 secs\n",
      "\n",
      "Start or epoch 4\n",
      "Epoch 4 Batch 0 Loss 1.5557 Accuracy 0.3005\n",
      "Epoch 4 Batch 50 Loss 1.5099 Accuracy 0.2952\n",
      "Epoch 4 Batch 100 Loss 1.4920 Accuracy 0.2958\n",
      "Epoch 4 Batch 150 Loss 1.4892 Accuracy 0.2952\n",
      "Epoch 4 Batch 200 Loss 1.4883 Accuracy 0.2949\n",
      "Epoch 4 Batch 250 Loss 1.4877 Accuracy 0.2954\n",
      "Epoch 4 Batch 300 Loss 1.4886 Accuracy 0.2956\n",
      "Epoch 4 Batch 350 Loss 1.4864 Accuracy 0.2965\n",
      "Epoch 4 Batch 400 Loss 1.4854 Accuracy 0.2969\n",
      "Epoch 4 Batch 450 Loss 1.4886 Accuracy 0.2973\n",
      "Epoch 4 Batch 500 Loss 1.4894 Accuracy 0.2975\n",
      "Epoch 4 Batch 550 Loss 1.4899 Accuracy 0.2976\n",
      "Epoch 4 Batch 600 Loss 1.4899 Accuracy 0.2977\n",
      "Epoch 4 Batch 650 Loss 1.4900 Accuracy 0.2976\n",
      "Epoch 4 Batch 700 Loss 1.4902 Accuracy 0.2977\n",
      "Epoch 4 Batch 750 Loss 1.4908 Accuracy 0.2979\n",
      "Epoch 4 Batch 800 Loss 1.4912 Accuracy 0.2980\n",
      "Epoch 4 Batch 850 Loss 1.4911 Accuracy 0.2978\n",
      "Epoch 4 Batch 900 Loss 1.4906 Accuracy 0.2977\n",
      "Epoch 4 Batch 950 Loss 1.4903 Accuracy 0.2977\n",
      "Epoch 4 Batch 1000 Loss 1.4884 Accuracy 0.2978\n",
      "Epoch 4 Batch 1050 Loss 1.4880 Accuracy 0.2981\n",
      "Epoch 4 Batch 1100 Loss 1.4881 Accuracy 0.2981\n",
      "Epoch 4 Batch 1150 Loss 1.4886 Accuracy 0.2982\n",
      "Epoch 4 Batch 1200 Loss 1.4863 Accuracy 0.2982\n",
      "Epoch 4 Batch 1250 Loss 1.4851 Accuracy 0.2983\n",
      "Epoch 4 Batch 1300 Loss 1.4851 Accuracy 0.2983\n",
      "Epoch 4 Batch 1350 Loss 1.4849 Accuracy 0.2985\n",
      "Epoch 4 Batch 1400 Loss 1.4843 Accuracy 0.2984\n",
      "Epoch 4 Batch 1450 Loss 1.4837 Accuracy 0.2985\n",
      "Epoch 4 Batch 1500 Loss 1.4839 Accuracy 0.2987\n",
      "Epoch 4 Batch 1550 Loss 1.4834 Accuracy 0.2988\n",
      "Epoch 4 Batch 1600 Loss 1.4838 Accuracy 0.2988\n",
      "Epoch 4 Batch 1650 Loss 1.4833 Accuracy 0.2989\n",
      "Epoch 4 Batch 1700 Loss 1.4824 Accuracy 0.2989\n",
      "Epoch 4 Batch 1750 Loss 1.4825 Accuracy 0.2991\n",
      "Epoch 4 Batch 1800 Loss 1.4817 Accuracy 0.2992\n",
      "Epoch 4 Batch 1850 Loss 1.4812 Accuracy 0.2993\n",
      "Epoch 4 Batch 1900 Loss 1.4812 Accuracy 0.2994\n",
      "Epoch 4 Batch 1950 Loss 1.4803 Accuracy 0.2994\n",
      "Epoch 4 Batch 2000 Loss 1.4794 Accuracy 0.2995\n",
      "Epoch 4 Batch 2050 Loss 1.4786 Accuracy 0.2996\n",
      "Epoch 4 Batch 2100 Loss 1.4783 Accuracy 0.2997\n",
      "Epoch 4 Batch 2150 Loss 1.4781 Accuracy 0.2997\n",
      "Epoch 4 Batch 2200 Loss 1.4775 Accuracy 0.2997\n",
      "Epoch 4 Batch 2250 Loss 1.4770 Accuracy 0.2997\n",
      "Epoch 4 Batch 2300 Loss 1.4766 Accuracy 0.2998\n",
      "Epoch 4 Batch 2350 Loss 1.4765 Accuracy 0.2999\n",
      "Epoch 4 Batch 2400 Loss 1.4767 Accuracy 0.2999\n",
      "Epoch 4 Batch 2450 Loss 1.4766 Accuracy 0.3000\n",
      "Epoch 4 Batch 2500 Loss 1.4766 Accuracy 0.3000\n",
      "Epoch 4 Batch 2550 Loss 1.4761 Accuracy 0.3001\n",
      "Epoch 4 Batch 2600 Loss 1.4762 Accuracy 0.3002\n",
      "Epoch 4 Batch 2650 Loss 1.4758 Accuracy 0.3003\n",
      "Epoch 4 Batch 2700 Loss 1.4755 Accuracy 0.3005\n",
      "Epoch 4 Batch 2750 Loss 1.4749 Accuracy 0.3007\n",
      "Epoch 4 Batch 2800 Loss 1.4743 Accuracy 0.3009\n",
      "Epoch 4 Batch 2850 Loss 1.4736 Accuracy 0.3011\n",
      "Epoch 4 Batch 2900 Loss 1.4729 Accuracy 0.3012\n",
      "Epoch 4 Batch 2950 Loss 1.4723 Accuracy 0.3014\n",
      "Epoch 4 Batch 3000 Loss 1.4718 Accuracy 0.3016\n",
      "Epoch 4 Batch 3050 Loss 1.4711 Accuracy 0.3018\n",
      "Epoch 4 Batch 3100 Loss 1.4704 Accuracy 0.3019\n",
      "Epoch 4 Batch 3150 Loss 1.4696 Accuracy 0.3019\n",
      "Epoch 4 Batch 3200 Loss 1.4684 Accuracy 0.3021\n",
      "Epoch 4 Batch 3250 Loss 1.4675 Accuracy 0.3021\n",
      "Epoch 4 Batch 3300 Loss 1.4666 Accuracy 0.3022\n",
      "Epoch 4 Batch 3350 Loss 1.4656 Accuracy 0.3023\n",
      "Epoch 4 Batch 3400 Loss 1.4649 Accuracy 0.3024\n",
      "Epoch 4 Batch 3450 Loss 1.4641 Accuracy 0.3025\n",
      "Epoch 4 Batch 3500 Loss 1.4631 Accuracy 0.3026\n",
      "Epoch 4 Batch 3550 Loss 1.4622 Accuracy 0.3027\n",
      "Epoch 4 Batch 3600 Loss 1.4607 Accuracy 0.3028\n",
      "Epoch 4 Batch 3650 Loss 1.4595 Accuracy 0.3028\n",
      "Epoch 4 Batch 3700 Loss 1.4584 Accuracy 0.3029\n",
      "Epoch 4 Batch 3750 Loss 1.4578 Accuracy 0.3030\n",
      "Epoch 4 Batch 3800 Loss 1.4568 Accuracy 0.3031\n",
      "Epoch 4 Batch 3850 Loss 1.4559 Accuracy 0.3032\n",
      "Epoch 4 Batch 3900 Loss 1.4552 Accuracy 0.3033\n",
      "Epoch 4 Batch 3950 Loss 1.4545 Accuracy 0.3034\n",
      "Epoch 4 Batch 4000 Loss 1.4541 Accuracy 0.3035\n",
      "Epoch 4 Batch 4050 Loss 1.4536 Accuracy 0.3036\n",
      "Epoch 4 Batch 4100 Loss 1.4531 Accuracy 0.3037\n",
      "Epoch 4 Batch 4150 Loss 1.4527 Accuracy 0.3038\n",
      "Epoch 4 Batch 4200 Loss 1.4525 Accuracy 0.3040\n",
      "Epoch 4 Batch 4250 Loss 1.4521 Accuracy 0.3041\n",
      "Epoch 4 Batch 4300 Loss 1.4516 Accuracy 0.3042\n",
      "Epoch 4 Batch 4350 Loss 1.4511 Accuracy 0.3043\n",
      "Epoch 4 Batch 4400 Loss 1.4505 Accuracy 0.3044\n",
      "Epoch 4 Batch 4450 Loss 1.4504 Accuracy 0.3045\n",
      "Epoch 4 Batch 4500 Loss 1.4500 Accuracy 0.3047\n",
      "Epoch 4 Batch 4550 Loss 1.4494 Accuracy 0.3047\n",
      "Epoch 4 Batch 4600 Loss 1.4492 Accuracy 0.3049\n",
      "Epoch 4 Batch 4650 Loss 1.4486 Accuracy 0.3050\n",
      "Epoch 4 Batch 4700 Loss 1.4481 Accuracy 0.3051\n",
      "Epoch 4 Batch 4750 Loss 1.4476 Accuracy 0.3052\n",
      "Epoch 4 Batch 4800 Loss 1.4474 Accuracy 0.3053\n",
      "Epoch 4 Batch 4850 Loss 1.4469 Accuracy 0.3054\n",
      "Epoch 4 Batch 4900 Loss 1.4464 Accuracy 0.3055\n",
      "Epoch 4 Batch 4950 Loss 1.4463 Accuracy 0.3056\n",
      "Epoch 4 Batch 5000 Loss 1.4459 Accuracy 0.3057\n",
      "Epoch 4 Batch 5050 Loss 1.4457 Accuracy 0.3058\n",
      "Epoch 4 Batch 5100 Loss 1.4456 Accuracy 0.3059\n",
      "Epoch 4 Batch 5150 Loss 1.4453 Accuracy 0.3060\n",
      "Epoch 4 Batch 5200 Loss 1.4452 Accuracy 0.3061\n",
      "Epoch 4 Batch 5250 Loss 1.4452 Accuracy 0.3063\n",
      "Epoch 4 Batch 5300 Loss 1.4450 Accuracy 0.3064\n",
      "Epoch 4 Batch 5350 Loss 1.4446 Accuracy 0.3065\n",
      "Epoch 4 Batch 5400 Loss 1.4442 Accuracy 0.3067\n",
      "Epoch 4 Batch 5450 Loss 1.4439 Accuracy 0.3068\n",
      "Epoch 4 Batch 5500 Loss 1.4436 Accuracy 0.3070\n",
      "Epoch 4 Batch 5550 Loss 1.4434 Accuracy 0.3071\n",
      "Epoch 4 Batch 5600 Loss 1.4427 Accuracy 0.3072\n",
      "Epoch 4 Batch 5650 Loss 1.4427 Accuracy 0.3074\n",
      "Epoch 4 Batch 5700 Loss 1.4424 Accuracy 0.3075\n",
      "Epoch 4 Batch 5750 Loss 1.4422 Accuracy 0.3077\n",
      "Epoch 4 Batch 5800 Loss 1.4418 Accuracy 0.3078\n",
      "Epoch 4 Batch 5850 Loss 1.4415 Accuracy 0.3079\n",
      "Epoch 4 Batch 5900 Loss 1.4415 Accuracy 0.3080\n",
      "Epoch 4 Batch 5950 Loss 1.4411 Accuracy 0.3081\n",
      "Epoch 4 Batch 6000 Loss 1.4408 Accuracy 0.3082\n",
      "Epoch 4 Batch 6050 Loss 1.4409 Accuracy 0.3084\n",
      "Epoch 4 Batch 6100 Loss 1.4407 Accuracy 0.3085\n",
      "Epoch 4 Batch 6150 Loss 1.4405 Accuracy 0.3086\n",
      "Epoch 4 Batch 6200 Loss 1.4403 Accuracy 0.3087\n",
      "Epoch 4 Batch 6250 Loss 1.4402 Accuracy 0.3088\n",
      "Epoch 4 Batch 6300 Loss 1.4400 Accuracy 0.3089\n",
      "Epoch 4 Batch 6350 Loss 1.4397 Accuracy 0.3089\n",
      "Epoch 4 Batch 6400 Loss 1.4397 Accuracy 0.3090\n",
      "Epoch 4 Batch 6450 Loss 1.4395 Accuracy 0.3092\n",
      "Epoch 4 Batch 6500 Loss 1.4395 Accuracy 0.3093\n",
      "Epoch 4 Batch 6550 Loss 1.4392 Accuracy 0.3093\n",
      "Epoch 4 Batch 6600 Loss 1.4389 Accuracy 0.3094\n",
      "Epoch 4 Batch 6650 Loss 1.4389 Accuracy 0.3095\n",
      "Epoch 4 Batch 6700 Loss 1.4387 Accuracy 0.3096\n",
      "Epoch 4 Batch 6750 Loss 1.4385 Accuracy 0.3097\n",
      "Epoch 4 Batch 6800 Loss 1.4383 Accuracy 0.3098\n",
      "Epoch 4 Batch 6850 Loss 1.4380 Accuracy 0.3099\n",
      "Epoch 4 Batch 6900 Loss 1.4379 Accuracy 0.3099\n",
      "Epoch 4 Batch 6950 Loss 1.4376 Accuracy 0.3100\n",
      "Epoch 4 Batch 7000 Loss 1.4373 Accuracy 0.3101\n",
      "Epoch 4 Batch 7050 Loss 1.4369 Accuracy 0.3102\n",
      "Epoch 4 Batch 7100 Loss 1.4367 Accuracy 0.3103\n",
      "Epoch 4 Batch 7150 Loss 1.4365 Accuracy 0.3104\n",
      "Epoch 4 Batch 7200 Loss 1.4364 Accuracy 0.3104\n",
      "Epoch 4 Batch 7250 Loss 1.4362 Accuracy 0.3105\n",
      "Epoch 4 Batch 7300 Loss 1.4359 Accuracy 0.3106\n",
      "Epoch 4 Batch 7350 Loss 1.4355 Accuracy 0.3107\n",
      "Epoch 4 Batch 7400 Loss 1.4352 Accuracy 0.3107\n",
      "Epoch 4 Batch 7450 Loss 1.4349 Accuracy 0.3108\n",
      "Epoch 4 Batch 7500 Loss 1.4346 Accuracy 0.3109\n",
      "Epoch 4 Batch 7550 Loss 1.4342 Accuracy 0.3110\n",
      "Epoch 4 Batch 7600 Loss 1.4336 Accuracy 0.3111\n",
      "Epoch 4 Batch 7650 Loss 1.4330 Accuracy 0.3111\n",
      "Epoch 4 Batch 7700 Loss 1.4324 Accuracy 0.3112\n",
      "Epoch 4 Batch 7750 Loss 1.4320 Accuracy 0.3112\n",
      "Epoch 4 Batch 7800 Loss 1.4312 Accuracy 0.3113\n",
      "Epoch 4 Batch 7850 Loss 1.4305 Accuracy 0.3113\n",
      "Epoch 4 Batch 7900 Loss 1.4300 Accuracy 0.3113\n",
      "Epoch 4 Batch 7950 Loss 1.4293 Accuracy 0.3113\n",
      "Epoch 4 Batch 8000 Loss 1.4286 Accuracy 0.3113\n",
      "Epoch 4 Batch 8050 Loss 1.4282 Accuracy 0.3114\n",
      "Epoch 4 Batch 8100 Loss 1.4277 Accuracy 0.3114\n",
      "Epoch 4 Batch 8150 Loss 1.4270 Accuracy 0.3114\n",
      "Epoch 4 Batch 8200 Loss 1.4263 Accuracy 0.3115\n",
      "Epoch 4 Batch 8250 Loss 1.4256 Accuracy 0.3115\n",
      "Epoch 4 Batch 8300 Loss 1.4250 Accuracy 0.3115\n",
      "Epoch 4 Batch 8350 Loss 1.4245 Accuracy 0.3115\n",
      "Epoch 4 Batch 8400 Loss 1.4240 Accuracy 0.3115\n",
      "Epoch 4 Batch 8450 Loss 1.4235 Accuracy 0.3116\n",
      "Epoch 4 Batch 8500 Loss 1.4231 Accuracy 0.3116\n",
      "Epoch 4 Batch 8550 Loss 1.4226 Accuracy 0.3117\n",
      "Epoch 4 Batch 8600 Loss 1.4223 Accuracy 0.3117\n",
      "Epoch 4 Batch 8650 Loss 1.4220 Accuracy 0.3117\n",
      "Epoch 4 Batch 8700 Loss 1.4215 Accuracy 0.3118\n",
      "Epoch 4 Batch 8750 Loss 1.4209 Accuracy 0.3118\n",
      "Epoch 4 Batch 8800 Loss 1.4205 Accuracy 0.3119\n",
      "Epoch 4 Batch 8850 Loss 1.4201 Accuracy 0.3119\n",
      "Epoch 4 Batch 8900 Loss 1.4199 Accuracy 0.3120\n",
      "Epoch 4 Batch 8950 Loss 1.4194 Accuracy 0.3120\n",
      "Epoch 4 Batch 9000 Loss 1.4190 Accuracy 0.3121\n",
      "Epoch 4 Batch 9050 Loss 1.4186 Accuracy 0.3121\n",
      "Epoch 4 Batch 9100 Loss 1.4182 Accuracy 0.3122\n",
      "Epoch 4 Batch 9150 Loss 1.4179 Accuracy 0.3122\n",
      "Epoch 4 Batch 9200 Loss 1.4175 Accuracy 0.3123\n",
      "Epoch 4 Batch 9250 Loss 1.4172 Accuracy 0.3123\n",
      "Epoch 4 Batch 9300 Loss 1.4167 Accuracy 0.3124\n",
      "Epoch 4 Batch 9350 Loss 1.4163 Accuracy 0.3124\n",
      "Epoch 4 Batch 9400 Loss 1.4157 Accuracy 0.3125\n",
      "Epoch 4 Batch 9450 Loss 1.4154 Accuracy 0.3125\n",
      "Epoch 4 Batch 9500 Loss 1.4149 Accuracy 0.3126\n",
      "Epoch 4 Batch 9550 Loss 1.4146 Accuracy 0.3127\n",
      "Epoch 4 Batch 9600 Loss 1.4142 Accuracy 0.3127\n",
      "Epoch 4 Batch 9650 Loss 1.4139 Accuracy 0.3128\n",
      "Epoch 4 Batch 9700 Loss 1.4135 Accuracy 0.3129\n",
      "Epoch 4 Batch 9750 Loss 1.4132 Accuracy 0.3129\n",
      "Epoch 4 Batch 9800 Loss 1.4129 Accuracy 0.3130\n",
      "Epoch 4 Batch 9850 Loss 1.4126 Accuracy 0.3131\n",
      "Epoch 4 Batch 9900 Loss 1.4122 Accuracy 0.3131\n",
      "Epoch 4 Batch 9950 Loss 1.4118 Accuracy 0.3132\n",
      "Epoch 4 Batch 10000 Loss 1.4113 Accuracy 0.3132\n",
      "Epoch 4 Batch 10050 Loss 1.4109 Accuracy 0.3133\n",
      "Epoch 4 Batch 10100 Loss 1.4105 Accuracy 0.3133\n",
      "Epoch 4 Batch 10150 Loss 1.4099 Accuracy 0.3134\n",
      "Epoch 4 Batch 10200 Loss 1.4097 Accuracy 0.3134\n",
      "Epoch 4 Batch 10250 Loss 1.4093 Accuracy 0.3135\n",
      "Epoch 4 Batch 10300 Loss 1.4089 Accuracy 0.3135\n",
      "Epoch 4 Batch 10350 Loss 1.4086 Accuracy 0.3136\n",
      "Epoch 4 Batch 10400 Loss 1.4083 Accuracy 0.3136\n",
      "Epoch 4 Batch 10450 Loss 1.4081 Accuracy 0.3137\n",
      "Epoch 4 Batch 10500 Loss 1.4078 Accuracy 0.3137\n",
      "Epoch 4 Batch 10550 Loss 1.4075 Accuracy 0.3138\n",
      "Epoch 4 Batch 10600 Loss 1.4071 Accuracy 0.3138\n",
      "Epoch 4 Batch 10650 Loss 1.4067 Accuracy 0.3139\n",
      "Epoch 4 Batch 10700 Loss 1.4064 Accuracy 0.3139\n",
      "Epoch 4 Batch 10750 Loss 1.4059 Accuracy 0.3139\n",
      "Epoch 4 Batch 10800 Loss 1.4056 Accuracy 0.3140\n",
      "Epoch 4 Batch 10850 Loss 1.4054 Accuracy 0.3140\n",
      "Epoch 4 Batch 10900 Loss 1.4051 Accuracy 0.3141\n",
      "Epoch 4 Batch 10950 Loss 1.4047 Accuracy 0.3142\n",
      "Epoch 4 Batch 11000 Loss 1.4044 Accuracy 0.3142\n",
      "Epoch 4 Batch 11050 Loss 1.4041 Accuracy 0.3143\n",
      "Epoch 4 Batch 11100 Loss 1.4038 Accuracy 0.3143\n",
      "Epoch 4 Batch 11150 Loss 1.4035 Accuracy 0.3144\n",
      "Epoch 4 Batch 11200 Loss 1.4031 Accuracy 0.3145\n",
      "Epoch 4 Batch 11250 Loss 1.4028 Accuracy 0.3145\n",
      "Epoch 4 Batch 11300 Loss 1.4025 Accuracy 0.3146\n",
      "Epoch 4 Batch 11350 Loss 1.4021 Accuracy 0.3146\n",
      "Epoch 4 Batch 11400 Loss 1.4017 Accuracy 0.3147\n",
      "Epoch 4 Batch 11450 Loss 1.4013 Accuracy 0.3148\n",
      "Epoch 4 Batch 11500 Loss 1.4009 Accuracy 0.3148\n",
      "Epoch 4 Batch 11550 Loss 1.4006 Accuracy 0.3148\n",
      "Epoch 4 Batch 11600 Loss 1.4001 Accuracy 0.3148\n",
      "Epoch 4 Batch 11650 Loss 1.3997 Accuracy 0.3149\n",
      "Epoch 4 Batch 11700 Loss 1.3992 Accuracy 0.3149\n",
      "Epoch 4 Batch 11750 Loss 1.3988 Accuracy 0.3149\n",
      "Epoch 4 Batch 11800 Loss 1.3983 Accuracy 0.3150\n",
      "Epoch 4 Batch 11850 Loss 1.3978 Accuracy 0.3150\n",
      "Epoch 4 Batch 11900 Loss 1.3974 Accuracy 0.3151\n",
      "Epoch 4 Batch 11950 Loss 1.3971 Accuracy 0.3151\n",
      "Epoch 4 Batch 12000 Loss 1.3966 Accuracy 0.3152\n",
      "Epoch 4 Batch 12050 Loss 1.3962 Accuracy 0.3152\n",
      "Epoch 4 Batch 12100 Loss 1.3957 Accuracy 0.3152\n",
      "Epoch 4 Batch 12150 Loss 1.3953 Accuracy 0.3153\n",
      "Epoch 4 Batch 12200 Loss 1.3949 Accuracy 0.3153\n",
      "Epoch 4 Batch 12250 Loss 1.3946 Accuracy 0.3154\n",
      "Epoch 4 Batch 12300 Loss 1.3942 Accuracy 0.3154\n",
      "Epoch 4 Batch 12350 Loss 1.3938 Accuracy 0.3155\n",
      "Epoch 4 Batch 12400 Loss 1.3935 Accuracy 0.3155\n",
      "Epoch 4 Batch 12450 Loss 1.3931 Accuracy 0.3155\n",
      "Epoch 4 Batch 12500 Loss 1.3927 Accuracy 0.3156\n",
      "Epoch 4 Batch 12550 Loss 1.3924 Accuracy 0.3156\n",
      "Epoch 4 Batch 12600 Loss 1.3921 Accuracy 0.3157\n",
      "Epoch 4 Batch 12650 Loss 1.3918 Accuracy 0.3157\n",
      "Epoch 4 Batch 12700 Loss 1.3914 Accuracy 0.3158\n",
      "Epoch 4 Batch 12750 Loss 1.3911 Accuracy 0.3159\n",
      "Epoch 4 Batch 12800 Loss 1.3908 Accuracy 0.3159\n",
      "Epoch 4 Batch 12850 Loss 1.3906 Accuracy 0.3160\n",
      "Epoch 4 Batch 12900 Loss 1.3903 Accuracy 0.3161\n",
      "Epoch 4 Batch 12950 Loss 1.3901 Accuracy 0.3161\n",
      "Epoch 4 Batch 13000 Loss 1.3897 Accuracy 0.3162\n",
      "Epoch 4 Batch 13050 Loss 1.3894 Accuracy 0.3162\n",
      "Epoch 4 Batch 13100 Loss 1.3891 Accuracy 0.3163\n",
      "Epoch 4 Batch 13150 Loss 1.3888 Accuracy 0.3164\n",
      "Epoch 4 Batch 13200 Loss 1.3885 Accuracy 0.3164\n",
      "Epoch 4 Batch 13250 Loss 1.3882 Accuracy 0.3165\n",
      "Epoch 4 Batch 13300 Loss 1.3879 Accuracy 0.3166\n",
      "Epoch 4 Batch 13350 Loss 1.3876 Accuracy 0.3167\n",
      "Epoch 4 Batch 13400 Loss 1.3873 Accuracy 0.3167\n",
      "Epoch 4 Batch 13450 Loss 1.3870 Accuracy 0.3168\n",
      "Epoch 4 Batch 13500 Loss 1.3867 Accuracy 0.3169\n",
      "Epoch 4 Batch 13550 Loss 1.3864 Accuracy 0.3169\n",
      "Epoch 4 Batch 13600 Loss 1.3862 Accuracy 0.3170\n",
      "Epoch 4 Batch 13650 Loss 1.3859 Accuracy 0.3171\n",
      "Epoch 4 Batch 13700 Loss 1.3856 Accuracy 0.3171\n",
      "Epoch 4 Batch 13750 Loss 1.3854 Accuracy 0.3172\n",
      "Epoch 4 Batch 13800 Loss 1.3851 Accuracy 0.3173\n",
      "Epoch 4 Batch 13850 Loss 1.3849 Accuracy 0.3173\n",
      "Epoch 4 Batch 13900 Loss 1.3846 Accuracy 0.3174\n",
      "Epoch 4 Batch 13950 Loss 1.3845 Accuracy 0.3175\n",
      "Epoch 4 Batch 14000 Loss 1.3841 Accuracy 0.3176\n",
      "Epoch 4 Batch 14050 Loss 1.3839 Accuracy 0.3176\n",
      "Epoch 4 Batch 14100 Loss 1.3835 Accuracy 0.3177\n",
      "Epoch 4 Batch 14150 Loss 1.3833 Accuracy 0.3178\n",
      "Epoch 4 Batch 14200 Loss 1.3830 Accuracy 0.3178\n",
      "Epoch 4 Batch 14250 Loss 1.3827 Accuracy 0.3179\n",
      "Epoch 4 Batch 14300 Loss 1.3824 Accuracy 0.3180\n",
      "Epoch 4 Batch 14350 Loss 1.3822 Accuracy 0.3180\n",
      "Epoch 4 Batch 14400 Loss 1.3819 Accuracy 0.3181\n",
      "Epoch 4 Batch 14450 Loss 1.3817 Accuracy 0.3182\n",
      "Epoch 4 Batch 14500 Loss 1.3814 Accuracy 0.3182\n",
      "Epoch 4 Batch 14550 Loss 1.3811 Accuracy 0.3183\n",
      "Epoch 4 Batch 14600 Loss 1.3809 Accuracy 0.3184\n",
      "Epoch 4 Batch 14650 Loss 1.3805 Accuracy 0.3184\n",
      "Epoch 4 Batch 14700 Loss 1.3802 Accuracy 0.3184\n",
      "Epoch 4 Batch 14750 Loss 1.3801 Accuracy 0.3185\n",
      "Epoch 4 Batch 14800 Loss 1.3802 Accuracy 0.3185\n",
      "Epoch 4 Batch 14850 Loss 1.3803 Accuracy 0.3185\n",
      "Epoch 4 Batch 14900 Loss 1.3804 Accuracy 0.3185\n",
      "Epoch 4 Batch 14950 Loss 1.3807 Accuracy 0.3184\n",
      "Epoch 4 Batch 15000 Loss 1.3808 Accuracy 0.3184\n",
      "Epoch 4 Batch 15050 Loss 1.3810 Accuracy 0.3184\n",
      "Epoch 4 Batch 15100 Loss 1.3813 Accuracy 0.3183\n",
      "Epoch 4 Batch 15150 Loss 1.3816 Accuracy 0.3183\n",
      "Epoch 4 Batch 15200 Loss 1.3819 Accuracy 0.3182\n",
      "Epoch 4 Batch 15250 Loss 1.3821 Accuracy 0.3182\n",
      "Epoch 4 Batch 15300 Loss 1.3825 Accuracy 0.3181\n",
      "Epoch 4 Batch 15350 Loss 1.3827 Accuracy 0.3181\n",
      "Epoch 4 Batch 15400 Loss 1.3832 Accuracy 0.3180\n",
      "Epoch 4 Batch 15450 Loss 1.3834 Accuracy 0.3180\n",
      "Epoch 4 Batch 15500 Loss 1.3837 Accuracy 0.3180\n",
      "Epoch 4 Batch 15550 Loss 1.3841 Accuracy 0.3179\n",
      "Epoch 4 Batch 15600 Loss 1.3843 Accuracy 0.3179\n",
      "Epoch 4 Batch 15650 Loss 1.3846 Accuracy 0.3178\n",
      "Epoch 4 Batch 15700 Loss 1.3849 Accuracy 0.3178\n",
      "Epoch 4 Batch 15750 Loss 1.3852 Accuracy 0.3177\n",
      "Epoch 4 Batch 15800 Loss 1.3855 Accuracy 0.3177\n",
      "Epoch 4 Batch 15850 Loss 1.3858 Accuracy 0.3176\n",
      "Epoch 4 Batch 15900 Loss 1.3860 Accuracy 0.3175\n",
      "Epoch 4 Batch 15950 Loss 1.3863 Accuracy 0.3175\n",
      "Epoch 4 Batch 16000 Loss 1.3865 Accuracy 0.3174\n",
      "Epoch 4 Batch 16050 Loss 1.3867 Accuracy 0.3174\n",
      "Epoch 4 Batch 16100 Loss 1.3870 Accuracy 0.3173\n",
      "Epoch 4 Batch 16150 Loss 1.3872 Accuracy 0.3173\n",
      "Epoch 4 Batch 16200 Loss 1.3875 Accuracy 0.3172\n",
      "Epoch 4 Batch 16250 Loss 1.3878 Accuracy 0.3171\n",
      "Epoch 4 Batch 16300 Loss 1.3880 Accuracy 0.3171\n",
      "Epoch 4 Batch 16350 Loss 1.3882 Accuracy 0.3170\n",
      "Epoch 4 Batch 16400 Loss 1.3884 Accuracy 0.3170\n",
      "Epoch 4 Batch 16450 Loss 1.3886 Accuracy 0.3170\n",
      "Epoch 4 Batch 16500 Loss 1.3889 Accuracy 0.3169\n",
      "Epoch 4 Batch 16550 Loss 1.3891 Accuracy 0.3169\n",
      "Epoch 4 Batch 16600 Loss 1.3893 Accuracy 0.3168\n",
      "Epoch 4 Batch 16650 Loss 1.3894 Accuracy 0.3168\n",
      "Epoch 4 Batch 16700 Loss 1.3896 Accuracy 0.3167\n",
      "Epoch 4 Batch 16750 Loss 1.3898 Accuracy 0.3167\n",
      "Epoch 4 Batch 16800 Loss 1.3899 Accuracy 0.3167\n",
      "Epoch 4 Batch 16850 Loss 1.3901 Accuracy 0.3166\n",
      "Epoch 4 Batch 16900 Loss 1.3903 Accuracy 0.3166\n",
      "Epoch 4 Batch 16950 Loss 1.3904 Accuracy 0.3166\n",
      "Epoch 4 Batch 17000 Loss 1.3905 Accuracy 0.3166\n",
      "Epoch 4 Batch 17050 Loss 1.3907 Accuracy 0.3165\n",
      "Epoch 4 Batch 17100 Loss 1.3908 Accuracy 0.3165\n",
      "Epoch 4 Batch 17150 Loss 1.3910 Accuracy 0.3165\n",
      "Epoch 4 Batch 17200 Loss 1.3911 Accuracy 0.3164\n",
      "Epoch 4 Batch 17250 Loss 1.3912 Accuracy 0.3164\n",
      "Epoch 4 Batch 17300 Loss 1.3913 Accuracy 0.3164\n",
      "Epoch 4 Batch 17350 Loss 1.3915 Accuracy 0.3164\n",
      "Epoch 4 Batch 17400 Loss 1.3916 Accuracy 0.3163\n",
      "Epoch 4 Batch 17450 Loss 1.3917 Accuracy 0.3163\n",
      "Epoch 4 Batch 17500 Loss 1.3918 Accuracy 0.3163\n",
      "Epoch 4 Batch 17550 Loss 1.3920 Accuracy 0.3163\n",
      "Epoch 4 Batch 17600 Loss 1.3922 Accuracy 0.3163\n",
      "Epoch 4 Batch 17650 Loss 1.3924 Accuracy 0.3162\n",
      "Epoch 4 Batch 17700 Loss 1.3925 Accuracy 0.3162\n",
      "Epoch 4 Batch 17750 Loss 1.3927 Accuracy 0.3162\n",
      "Epoch 4 Batch 17800 Loss 1.3929 Accuracy 0.3161\n",
      "Epoch 4 Batch 17850 Loss 1.3931 Accuracy 0.3161\n",
      "Epoch 4 Batch 17900 Loss 1.3934 Accuracy 0.3161\n",
      "Epoch 4 Batch 17950 Loss 1.3935 Accuracy 0.3161\n",
      "Epoch 4 Batch 18000 Loss 1.3937 Accuracy 0.3160\n",
      "Epoch 4 Batch 18050 Loss 1.3938 Accuracy 0.3160\n",
      "Epoch 4 Batch 18100 Loss 1.3940 Accuracy 0.3159\n",
      "Epoch 4 Batch 18150 Loss 1.3942 Accuracy 0.3159\n",
      "Epoch 4 Batch 18200 Loss 1.3945 Accuracy 0.3159\n",
      "Epoch 4 Batch 18250 Loss 1.3946 Accuracy 0.3158\n",
      "Epoch 4 Batch 18300 Loss 1.3948 Accuracy 0.3158\n",
      "Epoch 4 Batch 18350 Loss 1.3949 Accuracy 0.3158\n",
      "Epoch 4 Batch 18400 Loss 1.3951 Accuracy 0.3158\n",
      "Epoch 4 Batch 18450 Loss 1.3953 Accuracy 0.3157\n",
      "Epoch 4 Batch 18500 Loss 1.3954 Accuracy 0.3157\n",
      "Epoch 4 Batch 18550 Loss 1.3955 Accuracy 0.3157\n",
      "Epoch 4 Batch 18600 Loss 1.3956 Accuracy 0.3156\n",
      "Epoch 4 Batch 18650 Loss 1.3957 Accuracy 0.3156\n",
      "Epoch 4 Batch 18700 Loss 1.3958 Accuracy 0.3155\n",
      "Epoch 4 Batch 18750 Loss 1.3960 Accuracy 0.3155\n",
      "Epoch 4 Batch 18800 Loss 1.3961 Accuracy 0.3155\n",
      "Epoch 4 Batch 18850 Loss 1.3962 Accuracy 0.3154\n",
      "Epoch 4 Batch 18900 Loss 1.3963 Accuracy 0.3154\n",
      "Epoch 4 Batch 18950 Loss 1.3964 Accuracy 0.3153\n",
      "Epoch 4 Batch 19000 Loss 1.3966 Accuracy 0.3153\n",
      "Epoch 4 Batch 19050 Loss 1.3967 Accuracy 0.3153\n",
      "Epoch 4 Batch 19100 Loss 1.3967 Accuracy 0.3152\n",
      "Epoch 4 Batch 19150 Loss 1.3968 Accuracy 0.3152\n",
      "Epoch 4 Batch 19200 Loss 1.3968 Accuracy 0.3152\n",
      "Epoch 4 Batch 19250 Loss 1.3969 Accuracy 0.3152\n",
      "Epoch 4 Batch 19300 Loss 1.3969 Accuracy 0.3151\n",
      "Epoch 4 Batch 19350 Loss 1.3970 Accuracy 0.3151\n",
      "Saving checkpoint for epoch 4 at ./tradutor_chek/ckpt-4\n",
      "Time taken for 1 epoch 15860.072555065155 secs\n",
      "\n",
      "Start or epoch 5\n",
      "Epoch 5 Batch 0 Loss 1.4227 Accuracy 0.3001\n",
      "Epoch 5 Batch 50 Loss 1.4889 Accuracy 0.3036\n",
      "Epoch 5 Batch 100 Loss 1.4706 Accuracy 0.3015\n",
      "Epoch 5 Batch 150 Loss 1.4597 Accuracy 0.3017\n",
      "Epoch 5 Batch 200 Loss 1.4558 Accuracy 0.3019\n",
      "Epoch 5 Batch 250 Loss 1.4536 Accuracy 0.3026\n",
      "Epoch 5 Batch 300 Loss 1.4512 Accuracy 0.3032\n",
      "Epoch 5 Batch 350 Loss 1.4469 Accuracy 0.3029\n",
      "Epoch 5 Batch 400 Loss 1.4479 Accuracy 0.3033\n",
      "Epoch 5 Batch 450 Loss 1.4490 Accuracy 0.3034\n",
      "Epoch 5 Batch 500 Loss 1.4491 Accuracy 0.3036\n",
      "Epoch 5 Batch 550 Loss 1.4492 Accuracy 0.3037\n",
      "Epoch 5 Batch 600 Loss 1.4503 Accuracy 0.3039\n",
      "Epoch 5 Batch 650 Loss 1.4502 Accuracy 0.3041\n",
      "Epoch 5 Batch 700 Loss 1.4507 Accuracy 0.3043\n",
      "Epoch 5 Batch 750 Loss 1.4497 Accuracy 0.3042\n",
      "Epoch 5 Batch 800 Loss 1.4493 Accuracy 0.3042\n",
      "Epoch 5 Batch 850 Loss 1.4492 Accuracy 0.3042\n",
      "Epoch 5 Batch 900 Loss 1.4483 Accuracy 0.3039\n",
      "Epoch 5 Batch 950 Loss 1.4481 Accuracy 0.3040\n",
      "Epoch 5 Batch 1000 Loss 1.4480 Accuracy 0.3041\n",
      "Epoch 5 Batch 1050 Loss 1.4473 Accuracy 0.3041\n",
      "Epoch 5 Batch 1100 Loss 1.4463 Accuracy 0.3043\n",
      "Epoch 5 Batch 1150 Loss 1.4452 Accuracy 0.3043\n",
      "Epoch 5 Batch 1200 Loss 1.4439 Accuracy 0.3044\n",
      "Epoch 5 Batch 1250 Loss 1.4429 Accuracy 0.3045\n",
      "Epoch 5 Batch 1300 Loss 1.4434 Accuracy 0.3046\n",
      "Epoch 5 Batch 1350 Loss 1.4429 Accuracy 0.3046\n",
      "Epoch 5 Batch 1400 Loss 1.4424 Accuracy 0.3046\n",
      "Epoch 5 Batch 1450 Loss 1.4421 Accuracy 0.3047\n",
      "Epoch 5 Batch 1500 Loss 1.4420 Accuracy 0.3048\n",
      "Epoch 5 Batch 1550 Loss 1.4419 Accuracy 0.3049\n",
      "Epoch 5 Batch 1600 Loss 1.4420 Accuracy 0.3050\n",
      "Epoch 5 Batch 1650 Loss 1.4417 Accuracy 0.3051\n",
      "Epoch 5 Batch 1700 Loss 1.4413 Accuracy 0.3051\n",
      "Epoch 5 Batch 1750 Loss 1.4405 Accuracy 0.3053\n",
      "Epoch 5 Batch 1800 Loss 1.4396 Accuracy 0.3053\n",
      "Epoch 5 Batch 1850 Loss 1.4388 Accuracy 0.3053\n",
      "Epoch 5 Batch 1900 Loss 1.4386 Accuracy 0.3054\n",
      "Epoch 5 Batch 1950 Loss 1.4380 Accuracy 0.3055\n",
      "Epoch 5 Batch 2000 Loss 1.4381 Accuracy 0.3056\n",
      "Epoch 5 Batch 2050 Loss 1.4369 Accuracy 0.3056\n",
      "Epoch 5 Batch 2100 Loss 1.4368 Accuracy 0.3057\n",
      "Epoch 5 Batch 2150 Loss 1.4370 Accuracy 0.3058\n",
      "Epoch 5 Batch 2200 Loss 1.4376 Accuracy 0.3059\n",
      "Epoch 5 Batch 2250 Loss 1.4375 Accuracy 0.3059\n",
      "Epoch 5 Batch 2300 Loss 1.4369 Accuracy 0.3058\n",
      "Epoch 5 Batch 2350 Loss 1.4364 Accuracy 0.3058\n",
      "Epoch 5 Batch 2400 Loss 1.4363 Accuracy 0.3059\n",
      "Epoch 5 Batch 2450 Loss 1.4364 Accuracy 0.3060\n",
      "Epoch 5 Batch 2500 Loss 1.4366 Accuracy 0.3062\n",
      "Epoch 5 Batch 2550 Loss 1.4365 Accuracy 0.3063\n",
      "Epoch 5 Batch 2600 Loss 1.4362 Accuracy 0.3065\n",
      "Epoch 5 Batch 2650 Loss 1.4357 Accuracy 0.3066\n",
      "Epoch 5 Batch 2700 Loss 1.4352 Accuracy 0.3067\n",
      "Epoch 5 Batch 2750 Loss 1.4348 Accuracy 0.3069\n",
      "Epoch 5 Batch 2800 Loss 1.4347 Accuracy 0.3071\n",
      "Epoch 5 Batch 2850 Loss 1.4341 Accuracy 0.3073\n",
      "Epoch 5 Batch 2900 Loss 1.4333 Accuracy 0.3074\n",
      "Epoch 5 Batch 2950 Loss 1.4323 Accuracy 0.3076\n",
      "Epoch 5 Batch 3000 Loss 1.4316 Accuracy 0.3077\n",
      "Epoch 5 Batch 3050 Loss 1.4307 Accuracy 0.3079\n",
      "Epoch 5 Batch 3100 Loss 1.4303 Accuracy 0.3080\n",
      "Epoch 5 Batch 3150 Loss 1.4291 Accuracy 0.3082\n",
      "Epoch 5 Batch 3200 Loss 1.4285 Accuracy 0.3083\n",
      "Epoch 5 Batch 3250 Loss 1.4279 Accuracy 0.3085\n",
      "Epoch 5 Batch 3300 Loss 1.4269 Accuracy 0.3086\n",
      "Epoch 5 Batch 3350 Loss 1.4257 Accuracy 0.3087\n",
      "Epoch 5 Batch 3400 Loss 1.4249 Accuracy 0.3088\n",
      "Epoch 5 Batch 3450 Loss 1.4239 Accuracy 0.3089\n",
      "Epoch 5 Batch 3500 Loss 1.4230 Accuracy 0.3090\n",
      "Epoch 5 Batch 3550 Loss 1.4218 Accuracy 0.3091\n",
      "Epoch 5 Batch 3600 Loss 1.4206 Accuracy 0.3092\n",
      "Epoch 5 Batch 3650 Loss 1.4199 Accuracy 0.3092\n",
      "Epoch 5 Batch 3700 Loss 1.4187 Accuracy 0.3093\n",
      "Epoch 5 Batch 3750 Loss 1.4176 Accuracy 0.3094\n",
      "Epoch 5 Batch 3800 Loss 1.4164 Accuracy 0.3095\n",
      "Epoch 5 Batch 3850 Loss 1.4157 Accuracy 0.3096\n",
      "Epoch 5 Batch 3900 Loss 1.4148 Accuracy 0.3097\n",
      "Epoch 5 Batch 3950 Loss 1.4143 Accuracy 0.3098\n",
      "Epoch 5 Batch 4000 Loss 1.4141 Accuracy 0.3099\n",
      "Epoch 5 Batch 4050 Loss 1.4134 Accuracy 0.3101\n",
      "Epoch 5 Batch 4100 Loss 1.4125 Accuracy 0.3101\n",
      "Epoch 5 Batch 4150 Loss 1.4122 Accuracy 0.3102\n",
      "Epoch 5 Batch 4200 Loss 1.4117 Accuracy 0.3103\n",
      "Epoch 5 Batch 4250 Loss 1.4113 Accuracy 0.3104\n",
      "Epoch 5 Batch 4300 Loss 1.4108 Accuracy 0.3105\n",
      "Epoch 5 Batch 4350 Loss 1.4107 Accuracy 0.3107\n",
      "Epoch 5 Batch 4400 Loss 1.4103 Accuracy 0.3108\n",
      "Epoch 5 Batch 4450 Loss 1.4099 Accuracy 0.3109\n",
      "Epoch 5 Batch 4500 Loss 1.4096 Accuracy 0.3110\n",
      "Epoch 5 Batch 4550 Loss 1.4092 Accuracy 0.3111\n",
      "Epoch 5 Batch 4600 Loss 1.4087 Accuracy 0.3113\n",
      "Epoch 5 Batch 4650 Loss 1.4085 Accuracy 0.3114\n",
      "Epoch 5 Batch 4700 Loss 1.4080 Accuracy 0.3115\n",
      "Epoch 5 Batch 4750 Loss 1.4074 Accuracy 0.3116\n",
      "Epoch 5 Batch 4800 Loss 1.4073 Accuracy 0.3117\n",
      "Epoch 5 Batch 4850 Loss 1.4071 Accuracy 0.3118\n",
      "Epoch 5 Batch 4900 Loss 1.4069 Accuracy 0.3118\n",
      "Epoch 5 Batch 4950 Loss 1.4067 Accuracy 0.3119\n",
      "Epoch 5 Batch 5000 Loss 1.4062 Accuracy 0.3120\n",
      "Epoch 5 Batch 5050 Loss 1.4061 Accuracy 0.3122\n",
      "Epoch 5 Batch 5100 Loss 1.4058 Accuracy 0.3123\n",
      "Epoch 5 Batch 5150 Loss 1.4055 Accuracy 0.3124\n",
      "Epoch 5 Batch 5200 Loss 1.4053 Accuracy 0.3125\n",
      "Epoch 5 Batch 5250 Loss 1.4050 Accuracy 0.3126\n",
      "Epoch 5 Batch 5300 Loss 1.4048 Accuracy 0.3127\n",
      "Epoch 5 Batch 5350 Loss 1.4045 Accuracy 0.3128\n",
      "Epoch 5 Batch 5400 Loss 1.4040 Accuracy 0.3130\n",
      "Epoch 5 Batch 5450 Loss 1.4037 Accuracy 0.3131\n",
      "Epoch 5 Batch 5500 Loss 1.4030 Accuracy 0.3133\n",
      "Epoch 5 Batch 5550 Loss 1.4029 Accuracy 0.3134\n",
      "Epoch 5 Batch 5600 Loss 1.4027 Accuracy 0.3136\n",
      "Epoch 5 Batch 5650 Loss 1.4023 Accuracy 0.3137\n",
      "Epoch 5 Batch 5700 Loss 1.4020 Accuracy 0.3139\n",
      "Epoch 5 Batch 5750 Loss 1.4018 Accuracy 0.3140\n",
      "Epoch 5 Batch 5800 Loss 1.4016 Accuracy 0.3141\n",
      "Epoch 5 Batch 5850 Loss 1.4014 Accuracy 0.3142\n",
      "Epoch 5 Batch 5900 Loss 1.4013 Accuracy 0.3143\n",
      "Epoch 5 Batch 5950 Loss 1.4013 Accuracy 0.3145\n",
      "Epoch 5 Batch 6000 Loss 1.4009 Accuracy 0.3146\n",
      "Epoch 5 Batch 6050 Loss 1.4008 Accuracy 0.3147\n",
      "Epoch 5 Batch 6100 Loss 1.4005 Accuracy 0.3148\n",
      "Epoch 5 Batch 6150 Loss 1.4004 Accuracy 0.3149\n",
      "Epoch 5 Batch 6200 Loss 1.4002 Accuracy 0.3150\n",
      "Epoch 5 Batch 6250 Loss 1.4000 Accuracy 0.3151\n",
      "Epoch 5 Batch 6300 Loss 1.3999 Accuracy 0.3152\n",
      "Epoch 5 Batch 6350 Loss 1.3999 Accuracy 0.3153\n",
      "Epoch 5 Batch 6400 Loss 1.3997 Accuracy 0.3154\n",
      "Epoch 5 Batch 6450 Loss 1.3995 Accuracy 0.3155\n",
      "Epoch 5 Batch 6500 Loss 1.3993 Accuracy 0.3156\n",
      "Epoch 5 Batch 6550 Loss 1.3991 Accuracy 0.3157\n",
      "Epoch 5 Batch 6600 Loss 1.3988 Accuracy 0.3158\n",
      "Epoch 5 Batch 6650 Loss 1.3988 Accuracy 0.3159\n",
      "Epoch 5 Batch 6700 Loss 1.3986 Accuracy 0.3160\n",
      "Epoch 5 Batch 6750 Loss 1.3985 Accuracy 0.3161\n",
      "Epoch 5 Batch 6800 Loss 1.3982 Accuracy 0.3161\n",
      "Epoch 5 Batch 6850 Loss 1.3978 Accuracy 0.3162\n",
      "Epoch 5 Batch 6900 Loss 1.3976 Accuracy 0.3163\n",
      "Epoch 5 Batch 6950 Loss 1.3971 Accuracy 0.3163\n",
      "Epoch 5 Batch 7000 Loss 1.3969 Accuracy 0.3164\n",
      "Epoch 5 Batch 7050 Loss 1.3965 Accuracy 0.3165\n",
      "Epoch 5 Batch 7100 Loss 1.3964 Accuracy 0.3166\n",
      "Epoch 5 Batch 7150 Loss 1.3961 Accuracy 0.3167\n",
      "Epoch 5 Batch 7200 Loss 1.3957 Accuracy 0.3168\n",
      "Epoch 5 Batch 7250 Loss 1.3955 Accuracy 0.3169\n",
      "Epoch 5 Batch 7300 Loss 1.3952 Accuracy 0.3170\n",
      "Epoch 5 Batch 7350 Loss 1.3948 Accuracy 0.3170\n",
      "Epoch 5 Batch 7400 Loss 1.3946 Accuracy 0.3171\n",
      "Epoch 5 Batch 7450 Loss 1.3944 Accuracy 0.3172\n",
      "Epoch 5 Batch 7500 Loss 1.3939 Accuracy 0.3172\n",
      "Epoch 5 Batch 7550 Loss 1.3935 Accuracy 0.3173\n",
      "Epoch 5 Batch 7600 Loss 1.3931 Accuracy 0.3174\n",
      "Epoch 5 Batch 7650 Loss 1.3925 Accuracy 0.3174\n",
      "Epoch 5 Batch 7700 Loss 1.3921 Accuracy 0.3175\n",
      "Epoch 5 Batch 7750 Loss 1.3916 Accuracy 0.3175\n",
      "Epoch 5 Batch 7800 Loss 1.3910 Accuracy 0.3175\n",
      "Epoch 5 Batch 7850 Loss 1.3904 Accuracy 0.3176\n",
      "Epoch 5 Batch 7900 Loss 1.3898 Accuracy 0.3176\n",
      "Epoch 5 Batch 7950 Loss 1.3892 Accuracy 0.3177\n",
      "Epoch 5 Batch 8000 Loss 1.3884 Accuracy 0.3177\n",
      "Epoch 5 Batch 8050 Loss 1.3877 Accuracy 0.3177\n",
      "Epoch 5 Batch 8100 Loss 1.3871 Accuracy 0.3177\n",
      "Epoch 5 Batch 8150 Loss 1.3865 Accuracy 0.3177\n",
      "Epoch 5 Batch 8200 Loss 1.3859 Accuracy 0.3177\n",
      "Epoch 5 Batch 8250 Loss 1.3855 Accuracy 0.3178\n",
      "Epoch 5 Batch 8300 Loss 1.3849 Accuracy 0.3178\n",
      "Epoch 5 Batch 8350 Loss 1.3844 Accuracy 0.3178\n",
      "Epoch 5 Batch 8400 Loss 1.3837 Accuracy 0.3178\n",
      "Epoch 5 Batch 8450 Loss 1.3833 Accuracy 0.3179\n",
      "Epoch 5 Batch 8500 Loss 1.3829 Accuracy 0.3179\n",
      "Epoch 5 Batch 8550 Loss 1.3825 Accuracy 0.3179\n",
      "Epoch 5 Batch 8600 Loss 1.3821 Accuracy 0.3180\n",
      "Epoch 5 Batch 8650 Loss 1.3819 Accuracy 0.3180\n",
      "Epoch 5 Batch 8700 Loss 1.3815 Accuracy 0.3181\n",
      "Epoch 5 Batch 8750 Loss 1.3813 Accuracy 0.3181\n",
      "Epoch 5 Batch 8800 Loss 1.3810 Accuracy 0.3182\n",
      "Epoch 5 Batch 8850 Loss 1.3806 Accuracy 0.3182\n",
      "Epoch 5 Batch 8900 Loss 1.3802 Accuracy 0.3183\n",
      "Epoch 5 Batch 8950 Loss 1.3798 Accuracy 0.3183\n",
      "Epoch 5 Batch 9000 Loss 1.3793 Accuracy 0.3184\n",
      "Epoch 5 Batch 9050 Loss 1.3791 Accuracy 0.3184\n",
      "Epoch 5 Batch 9100 Loss 1.3786 Accuracy 0.3185\n",
      "Epoch 5 Batch 9150 Loss 1.3781 Accuracy 0.3185\n",
      "Epoch 5 Batch 9200 Loss 1.3777 Accuracy 0.3186\n",
      "Epoch 5 Batch 9250 Loss 1.3773 Accuracy 0.3186\n",
      "Epoch 5 Batch 9300 Loss 1.3770 Accuracy 0.3187\n",
      "Epoch 5 Batch 9350 Loss 1.3766 Accuracy 0.3187\n",
      "Epoch 5 Batch 9400 Loss 1.3762 Accuracy 0.3188\n",
      "Epoch 5 Batch 9450 Loss 1.3758 Accuracy 0.3188\n",
      "Epoch 5 Batch 9500 Loss 1.3754 Accuracy 0.3189\n",
      "Epoch 5 Batch 9550 Loss 1.3750 Accuracy 0.3189\n",
      "Epoch 5 Batch 9600 Loss 1.3746 Accuracy 0.3190\n",
      "Epoch 5 Batch 9650 Loss 1.3742 Accuracy 0.3191\n",
      "Epoch 5 Batch 9700 Loss 1.3739 Accuracy 0.3191\n",
      "Epoch 5 Batch 9750 Loss 1.3734 Accuracy 0.3192\n",
      "Epoch 5 Batch 9800 Loss 1.3730 Accuracy 0.3193\n",
      "Epoch 5 Batch 9850 Loss 1.3727 Accuracy 0.3193\n",
      "Epoch 5 Batch 9900 Loss 1.3724 Accuracy 0.3194\n",
      "Epoch 5 Batch 9950 Loss 1.3722 Accuracy 0.3195\n",
      "Epoch 5 Batch 10000 Loss 1.3717 Accuracy 0.3195\n",
      "Epoch 5 Batch 10050 Loss 1.3713 Accuracy 0.3196\n",
      "Epoch 5 Batch 10100 Loss 1.3710 Accuracy 0.3196\n",
      "Epoch 5 Batch 10150 Loss 1.3707 Accuracy 0.3197\n",
      "Epoch 5 Batch 10200 Loss 1.3703 Accuracy 0.3197\n",
      "Epoch 5 Batch 10250 Loss 1.3699 Accuracy 0.3197\n",
      "Epoch 5 Batch 10300 Loss 1.3696 Accuracy 0.3198\n",
      "Epoch 5 Batch 10350 Loss 1.3692 Accuracy 0.3198\n",
      "Epoch 5 Batch 10400 Loss 1.3690 Accuracy 0.3199\n",
      "Epoch 5 Batch 10450 Loss 1.3686 Accuracy 0.3199\n",
      "Epoch 5 Batch 10500 Loss 1.3682 Accuracy 0.3200\n",
      "Epoch 5 Batch 10550 Loss 1.3679 Accuracy 0.3200\n",
      "Epoch 5 Batch 10600 Loss 1.3674 Accuracy 0.3200\n",
      "Epoch 5 Batch 10650 Loss 1.3671 Accuracy 0.3201\n",
      "Epoch 5 Batch 10700 Loss 1.3667 Accuracy 0.3201\n",
      "Epoch 5 Batch 10750 Loss 1.3664 Accuracy 0.3202\n",
      "Epoch 5 Batch 10800 Loss 1.3661 Accuracy 0.3202\n",
      "Epoch 5 Batch 10850 Loss 1.3657 Accuracy 0.3203\n",
      "Epoch 5 Batch 10900 Loss 1.3653 Accuracy 0.3203\n",
      "Epoch 5 Batch 10950 Loss 1.3650 Accuracy 0.3203\n",
      "Epoch 5 Batch 11000 Loss 1.3647 Accuracy 0.3204\n",
      "Epoch 5 Batch 11050 Loss 1.3644 Accuracy 0.3205\n",
      "Epoch 5 Batch 11100 Loss 1.3643 Accuracy 0.3206\n",
      "Epoch 5 Batch 11150 Loss 1.3639 Accuracy 0.3206\n",
      "Epoch 5 Batch 11200 Loss 1.3636 Accuracy 0.3207\n",
      "Epoch 5 Batch 11250 Loss 1.3632 Accuracy 0.3207\n",
      "Epoch 5 Batch 11300 Loss 1.3629 Accuracy 0.3208\n",
      "Epoch 5 Batch 11350 Loss 1.3626 Accuracy 0.3208\n",
      "Epoch 5 Batch 11400 Loss 1.3624 Accuracy 0.3209\n",
      "Epoch 5 Batch 11450 Loss 1.3620 Accuracy 0.3209\n",
      "Epoch 5 Batch 11500 Loss 1.3616 Accuracy 0.3210\n",
      "Epoch 5 Batch 11550 Loss 1.3613 Accuracy 0.3210\n",
      "Epoch 5 Batch 11600 Loss 1.3609 Accuracy 0.3210\n",
      "Epoch 5 Batch 11650 Loss 1.3605 Accuracy 0.3210\n",
      "Epoch 5 Batch 11700 Loss 1.3601 Accuracy 0.3211\n",
      "Epoch 5 Batch 11750 Loss 1.3596 Accuracy 0.3211\n",
      "Epoch 5 Batch 11800 Loss 1.3591 Accuracy 0.3211\n",
      "Epoch 5 Batch 11850 Loss 1.3586 Accuracy 0.3212\n",
      "Epoch 5 Batch 11900 Loss 1.3582 Accuracy 0.3212\n",
      "Epoch 5 Batch 11950 Loss 1.3578 Accuracy 0.3212\n",
      "Epoch 5 Batch 12000 Loss 1.3575 Accuracy 0.3213\n",
      "Epoch 5 Batch 12050 Loss 1.3571 Accuracy 0.3214\n",
      "Epoch 5 Batch 12100 Loss 1.3567 Accuracy 0.3214\n",
      "Epoch 5 Batch 12150 Loss 1.3562 Accuracy 0.3214\n",
      "Epoch 5 Batch 12200 Loss 1.3559 Accuracy 0.3215\n",
      "Epoch 5 Batch 12250 Loss 1.3556 Accuracy 0.3215\n",
      "Epoch 5 Batch 12300 Loss 1.3552 Accuracy 0.3216\n",
      "Epoch 5 Batch 12350 Loss 1.3549 Accuracy 0.3216\n",
      "Epoch 5 Batch 12400 Loss 1.3546 Accuracy 0.3216\n",
      "Epoch 5 Batch 12450 Loss 1.3542 Accuracy 0.3217\n",
      "Epoch 5 Batch 12500 Loss 1.3538 Accuracy 0.3217\n",
      "Epoch 5 Batch 12550 Loss 1.3535 Accuracy 0.3218\n",
      "Epoch 5 Batch 12600 Loss 1.3532 Accuracy 0.3218\n",
      "Epoch 5 Batch 12650 Loss 1.3529 Accuracy 0.3219\n",
      "Epoch 5 Batch 12700 Loss 1.3526 Accuracy 0.3219\n",
      "Epoch 5 Batch 12750 Loss 1.3524 Accuracy 0.3220\n",
      "Epoch 5 Batch 12800 Loss 1.3522 Accuracy 0.3221\n",
      "Epoch 5 Batch 12850 Loss 1.3519 Accuracy 0.3221\n",
      "Epoch 5 Batch 12900 Loss 1.3516 Accuracy 0.3222\n",
      "Epoch 5 Batch 12950 Loss 1.3513 Accuracy 0.3223\n",
      "Epoch 5 Batch 13000 Loss 1.3510 Accuracy 0.3223\n",
      "Epoch 5 Batch 13050 Loss 1.3507 Accuracy 0.3224\n",
      "Epoch 5 Batch 13100 Loss 1.3504 Accuracy 0.3225\n",
      "Epoch 5 Batch 13150 Loss 1.3501 Accuracy 0.3225\n",
      "Epoch 5 Batch 13200 Loss 1.3499 Accuracy 0.3226\n",
      "Epoch 5 Batch 13250 Loss 1.3496 Accuracy 0.3227\n",
      "Epoch 5 Batch 13300 Loss 1.3493 Accuracy 0.3227\n",
      "Epoch 5 Batch 13350 Loss 1.3490 Accuracy 0.3228\n",
      "Epoch 5 Batch 13400 Loss 1.3487 Accuracy 0.3229\n",
      "Epoch 5 Batch 13450 Loss 1.3485 Accuracy 0.3229\n",
      "Epoch 5 Batch 13500 Loss 1.3482 Accuracy 0.3230\n",
      "Epoch 5 Batch 13550 Loss 1.3479 Accuracy 0.3231\n",
      "Epoch 5 Batch 13600 Loss 1.3477 Accuracy 0.3231\n",
      "Epoch 5 Batch 13650 Loss 1.3474 Accuracy 0.3232\n",
      "Epoch 5 Batch 13700 Loss 1.3470 Accuracy 0.3232\n",
      "Epoch 5 Batch 13750 Loss 1.3469 Accuracy 0.3233\n",
      "Epoch 5 Batch 13800 Loss 1.3467 Accuracy 0.3234\n",
      "Epoch 5 Batch 13850 Loss 1.3464 Accuracy 0.3234\n",
      "Epoch 5 Batch 13900 Loss 1.3462 Accuracy 0.3235\n",
      "Epoch 5 Batch 13950 Loss 1.3459 Accuracy 0.3236\n",
      "Epoch 5 Batch 14000 Loss 1.3457 Accuracy 0.3237\n",
      "Epoch 5 Batch 14050 Loss 1.3455 Accuracy 0.3237\n",
      "Epoch 5 Batch 14100 Loss 1.3452 Accuracy 0.3238\n",
      "Epoch 5 Batch 14150 Loss 1.3450 Accuracy 0.3239\n",
      "Epoch 5 Batch 14200 Loss 1.3447 Accuracy 0.3239\n",
      "Epoch 5 Batch 14250 Loss 1.3444 Accuracy 0.3240\n",
      "Epoch 5 Batch 14300 Loss 1.3442 Accuracy 0.3241\n",
      "Epoch 5 Batch 14350 Loss 1.3438 Accuracy 0.3241\n",
      "Epoch 5 Batch 14400 Loss 1.3435 Accuracy 0.3242\n",
      "Epoch 5 Batch 14450 Loss 1.3433 Accuracy 0.3243\n",
      "Epoch 5 Batch 14500 Loss 1.3430 Accuracy 0.3243\n",
      "Epoch 5 Batch 14550 Loss 1.3428 Accuracy 0.3244\n",
      "Epoch 5 Batch 14600 Loss 1.3425 Accuracy 0.3245\n",
      "Epoch 5 Batch 14650 Loss 1.3422 Accuracy 0.3245\n",
      "Epoch 5 Batch 14700 Loss 1.3420 Accuracy 0.3245\n",
      "Epoch 5 Batch 14750 Loss 1.3420 Accuracy 0.3245\n",
      "Epoch 5 Batch 14800 Loss 1.3420 Accuracy 0.3245\n",
      "Epoch 5 Batch 14850 Loss 1.3420 Accuracy 0.3245\n",
      "Epoch 5 Batch 14900 Loss 1.3422 Accuracy 0.3245\n",
      "Epoch 5 Batch 14950 Loss 1.3425 Accuracy 0.3245\n",
      "Epoch 5 Batch 15000 Loss 1.3428 Accuracy 0.3245\n",
      "Epoch 5 Batch 15050 Loss 1.3431 Accuracy 0.3244\n",
      "Epoch 5 Batch 15100 Loss 1.3434 Accuracy 0.3244\n",
      "Epoch 5 Batch 15150 Loss 1.3437 Accuracy 0.3243\n",
      "Epoch 5 Batch 15200 Loss 1.3440 Accuracy 0.3243\n",
      "Epoch 5 Batch 15250 Loss 1.3443 Accuracy 0.3242\n",
      "Epoch 5 Batch 15300 Loss 1.3446 Accuracy 0.3242\n",
      "Epoch 5 Batch 15350 Loss 1.3449 Accuracy 0.3241\n",
      "Epoch 5 Batch 15400 Loss 1.3451 Accuracy 0.3241\n",
      "Epoch 5 Batch 15450 Loss 1.3454 Accuracy 0.3240\n",
      "Epoch 5 Batch 15500 Loss 1.3457 Accuracy 0.3240\n",
      "Epoch 5 Batch 15550 Loss 1.3461 Accuracy 0.3239\n",
      "Epoch 5 Batch 15600 Loss 1.3464 Accuracy 0.3239\n",
      "Epoch 5 Batch 15650 Loss 1.3467 Accuracy 0.3238\n",
      "Epoch 5 Batch 15700 Loss 1.3469 Accuracy 0.3237\n",
      "Epoch 5 Batch 15750 Loss 1.3472 Accuracy 0.3237\n",
      "Epoch 5 Batch 15800 Loss 1.3476 Accuracy 0.3236\n",
      "Epoch 5 Batch 15850 Loss 1.3479 Accuracy 0.3236\n",
      "Epoch 5 Batch 15900 Loss 1.3482 Accuracy 0.3235\n",
      "Epoch 5 Batch 15950 Loss 1.3485 Accuracy 0.3235\n",
      "Epoch 5 Batch 16000 Loss 1.3488 Accuracy 0.3234\n",
      "Epoch 5 Batch 16050 Loss 1.3491 Accuracy 0.3233\n",
      "Epoch 5 Batch 16100 Loss 1.3494 Accuracy 0.3233\n",
      "Epoch 5 Batch 16150 Loss 1.3497 Accuracy 0.3232\n",
      "Epoch 5 Batch 16200 Loss 1.3500 Accuracy 0.3232\n",
      "Epoch 5 Batch 16250 Loss 1.3503 Accuracy 0.3231\n",
      "Epoch 5 Batch 16300 Loss 1.3505 Accuracy 0.3231\n",
      "Epoch 5 Batch 16350 Loss 1.3508 Accuracy 0.3230\n",
      "Epoch 5 Batch 16400 Loss 1.3510 Accuracy 0.3230\n",
      "Epoch 5 Batch 16450 Loss 1.3513 Accuracy 0.3229\n",
      "Epoch 5 Batch 16500 Loss 1.3515 Accuracy 0.3229\n",
      "Epoch 5 Batch 16550 Loss 1.3517 Accuracy 0.3228\n",
      "Epoch 5 Batch 16600 Loss 1.3519 Accuracy 0.3228\n",
      "Epoch 5 Batch 16650 Loss 1.3520 Accuracy 0.3227\n",
      "Epoch 5 Batch 16700 Loss 1.3522 Accuracy 0.3227\n",
      "Epoch 5 Batch 16750 Loss 1.3524 Accuracy 0.3226\n",
      "Epoch 5 Batch 16800 Loss 1.3526 Accuracy 0.3226\n",
      "Epoch 5 Batch 16850 Loss 1.3529 Accuracy 0.3226\n",
      "Epoch 5 Batch 16900 Loss 1.3530 Accuracy 0.3225\n",
      "Epoch 5 Batch 16950 Loss 1.3532 Accuracy 0.3225\n",
      "Epoch 5 Batch 17000 Loss 1.3534 Accuracy 0.3225\n",
      "Epoch 5 Batch 17050 Loss 1.3536 Accuracy 0.3224\n",
      "Epoch 5 Batch 17100 Loss 1.3538 Accuracy 0.3224\n",
      "Epoch 5 Batch 17150 Loss 1.3539 Accuracy 0.3224\n",
      "Epoch 5 Batch 17200 Loss 1.3541 Accuracy 0.3223\n",
      "Epoch 5 Batch 17250 Loss 1.3543 Accuracy 0.3223\n",
      "Epoch 5 Batch 17300 Loss 1.3544 Accuracy 0.3223\n",
      "Epoch 5 Batch 17350 Loss 1.3545 Accuracy 0.3223\n",
      "Epoch 5 Batch 17400 Loss 1.3546 Accuracy 0.3222\n",
      "Epoch 5 Batch 17450 Loss 1.3548 Accuracy 0.3222\n",
      "Epoch 5 Batch 17500 Loss 1.3549 Accuracy 0.3222\n",
      "Epoch 5 Batch 17550 Loss 1.3551 Accuracy 0.3221\n",
      "Epoch 5 Batch 17600 Loss 1.3553 Accuracy 0.3221\n",
      "Epoch 5 Batch 17650 Loss 1.3555 Accuracy 0.3221\n",
      "Epoch 5 Batch 17700 Loss 1.3556 Accuracy 0.3220\n",
      "Epoch 5 Batch 17750 Loss 1.3559 Accuracy 0.3220\n",
      "Epoch 5 Batch 17800 Loss 1.3561 Accuracy 0.3220\n",
      "Epoch 5 Batch 17850 Loss 1.3563 Accuracy 0.3219\n",
      "Epoch 5 Batch 17900 Loss 1.3566 Accuracy 0.3219\n",
      "Epoch 5 Batch 17950 Loss 1.3568 Accuracy 0.3219\n",
      "Epoch 5 Batch 18000 Loss 1.3569 Accuracy 0.3218\n",
      "Epoch 5 Batch 18050 Loss 1.3572 Accuracy 0.3218\n",
      "Epoch 5 Batch 18100 Loss 1.3574 Accuracy 0.3217\n",
      "Epoch 5 Batch 18150 Loss 1.3575 Accuracy 0.3217\n",
      "Epoch 5 Batch 18200 Loss 1.3578 Accuracy 0.3217\n",
      "Epoch 5 Batch 18250 Loss 1.3580 Accuracy 0.3216\n",
      "Epoch 5 Batch 18300 Loss 1.3582 Accuracy 0.3216\n",
      "Epoch 5 Batch 18350 Loss 1.3584 Accuracy 0.3216\n",
      "Epoch 5 Batch 18400 Loss 1.3585 Accuracy 0.3215\n",
      "Epoch 5 Batch 18450 Loss 1.3587 Accuracy 0.3215\n",
      "Epoch 5 Batch 18500 Loss 1.3588 Accuracy 0.3215\n",
      "Epoch 5 Batch 18550 Loss 1.3589 Accuracy 0.3214\n",
      "Epoch 5 Batch 18600 Loss 1.3591 Accuracy 0.3214\n",
      "Epoch 5 Batch 18650 Loss 1.3592 Accuracy 0.3213\n",
      "Epoch 5 Batch 18700 Loss 1.3594 Accuracy 0.3213\n",
      "Epoch 5 Batch 18750 Loss 1.3595 Accuracy 0.3213\n",
      "Epoch 5 Batch 18800 Loss 1.3597 Accuracy 0.3212\n",
      "Epoch 5 Batch 18850 Loss 1.3598 Accuracy 0.3212\n",
      "Epoch 5 Batch 18900 Loss 1.3600 Accuracy 0.3211\n",
      "Epoch 5 Batch 18950 Loss 1.3601 Accuracy 0.3211\n",
      "Epoch 5 Batch 19000 Loss 1.3602 Accuracy 0.3210\n",
      "Epoch 5 Batch 19050 Loss 1.3603 Accuracy 0.3210\n",
      "Epoch 5 Batch 19100 Loss 1.3604 Accuracy 0.3210\n",
      "Epoch 5 Batch 19150 Loss 1.3605 Accuracy 0.3209\n",
      "Epoch 5 Batch 19200 Loss 1.3606 Accuracy 0.3209\n",
      "Epoch 5 Batch 19250 Loss 1.3607 Accuracy 0.3209\n",
      "Epoch 5 Batch 19300 Loss 1.3608 Accuracy 0.3208\n",
      "Epoch 5 Batch 19350 Loss 1.3608 Accuracy 0.3208\n",
      "Saving checkpoint for epoch 5 at ./tradutor_chek/ckpt-5\n",
      "Time taken for 1 epoch 16252.296688079834 secs\n",
      "\n",
      "Start or epoch 6\n",
      "Epoch 6 Batch 0 Loss 1.3309 Accuracy 0.2853\n",
      "Epoch 6 Batch 50 Loss 1.4327 Accuracy 0.3020\n",
      "Epoch 6 Batch 100 Loss 1.4280 Accuracy 0.3037\n",
      "Epoch 6 Batch 150 Loss 1.4292 Accuracy 0.3048\n",
      "Epoch 6 Batch 200 Loss 1.4280 Accuracy 0.3057\n",
      "Epoch 6 Batch 250 Loss 1.4217 Accuracy 0.3064\n",
      "Epoch 6 Batch 300 Loss 1.4220 Accuracy 0.3068\n",
      "Epoch 6 Batch 350 Loss 1.4176 Accuracy 0.3071\n",
      "Epoch 6 Batch 400 Loss 1.4171 Accuracy 0.3075\n",
      "Epoch 6 Batch 450 Loss 1.4160 Accuracy 0.3076\n",
      "Epoch 6 Batch 500 Loss 1.4159 Accuracy 0.3077\n",
      "Epoch 6 Batch 550 Loss 1.4165 Accuracy 0.3077\n",
      "Epoch 6 Batch 600 Loss 1.4181 Accuracy 0.3079\n",
      "Epoch 6 Batch 650 Loss 1.4205 Accuracy 0.3080\n",
      "Epoch 6 Batch 700 Loss 1.4200 Accuracy 0.3080\n",
      "Epoch 6 Batch 750 Loss 1.4208 Accuracy 0.3081\n",
      "Epoch 6 Batch 800 Loss 1.4203 Accuracy 0.3081\n",
      "Epoch 6 Batch 850 Loss 1.4199 Accuracy 0.3082\n",
      "Epoch 6 Batch 900 Loss 1.4190 Accuracy 0.3083\n",
      "Epoch 6 Batch 950 Loss 1.4187 Accuracy 0.3083\n",
      "Epoch 6 Batch 1000 Loss 1.4183 Accuracy 0.3084\n",
      "Epoch 6 Batch 1050 Loss 1.4189 Accuracy 0.3084\n",
      "Epoch 6 Batch 1100 Loss 1.4183 Accuracy 0.3086\n",
      "Epoch 6 Batch 1150 Loss 1.4173 Accuracy 0.3088\n",
      "Epoch 6 Batch 1200 Loss 1.4170 Accuracy 0.3088\n",
      "Epoch 6 Batch 1250 Loss 1.4163 Accuracy 0.3088\n",
      "Epoch 6 Batch 1300 Loss 1.4161 Accuracy 0.3089\n",
      "Epoch 6 Batch 1350 Loss 1.4148 Accuracy 0.3090\n",
      "Epoch 6 Batch 1400 Loss 1.4146 Accuracy 0.3090\n",
      "Epoch 6 Batch 1450 Loss 1.4146 Accuracy 0.3090\n",
      "Epoch 6 Batch 1500 Loss 1.4146 Accuracy 0.3091\n",
      "Epoch 6 Batch 1550 Loss 1.4144 Accuracy 0.3091\n",
      "Epoch 6 Batch 1600 Loss 1.4130 Accuracy 0.3092\n",
      "Epoch 6 Batch 1650 Loss 1.4125 Accuracy 0.3092\n",
      "Epoch 6 Batch 1700 Loss 1.4131 Accuracy 0.3095\n",
      "Epoch 6 Batch 1750 Loss 1.4129 Accuracy 0.3096\n",
      "Epoch 6 Batch 1800 Loss 1.4125 Accuracy 0.3097\n",
      "Epoch 6 Batch 1850 Loss 1.4121 Accuracy 0.3098\n",
      "Epoch 6 Batch 1900 Loss 1.4113 Accuracy 0.3097\n",
      "Epoch 6 Batch 1950 Loss 1.4105 Accuracy 0.3097\n",
      "Epoch 6 Batch 2000 Loss 1.4099 Accuracy 0.3098\n",
      "Epoch 6 Batch 2050 Loss 1.4090 Accuracy 0.3098\n",
      "Epoch 6 Batch 2100 Loss 1.4092 Accuracy 0.3099\n",
      "Epoch 6 Batch 2150 Loss 1.4088 Accuracy 0.3100\n",
      "Epoch 6 Batch 2200 Loss 1.4080 Accuracy 0.3101\n",
      "Epoch 6 Batch 2250 Loss 1.4077 Accuracy 0.3100\n",
      "Epoch 6 Batch 2300 Loss 1.4075 Accuracy 0.3101\n",
      "Epoch 6 Batch 2350 Loss 1.4075 Accuracy 0.3102\n",
      "Epoch 6 Batch 2400 Loss 1.4077 Accuracy 0.3103\n",
      "Epoch 6 Batch 2450 Loss 1.4083 Accuracy 0.3103\n",
      "Epoch 6 Batch 2500 Loss 1.4087 Accuracy 0.3105\n",
      "Epoch 6 Batch 2550 Loss 1.4082 Accuracy 0.3105\n",
      "Epoch 6 Batch 2600 Loss 1.4081 Accuracy 0.3107\n",
      "Epoch 6 Batch 2650 Loss 1.4080 Accuracy 0.3108\n",
      "Epoch 6 Batch 2700 Loss 1.4074 Accuracy 0.3109\n",
      "Epoch 6 Batch 2750 Loss 1.4067 Accuracy 0.3112\n",
      "Epoch 6 Batch 2800 Loss 1.4064 Accuracy 0.3114\n",
      "Epoch 6 Batch 2850 Loss 1.4060 Accuracy 0.3116\n",
      "Epoch 6 Batch 2900 Loss 1.4053 Accuracy 0.3118\n",
      "Epoch 6 Batch 2950 Loss 1.4044 Accuracy 0.3120\n",
      "Epoch 6 Batch 3000 Loss 1.4034 Accuracy 0.3121\n",
      "Epoch 6 Batch 3050 Loss 1.4029 Accuracy 0.3123\n",
      "Epoch 6 Batch 3100 Loss 1.4020 Accuracy 0.3124\n",
      "Epoch 6 Batch 3150 Loss 1.4009 Accuracy 0.3125\n",
      "Epoch 6 Batch 3200 Loss 1.4004 Accuracy 0.3127\n",
      "Epoch 6 Batch 3250 Loss 1.3992 Accuracy 0.3128\n",
      "Epoch 6 Batch 3300 Loss 1.3981 Accuracy 0.3129\n",
      "Epoch 6 Batch 3350 Loss 1.3974 Accuracy 0.3130\n",
      "Epoch 6 Batch 3400 Loss 1.3966 Accuracy 0.3131\n",
      "Epoch 6 Batch 3450 Loss 1.3956 Accuracy 0.3132\n",
      "Epoch 6 Batch 3500 Loss 1.3949 Accuracy 0.3133\n",
      "Epoch 6 Batch 3550 Loss 1.3939 Accuracy 0.3134\n",
      "Epoch 6 Batch 3600 Loss 1.3926 Accuracy 0.3135\n",
      "Epoch 6 Batch 3650 Loss 1.3917 Accuracy 0.3136\n",
      "Epoch 6 Batch 3700 Loss 1.3906 Accuracy 0.3137\n",
      "Epoch 6 Batch 3750 Loss 1.3894 Accuracy 0.3138\n",
      "Epoch 6 Batch 3800 Loss 1.3882 Accuracy 0.3139\n",
      "Epoch 6 Batch 3850 Loss 1.3871 Accuracy 0.3140\n",
      "Epoch 6 Batch 3900 Loss 1.3866 Accuracy 0.3141\n",
      "Epoch 6 Batch 3950 Loss 1.3858 Accuracy 0.3142\n",
      "Epoch 6 Batch 4000 Loss 1.3854 Accuracy 0.3143\n",
      "Epoch 6 Batch 4050 Loss 1.3851 Accuracy 0.3144\n",
      "Epoch 6 Batch 4100 Loss 1.3844 Accuracy 0.3145\n",
      "Epoch 6 Batch 4150 Loss 1.3839 Accuracy 0.3147\n",
      "Epoch 6 Batch 4200 Loss 1.3836 Accuracy 0.3148\n",
      "Epoch 6 Batch 4250 Loss 1.3829 Accuracy 0.3149\n",
      "Epoch 6 Batch 4300 Loss 1.3827 Accuracy 0.3151\n",
      "Epoch 6 Batch 4350 Loss 1.3820 Accuracy 0.3152\n",
      "Epoch 6 Batch 4400 Loss 1.3816 Accuracy 0.3153\n",
      "Epoch 6 Batch 4450 Loss 1.3816 Accuracy 0.3154\n",
      "Epoch 6 Batch 4500 Loss 1.3814 Accuracy 0.3155\n",
      "Epoch 6 Batch 4550 Loss 1.3810 Accuracy 0.3156\n",
      "Epoch 6 Batch 4600 Loss 1.3805 Accuracy 0.3157\n",
      "Epoch 6 Batch 4650 Loss 1.3803 Accuracy 0.3159\n",
      "Epoch 6 Batch 4700 Loss 1.3797 Accuracy 0.3160\n",
      "Epoch 6 Batch 4750 Loss 1.3793 Accuracy 0.3161\n",
      "Epoch 6 Batch 4800 Loss 1.3787 Accuracy 0.3162\n",
      "Epoch 6 Batch 4850 Loss 1.3785 Accuracy 0.3163\n",
      "Epoch 6 Batch 4900 Loss 1.3783 Accuracy 0.3164\n",
      "Epoch 6 Batch 4950 Loss 1.3779 Accuracy 0.3165\n",
      "Epoch 6 Batch 5000 Loss 1.3776 Accuracy 0.3166\n",
      "Epoch 6 Batch 5050 Loss 1.3770 Accuracy 0.3167\n",
      "Epoch 6 Batch 5100 Loss 1.3768 Accuracy 0.3168\n",
      "Epoch 6 Batch 5150 Loss 1.3768 Accuracy 0.3169\n",
      "Epoch 6 Batch 5200 Loss 1.3765 Accuracy 0.3170\n",
      "Epoch 6 Batch 5250 Loss 1.3763 Accuracy 0.3172\n",
      "Epoch 6 Batch 5300 Loss 1.3759 Accuracy 0.3173\n",
      "Epoch 6 Batch 5350 Loss 1.3753 Accuracy 0.3174\n",
      "Epoch 6 Batch 5400 Loss 1.3749 Accuracy 0.3175\n",
      "Epoch 6 Batch 5450 Loss 1.3745 Accuracy 0.3176\n",
      "Epoch 6 Batch 5500 Loss 1.3741 Accuracy 0.3178\n",
      "Epoch 6 Batch 5550 Loss 1.3739 Accuracy 0.3180\n",
      "Epoch 6 Batch 5600 Loss 1.3739 Accuracy 0.3181\n",
      "Epoch 6 Batch 5650 Loss 1.3739 Accuracy 0.3183\n",
      "Epoch 6 Batch 5700 Loss 1.3737 Accuracy 0.3184\n",
      "Epoch 6 Batch 5750 Loss 1.3734 Accuracy 0.3186\n",
      "Epoch 6 Batch 5800 Loss 1.3731 Accuracy 0.3187\n",
      "Epoch 6 Batch 5850 Loss 1.3729 Accuracy 0.3187\n",
      "Epoch 6 Batch 5900 Loss 1.3728 Accuracy 0.3189\n",
      "Epoch 6 Batch 5950 Loss 1.3724 Accuracy 0.3190\n",
      "Epoch 6 Batch 6000 Loss 1.3720 Accuracy 0.3191\n",
      "Epoch 6 Batch 6050 Loss 1.3718 Accuracy 0.3192\n",
      "Epoch 6 Batch 6100 Loss 1.3717 Accuracy 0.3193\n",
      "Epoch 6 Batch 6150 Loss 1.3716 Accuracy 0.3194\n",
      "Epoch 6 Batch 6200 Loss 1.3715 Accuracy 0.3195\n",
      "Epoch 6 Batch 6250 Loss 1.3713 Accuracy 0.3196\n",
      "Epoch 6 Batch 6300 Loss 1.3713 Accuracy 0.3197\n",
      "Epoch 6 Batch 6350 Loss 1.3710 Accuracy 0.3198\n",
      "Epoch 6 Batch 6400 Loss 1.3708 Accuracy 0.3199\n",
      "Epoch 6 Batch 6450 Loss 1.3708 Accuracy 0.3200\n",
      "Epoch 6 Batch 6500 Loss 1.3708 Accuracy 0.3202\n",
      "Epoch 6 Batch 6550 Loss 1.3707 Accuracy 0.3203\n",
      "Epoch 6 Batch 6600 Loss 1.3706 Accuracy 0.3204\n",
      "Epoch 6 Batch 6650 Loss 1.3702 Accuracy 0.3204\n",
      "Epoch 6 Batch 6700 Loss 1.3697 Accuracy 0.3205\n",
      "Epoch 6 Batch 6750 Loss 1.3694 Accuracy 0.3205\n",
      "Epoch 6 Batch 6800 Loss 1.3691 Accuracy 0.3206\n",
      "Epoch 6 Batch 6850 Loss 1.3689 Accuracy 0.3207\n",
      "Epoch 6 Batch 6900 Loss 1.3688 Accuracy 0.3208\n",
      "Epoch 6 Batch 6950 Loss 1.3685 Accuracy 0.3208\n",
      "Epoch 6 Batch 7000 Loss 1.3683 Accuracy 0.3209\n",
      "Epoch 6 Batch 7050 Loss 1.3679 Accuracy 0.3210\n",
      "Epoch 6 Batch 7100 Loss 1.3677 Accuracy 0.3211\n",
      "Epoch 6 Batch 7150 Loss 1.3674 Accuracy 0.3212\n",
      "Epoch 6 Batch 7200 Loss 1.3672 Accuracy 0.3213\n",
      "Epoch 6 Batch 7250 Loss 1.3670 Accuracy 0.3214\n",
      "Epoch 6 Batch 7300 Loss 1.3666 Accuracy 0.3214\n",
      "Epoch 6 Batch 7350 Loss 1.3664 Accuracy 0.3215\n",
      "Epoch 6 Batch 7400 Loss 1.3660 Accuracy 0.3215\n",
      "Epoch 6 Batch 7450 Loss 1.3658 Accuracy 0.3216\n",
      "Epoch 6 Batch 7500 Loss 1.3654 Accuracy 0.3217\n",
      "Epoch 6 Batch 7550 Loss 1.3649 Accuracy 0.3218\n",
      "Epoch 6 Batch 7600 Loss 1.3644 Accuracy 0.3219\n",
      "Epoch 6 Batch 7650 Loss 1.3639 Accuracy 0.3219\n",
      "Epoch 6 Batch 7700 Loss 1.3634 Accuracy 0.3220\n",
      "Epoch 6 Batch 7750 Loss 1.3628 Accuracy 0.3220\n",
      "Epoch 6 Batch 7800 Loss 1.3623 Accuracy 0.3221\n",
      "Epoch 6 Batch 7850 Loss 1.3616 Accuracy 0.3221\n",
      "Epoch 6 Batch 7900 Loss 1.3612 Accuracy 0.3221\n",
      "Epoch 6 Batch 7950 Loss 1.3608 Accuracy 0.3222\n",
      "Epoch 6 Batch 8000 Loss 1.3602 Accuracy 0.3222\n",
      "Epoch 6 Batch 8050 Loss 1.3596 Accuracy 0.3222\n",
      "Epoch 6 Batch 8100 Loss 1.3589 Accuracy 0.3222\n",
      "Epoch 6 Batch 8150 Loss 1.3582 Accuracy 0.3223\n",
      "Epoch 6 Batch 8200 Loss 1.3577 Accuracy 0.3223\n",
      "Epoch 6 Batch 8250 Loss 1.3570 Accuracy 0.3223\n",
      "Epoch 6 Batch 8300 Loss 1.3565 Accuracy 0.3223\n",
      "Epoch 6 Batch 8350 Loss 1.3561 Accuracy 0.3223\n",
      "Epoch 6 Batch 8400 Loss 1.3557 Accuracy 0.3224\n",
      "Epoch 6 Batch 8450 Loss 1.3551 Accuracy 0.3224\n",
      "Epoch 6 Batch 8500 Loss 1.3548 Accuracy 0.3224\n",
      "Epoch 6 Batch 8550 Loss 1.3544 Accuracy 0.3225\n",
      "Epoch 6 Batch 8600 Loss 1.3538 Accuracy 0.3225\n",
      "Epoch 6 Batch 8650 Loss 1.3536 Accuracy 0.3225\n",
      "Epoch 6 Batch 8700 Loss 1.3531 Accuracy 0.3226\n",
      "Epoch 6 Batch 8750 Loss 1.3527 Accuracy 0.3226\n",
      "Epoch 6 Batch 8800 Loss 1.3523 Accuracy 0.3227\n",
      "Epoch 6 Batch 8850 Loss 1.3519 Accuracy 0.3227\n",
      "Epoch 6 Batch 8900 Loss 1.3516 Accuracy 0.3228\n",
      "Epoch 6 Batch 8950 Loss 1.3513 Accuracy 0.3228\n",
      "Epoch 6 Batch 9000 Loss 1.3509 Accuracy 0.3229\n",
      "Epoch 6 Batch 9050 Loss 1.3506 Accuracy 0.3229\n",
      "Epoch 6 Batch 9100 Loss 1.3502 Accuracy 0.3229\n",
      "Epoch 6 Batch 9150 Loss 1.3497 Accuracy 0.3230\n",
      "Epoch 6 Batch 9200 Loss 1.3493 Accuracy 0.3230\n",
      "Epoch 6 Batch 9250 Loss 1.3489 Accuracy 0.3231\n",
      "Epoch 6 Batch 9300 Loss 1.3484 Accuracy 0.3231\n",
      "Epoch 6 Batch 9350 Loss 1.3480 Accuracy 0.3232\n",
      "Epoch 6 Batch 9400 Loss 1.3477 Accuracy 0.3232\n",
      "Epoch 6 Batch 9450 Loss 1.3473 Accuracy 0.3233\n",
      "Epoch 6 Batch 9500 Loss 1.3469 Accuracy 0.3234\n",
      "Epoch 6 Batch 9550 Loss 1.3465 Accuracy 0.3234\n",
      "Epoch 6 Batch 9600 Loss 1.3461 Accuracy 0.3235\n",
      "Epoch 6 Batch 9650 Loss 1.3458 Accuracy 0.3235\n",
      "Epoch 6 Batch 9700 Loss 1.3455 Accuracy 0.3236\n",
      "Epoch 6 Batch 9750 Loss 1.3452 Accuracy 0.3237\n",
      "Epoch 6 Batch 9800 Loss 1.3447 Accuracy 0.3237\n",
      "Epoch 6 Batch 9850 Loss 1.3444 Accuracy 0.3238\n",
      "Epoch 6 Batch 9900 Loss 1.3440 Accuracy 0.3238\n",
      "Epoch 6 Batch 9950 Loss 1.3437 Accuracy 0.3239\n",
      "Epoch 6 Batch 10000 Loss 1.3433 Accuracy 0.3240\n",
      "Epoch 6 Batch 10050 Loss 1.3429 Accuracy 0.3240\n",
      "Epoch 6 Batch 10100 Loss 1.3426 Accuracy 0.3240\n",
      "Epoch 6 Batch 10150 Loss 1.3422 Accuracy 0.3241\n",
      "Epoch 6 Batch 10200 Loss 1.3419 Accuracy 0.3241\n",
      "Epoch 6 Batch 10250 Loss 1.3416 Accuracy 0.3242\n",
      "Epoch 6 Batch 10300 Loss 1.3413 Accuracy 0.3242\n",
      "Epoch 6 Batch 10350 Loss 1.3409 Accuracy 0.3243\n",
      "Epoch 6 Batch 10400 Loss 1.3406 Accuracy 0.3243\n",
      "Epoch 6 Batch 10450 Loss 1.3402 Accuracy 0.3243\n",
      "Epoch 6 Batch 10500 Loss 1.3399 Accuracy 0.3244\n",
      "Epoch 6 Batch 10550 Loss 1.3398 Accuracy 0.3244\n",
      "Epoch 6 Batch 10600 Loss 1.3394 Accuracy 0.3245\n",
      "Epoch 6 Batch 10650 Loss 1.3392 Accuracy 0.3245\n",
      "Epoch 6 Batch 10700 Loss 1.3387 Accuracy 0.3246\n",
      "Epoch 6 Batch 10750 Loss 1.3383 Accuracy 0.3246\n",
      "Epoch 6 Batch 10800 Loss 1.3380 Accuracy 0.3247\n",
      "Epoch 6 Batch 10850 Loss 1.3377 Accuracy 0.3247\n",
      "Epoch 6 Batch 10900 Loss 1.3374 Accuracy 0.3248\n",
      "Epoch 6 Batch 10950 Loss 1.3370 Accuracy 0.3248\n",
      "Epoch 6 Batch 11000 Loss 1.3367 Accuracy 0.3249\n",
      "Epoch 6 Batch 11050 Loss 1.3363 Accuracy 0.3249\n",
      "Epoch 6 Batch 11100 Loss 1.3359 Accuracy 0.3250\n",
      "Epoch 6 Batch 11150 Loss 1.3355 Accuracy 0.3250\n",
      "Epoch 6 Batch 11200 Loss 1.3353 Accuracy 0.3251\n",
      "Epoch 6 Batch 11250 Loss 1.3351 Accuracy 0.3252\n",
      "Epoch 6 Batch 11300 Loss 1.3349 Accuracy 0.3252\n",
      "Epoch 6 Batch 11350 Loss 1.3344 Accuracy 0.3253\n",
      "Epoch 6 Batch 11400 Loss 1.3341 Accuracy 0.3253\n",
      "Epoch 6 Batch 11450 Loss 1.3337 Accuracy 0.3254\n",
      "Epoch 6 Batch 11500 Loss 1.3334 Accuracy 0.3254\n",
      "Epoch 6 Batch 11550 Loss 1.3330 Accuracy 0.3255\n",
      "Epoch 6 Batch 11600 Loss 1.3326 Accuracy 0.3255\n",
      "Epoch 6 Batch 11650 Loss 1.3322 Accuracy 0.3255\n",
      "Epoch 6 Batch 11700 Loss 1.3318 Accuracy 0.3256\n",
      "Epoch 6 Batch 11750 Loss 1.3314 Accuracy 0.3256\n",
      "Epoch 6 Batch 11800 Loss 1.3311 Accuracy 0.3256\n",
      "Epoch 6 Batch 11850 Loss 1.3307 Accuracy 0.3257\n",
      "Epoch 6 Batch 11900 Loss 1.3304 Accuracy 0.3257\n",
      "Epoch 6 Batch 11950 Loss 1.3300 Accuracy 0.3258\n",
      "Epoch 6 Batch 12000 Loss 1.3296 Accuracy 0.3258\n",
      "Epoch 6 Batch 12050 Loss 1.3291 Accuracy 0.3258\n",
      "Epoch 6 Batch 12100 Loss 1.3287 Accuracy 0.3259\n",
      "Epoch 6 Batch 12150 Loss 1.3283 Accuracy 0.3259\n",
      "Epoch 6 Batch 12200 Loss 1.3279 Accuracy 0.3260\n",
      "Epoch 6 Batch 12250 Loss 1.3276 Accuracy 0.3260\n",
      "Epoch 6 Batch 12300 Loss 1.3272 Accuracy 0.3260\n",
      "Epoch 6 Batch 12350 Loss 1.3269 Accuracy 0.3261\n",
      "Epoch 6 Batch 12400 Loss 1.3265 Accuracy 0.3261\n",
      "Epoch 6 Batch 12450 Loss 1.3262 Accuracy 0.3262\n",
      "Epoch 6 Batch 12500 Loss 1.3258 Accuracy 0.3262\n",
      "Epoch 6 Batch 12550 Loss 1.3255 Accuracy 0.3263\n",
      "Epoch 6 Batch 12600 Loss 1.3252 Accuracy 0.3263\n",
      "Epoch 6 Batch 12650 Loss 1.3249 Accuracy 0.3264\n",
      "Epoch 6 Batch 12700 Loss 1.3246 Accuracy 0.3264\n",
      "Epoch 6 Batch 12750 Loss 1.3243 Accuracy 0.3265\n",
      "Epoch 6 Batch 12800 Loss 1.3241 Accuracy 0.3265\n",
      "Epoch 6 Batch 12850 Loss 1.3238 Accuracy 0.3266\n",
      "Epoch 6 Batch 12900 Loss 1.3235 Accuracy 0.3266\n",
      "Epoch 6 Batch 12950 Loss 1.3233 Accuracy 0.3267\n",
      "Epoch 6 Batch 13000 Loss 1.3230 Accuracy 0.3268\n",
      "Epoch 6 Batch 13050 Loss 1.3228 Accuracy 0.3268\n",
      "Epoch 6 Batch 13100 Loss 1.3225 Accuracy 0.3269\n",
      "Epoch 6 Batch 13150 Loss 1.3222 Accuracy 0.3270\n",
      "Epoch 6 Batch 13200 Loss 1.3219 Accuracy 0.3270\n",
      "Epoch 6 Batch 13250 Loss 1.3216 Accuracy 0.3271\n",
      "Epoch 6 Batch 13300 Loss 1.3214 Accuracy 0.3272\n",
      "Epoch 6 Batch 13350 Loss 1.3212 Accuracy 0.3272\n",
      "Epoch 6 Batch 13400 Loss 1.3209 Accuracy 0.3273\n",
      "Epoch 6 Batch 13450 Loss 1.3206 Accuracy 0.3274\n",
      "Epoch 6 Batch 13500 Loss 1.3203 Accuracy 0.3274\n",
      "Epoch 6 Batch 13550 Loss 1.3200 Accuracy 0.3275\n",
      "Epoch 6 Batch 13600 Loss 1.3198 Accuracy 0.3275\n",
      "Epoch 6 Batch 13650 Loss 1.3196 Accuracy 0.3276\n",
      "Epoch 6 Batch 13700 Loss 1.3193 Accuracy 0.3277\n",
      "Epoch 6 Batch 13750 Loss 1.3191 Accuracy 0.3277\n",
      "Epoch 6 Batch 13800 Loss 1.3188 Accuracy 0.3278\n",
      "Epoch 6 Batch 13850 Loss 1.3186 Accuracy 0.3279\n",
      "Epoch 6 Batch 13900 Loss 1.3183 Accuracy 0.3280\n",
      "Epoch 6 Batch 13950 Loss 1.3180 Accuracy 0.3280\n",
      "Epoch 6 Batch 14000 Loss 1.3179 Accuracy 0.3281\n",
      "Epoch 6 Batch 14050 Loss 1.3176 Accuracy 0.3281\n",
      "Epoch 6 Batch 14100 Loss 1.3173 Accuracy 0.3282\n",
      "Epoch 6 Batch 14150 Loss 1.3171 Accuracy 0.3283\n",
      "Epoch 6 Batch 14200 Loss 1.3168 Accuracy 0.3283\n",
      "Epoch 6 Batch 14250 Loss 1.3165 Accuracy 0.3284\n",
      "Epoch 6 Batch 14300 Loss 1.3163 Accuracy 0.3285\n",
      "Epoch 6 Batch 14350 Loss 1.3160 Accuracy 0.3285\n",
      "Epoch 6 Batch 14400 Loss 1.3157 Accuracy 0.3286\n",
      "Epoch 6 Batch 14450 Loss 1.3156 Accuracy 0.3287\n",
      "Epoch 6 Batch 14500 Loss 1.3153 Accuracy 0.3287\n",
      "Epoch 6 Batch 14550 Loss 1.3150 Accuracy 0.3288\n",
      "Epoch 6 Batch 14600 Loss 1.3147 Accuracy 0.3288\n",
      "Epoch 6 Batch 14650 Loss 1.3145 Accuracy 0.3289\n",
      "Epoch 6 Batch 14700 Loss 1.3143 Accuracy 0.3290\n",
      "Epoch 6 Batch 14750 Loss 1.3143 Accuracy 0.3290\n",
      "Epoch 6 Batch 14800 Loss 1.3143 Accuracy 0.3290\n",
      "Epoch 6 Batch 14850 Loss 1.3143 Accuracy 0.3290\n",
      "Epoch 6 Batch 14900 Loss 1.3145 Accuracy 0.3289\n",
      "Epoch 6 Batch 14950 Loss 1.3148 Accuracy 0.3289\n",
      "Epoch 6 Batch 15000 Loss 1.3150 Accuracy 0.3289\n",
      "Epoch 6 Batch 15050 Loss 1.3152 Accuracy 0.3288\n",
      "Epoch 6 Batch 15100 Loss 1.3155 Accuracy 0.3288\n",
      "Epoch 6 Batch 15150 Loss 1.3159 Accuracy 0.3287\n",
      "Epoch 6 Batch 15200 Loss 1.3162 Accuracy 0.3287\n",
      "Epoch 6 Batch 15250 Loss 1.3166 Accuracy 0.3286\n",
      "Epoch 6 Batch 15300 Loss 1.3169 Accuracy 0.3286\n",
      "Epoch 6 Batch 15350 Loss 1.3173 Accuracy 0.3285\n",
      "Epoch 6 Batch 15400 Loss 1.3176 Accuracy 0.3285\n",
      "Epoch 6 Batch 15450 Loss 1.3178 Accuracy 0.3284\n",
      "Epoch 6 Batch 15500 Loss 1.3182 Accuracy 0.3284\n",
      "Epoch 6 Batch 15550 Loss 1.3185 Accuracy 0.3283\n",
      "Epoch 6 Batch 15600 Loss 1.3189 Accuracy 0.3283\n",
      "Epoch 6 Batch 15650 Loss 1.3192 Accuracy 0.3282\n",
      "Epoch 6 Batch 15700 Loss 1.3196 Accuracy 0.3281\n",
      "Epoch 6 Batch 15750 Loss 1.3199 Accuracy 0.3281\n",
      "Epoch 6 Batch 15800 Loss 1.3202 Accuracy 0.3280\n",
      "Epoch 6 Batch 15850 Loss 1.3205 Accuracy 0.3280\n",
      "Epoch 6 Batch 15900 Loss 1.3207 Accuracy 0.3279\n",
      "Epoch 6 Batch 15950 Loss 1.3210 Accuracy 0.3278\n",
      "Epoch 6 Batch 16000 Loss 1.3213 Accuracy 0.3278\n",
      "Epoch 6 Batch 16050 Loss 1.3215 Accuracy 0.3277\n",
      "Epoch 6 Batch 16100 Loss 1.3218 Accuracy 0.3276\n",
      "Epoch 6 Batch 16150 Loss 1.3220 Accuracy 0.3276\n",
      "Epoch 6 Batch 16200 Loss 1.3223 Accuracy 0.3275\n",
      "Epoch 6 Batch 16250 Loss 1.3226 Accuracy 0.3275\n",
      "Epoch 6 Batch 16300 Loss 1.3228 Accuracy 0.3274\n",
      "Epoch 6 Batch 16350 Loss 1.3231 Accuracy 0.3274\n",
      "Epoch 6 Batch 16400 Loss 1.3234 Accuracy 0.3273\n",
      "Epoch 6 Batch 16450 Loss 1.3236 Accuracy 0.3272\n",
      "Epoch 6 Batch 16500 Loss 1.3239 Accuracy 0.3272\n",
      "Epoch 6 Batch 16550 Loss 1.3242 Accuracy 0.3272\n",
      "Epoch 6 Batch 16600 Loss 1.3245 Accuracy 0.3271\n",
      "Epoch 6 Batch 16650 Loss 1.3247 Accuracy 0.3271\n",
      "Epoch 6 Batch 16700 Loss 1.3249 Accuracy 0.3270\n",
      "Epoch 6 Batch 16750 Loss 1.3251 Accuracy 0.3270\n",
      "Epoch 6 Batch 16800 Loss 1.3253 Accuracy 0.3269\n",
      "Epoch 6 Batch 16850 Loss 1.3256 Accuracy 0.3269\n",
      "Epoch 6 Batch 16900 Loss 1.3257 Accuracy 0.3268\n",
      "Epoch 6 Batch 16950 Loss 1.3259 Accuracy 0.3268\n",
      "Epoch 6 Batch 17000 Loss 1.3262 Accuracy 0.3268\n",
      "Epoch 6 Batch 17050 Loss 1.3264 Accuracy 0.3268\n",
      "Epoch 6 Batch 17100 Loss 1.3266 Accuracy 0.3267\n",
      "Epoch 6 Batch 17150 Loss 1.3267 Accuracy 0.3267\n",
      "Epoch 6 Batch 17200 Loss 1.3269 Accuracy 0.3266\n",
      "Epoch 6 Batch 17250 Loss 1.3271 Accuracy 0.3266\n",
      "Epoch 6 Batch 17300 Loss 1.3272 Accuracy 0.3266\n",
      "Epoch 6 Batch 17350 Loss 1.3274 Accuracy 0.3266\n",
      "Epoch 6 Batch 17400 Loss 1.3275 Accuracy 0.3265\n",
      "Epoch 6 Batch 17450 Loss 1.3277 Accuracy 0.3265\n",
      "Epoch 6 Batch 17500 Loss 1.3278 Accuracy 0.3265\n",
      "Epoch 6 Batch 17550 Loss 1.3280 Accuracy 0.3264\n",
      "Epoch 6 Batch 17600 Loss 1.3282 Accuracy 0.3264\n",
      "Epoch 6 Batch 17650 Loss 1.3285 Accuracy 0.3264\n",
      "Epoch 6 Batch 17700 Loss 1.3286 Accuracy 0.3263\n",
      "Epoch 6 Batch 17750 Loss 1.3288 Accuracy 0.3263\n",
      "Epoch 6 Batch 17800 Loss 1.3291 Accuracy 0.3263\n",
      "Epoch 6 Batch 17850 Loss 1.3293 Accuracy 0.3262\n",
      "Epoch 6 Batch 17900 Loss 1.3296 Accuracy 0.3262\n",
      "Epoch 6 Batch 17950 Loss 1.3298 Accuracy 0.3262\n",
      "Epoch 6 Batch 18000 Loss 1.3299 Accuracy 0.3261\n",
      "Epoch 6 Batch 18050 Loss 1.3302 Accuracy 0.3261\n",
      "Epoch 6 Batch 18100 Loss 1.3304 Accuracy 0.3260\n",
      "Epoch 6 Batch 18150 Loss 1.3306 Accuracy 0.3260\n",
      "Epoch 6 Batch 18200 Loss 1.3308 Accuracy 0.3260\n",
      "Epoch 6 Batch 18250 Loss 1.3311 Accuracy 0.3259\n",
      "Epoch 6 Batch 18300 Loss 1.3312 Accuracy 0.3259\n",
      "Epoch 6 Batch 18350 Loss 1.3314 Accuracy 0.3258\n",
      "Epoch 6 Batch 18400 Loss 1.3316 Accuracy 0.3258\n",
      "Epoch 6 Batch 18450 Loss 1.3319 Accuracy 0.3258\n",
      "Epoch 6 Batch 18500 Loss 1.3321 Accuracy 0.3257\n",
      "Epoch 6 Batch 18550 Loss 1.3323 Accuracy 0.3257\n",
      "Epoch 6 Batch 18600 Loss 1.3324 Accuracy 0.3257\n",
      "Epoch 6 Batch 18650 Loss 1.3325 Accuracy 0.3256\n",
      "Epoch 6 Batch 18700 Loss 1.3327 Accuracy 0.3256\n",
      "Epoch 6 Batch 18750 Loss 1.3328 Accuracy 0.3255\n",
      "Epoch 6 Batch 18800 Loss 1.3329 Accuracy 0.3255\n",
      "Epoch 6 Batch 18850 Loss 1.3331 Accuracy 0.3254\n",
      "Epoch 6 Batch 18900 Loss 1.3333 Accuracy 0.3254\n",
      "Epoch 6 Batch 18950 Loss 1.3334 Accuracy 0.3253\n",
      "Epoch 6 Batch 19000 Loss 1.3335 Accuracy 0.3253\n",
      "Epoch 6 Batch 19050 Loss 1.3336 Accuracy 0.3252\n",
      "Epoch 6 Batch 19100 Loss 1.3337 Accuracy 0.3252\n",
      "Epoch 6 Batch 19150 Loss 1.3338 Accuracy 0.3252\n",
      "Epoch 6 Batch 19200 Loss 1.3339 Accuracy 0.3251\n",
      "Epoch 6 Batch 19250 Loss 1.3339 Accuracy 0.3251\n",
      "Epoch 6 Batch 19300 Loss 1.3340 Accuracy 0.3251\n",
      "Epoch 6 Batch 19350 Loss 1.3341 Accuracy 0.3250\n",
      "Saving checkpoint for epoch 6 at ./tradutor_chek/ckpt-6\n",
      "Time taken for 1 epoch 16062.222257137299 secs\n",
      "\n",
      "Start or epoch 7\n",
      "Epoch 7 Batch 0 Loss 1.3858 Accuracy 0.3061\n",
      "Epoch 7 Batch 50 Loss 1.4251 Accuracy 0.3078\n",
      "Epoch 7 Batch 100 Loss 1.4066 Accuracy 0.3078\n",
      "Epoch 7 Batch 150 Loss 1.4035 Accuracy 0.3081\n",
      "Epoch 7 Batch 200 Loss 1.3982 Accuracy 0.3082\n",
      "Epoch 7 Batch 250 Loss 1.3985 Accuracy 0.3089\n",
      "Epoch 7 Batch 300 Loss 1.4012 Accuracy 0.3095\n",
      "Epoch 7 Batch 350 Loss 1.3988 Accuracy 0.3101\n",
      "Epoch 7 Batch 400 Loss 1.3991 Accuracy 0.3105\n",
      "Epoch 7 Batch 450 Loss 1.3977 Accuracy 0.3106\n",
      "Epoch 7 Batch 500 Loss 1.3988 Accuracy 0.3111\n",
      "Epoch 7 Batch 550 Loss 1.3981 Accuracy 0.3114\n",
      "Epoch 7 Batch 600 Loss 1.3963 Accuracy 0.3113\n",
      "Epoch 7 Batch 650 Loss 1.3992 Accuracy 0.3116\n",
      "Epoch 7 Batch 700 Loss 1.3984 Accuracy 0.3116\n",
      "Epoch 7 Batch 750 Loss 1.3988 Accuracy 0.3116\n",
      "Epoch 7 Batch 800 Loss 1.3994 Accuracy 0.3116\n",
      "Epoch 7 Batch 850 Loss 1.3983 Accuracy 0.3114\n",
      "Epoch 7 Batch 900 Loss 1.3984 Accuracy 0.3116\n",
      "Epoch 7 Batch 950 Loss 1.3976 Accuracy 0.3116\n",
      "Epoch 7 Batch 1000 Loss 1.3962 Accuracy 0.3116\n",
      "Epoch 7 Batch 1050 Loss 1.3953 Accuracy 0.3119\n",
      "Epoch 7 Batch 1100 Loss 1.3952 Accuracy 0.3120\n",
      "Epoch 7 Batch 1150 Loss 1.3947 Accuracy 0.3120\n",
      "Epoch 7 Batch 1200 Loss 1.3943 Accuracy 0.3120\n",
      "Epoch 7 Batch 1250 Loss 1.3937 Accuracy 0.3122\n",
      "Epoch 7 Batch 1300 Loss 1.3930 Accuracy 0.3123\n",
      "Epoch 7 Batch 1350 Loss 1.3930 Accuracy 0.3124\n",
      "Epoch 7 Batch 1400 Loss 1.3931 Accuracy 0.3126\n",
      "Epoch 7 Batch 1450 Loss 1.3927 Accuracy 0.3127\n",
      "Epoch 7 Batch 1500 Loss 1.3922 Accuracy 0.3126\n",
      "Epoch 7 Batch 1550 Loss 1.3922 Accuracy 0.3127\n",
      "Epoch 7 Batch 1600 Loss 1.3917 Accuracy 0.3127\n",
      "Epoch 7 Batch 1650 Loss 1.3919 Accuracy 0.3129\n",
      "Epoch 7 Batch 1700 Loss 1.3920 Accuracy 0.3130\n",
      "Epoch 7 Batch 1750 Loss 1.3914 Accuracy 0.3131\n",
      "Epoch 7 Batch 1800 Loss 1.3909 Accuracy 0.3131\n",
      "Epoch 7 Batch 1850 Loss 1.3901 Accuracy 0.3132\n",
      "Epoch 7 Batch 1900 Loss 1.3895 Accuracy 0.3132\n",
      "Epoch 7 Batch 1950 Loss 1.3886 Accuracy 0.3132\n",
      "Epoch 7 Batch 2000 Loss 1.3875 Accuracy 0.3132\n",
      "Epoch 7 Batch 2050 Loss 1.3871 Accuracy 0.3133\n",
      "Epoch 7 Batch 2100 Loss 1.3866 Accuracy 0.3133\n",
      "Epoch 7 Batch 2150 Loss 1.3859 Accuracy 0.3134\n",
      "Epoch 7 Batch 2200 Loss 1.3861 Accuracy 0.3135\n",
      "Epoch 7 Batch 2250 Loss 1.3861 Accuracy 0.3136\n",
      "Epoch 7 Batch 2300 Loss 1.3864 Accuracy 0.3136\n",
      "Epoch 7 Batch 2350 Loss 1.3859 Accuracy 0.3137\n",
      "Epoch 7 Batch 2400 Loss 1.3855 Accuracy 0.3137\n",
      "Epoch 7 Batch 2450 Loss 1.3859 Accuracy 0.3138\n",
      "Epoch 7 Batch 2500 Loss 1.3858 Accuracy 0.3139\n",
      "Epoch 7 Batch 2550 Loss 1.3863 Accuracy 0.3140\n",
      "Epoch 7 Batch 2600 Loss 1.3855 Accuracy 0.3141\n",
      "Epoch 7 Batch 2650 Loss 1.3852 Accuracy 0.3142\n",
      "Epoch 7 Batch 2700 Loss 1.3847 Accuracy 0.3144\n",
      "Epoch 7 Batch 2750 Loss 1.3843 Accuracy 0.3145\n",
      "Epoch 7 Batch 2800 Loss 1.3835 Accuracy 0.3147\n",
      "Epoch 7 Batch 2850 Loss 1.3829 Accuracy 0.3149\n",
      "Epoch 7 Batch 2900 Loss 1.3826 Accuracy 0.3151\n",
      "Epoch 7 Batch 2950 Loss 1.3817 Accuracy 0.3153\n",
      "Epoch 7 Batch 3000 Loss 1.3806 Accuracy 0.3155\n",
      "Epoch 7 Batch 3050 Loss 1.3796 Accuracy 0.3156\n",
      "Epoch 7 Batch 3100 Loss 1.3790 Accuracy 0.3158\n",
      "Epoch 7 Batch 3150 Loss 1.3784 Accuracy 0.3160\n",
      "Epoch 7 Batch 3200 Loss 1.3776 Accuracy 0.3161\n",
      "Epoch 7 Batch 3250 Loss 1.3767 Accuracy 0.3163\n",
      "Epoch 7 Batch 3300 Loss 1.3758 Accuracy 0.3163\n",
      "Epoch 7 Batch 3350 Loss 1.3752 Accuracy 0.3164\n",
      "Epoch 7 Batch 3400 Loss 1.3740 Accuracy 0.3165\n",
      "Epoch 7 Batch 3450 Loss 1.3730 Accuracy 0.3166\n",
      "Epoch 7 Batch 3500 Loss 1.3721 Accuracy 0.3167\n",
      "Epoch 7 Batch 3550 Loss 1.3712 Accuracy 0.3168\n",
      "Epoch 7 Batch 3600 Loss 1.3698 Accuracy 0.3169\n",
      "Epoch 7 Batch 3650 Loss 1.3688 Accuracy 0.3170\n",
      "Epoch 7 Batch 3700 Loss 1.3678 Accuracy 0.3171\n",
      "Epoch 7 Batch 3750 Loss 1.3666 Accuracy 0.3171\n",
      "Epoch 7 Batch 3800 Loss 1.3655 Accuracy 0.3172\n",
      "Epoch 7 Batch 3850 Loss 1.3647 Accuracy 0.3173\n",
      "Epoch 7 Batch 3900 Loss 1.3639 Accuracy 0.3175\n",
      "Epoch 7 Batch 3950 Loss 1.3629 Accuracy 0.3175\n",
      "Epoch 7 Batch 4000 Loss 1.3626 Accuracy 0.3176\n",
      "Epoch 7 Batch 4050 Loss 1.3620 Accuracy 0.3177\n",
      "Epoch 7 Batch 4100 Loss 1.3615 Accuracy 0.3179\n",
      "Epoch 7 Batch 4150 Loss 1.3610 Accuracy 0.3180\n",
      "Epoch 7 Batch 4200 Loss 1.3606 Accuracy 0.3181\n",
      "Epoch 7 Batch 4250 Loss 1.3602 Accuracy 0.3183\n",
      "Epoch 7 Batch 4300 Loss 1.3597 Accuracy 0.3184\n",
      "Epoch 7 Batch 4350 Loss 1.3591 Accuracy 0.3185\n",
      "Epoch 7 Batch 4400 Loss 1.3589 Accuracy 0.3186\n",
      "Epoch 7 Batch 4450 Loss 1.3588 Accuracy 0.3188\n",
      "Epoch 7 Batch 4500 Loss 1.3586 Accuracy 0.3189\n",
      "Epoch 7 Batch 4550 Loss 1.3581 Accuracy 0.3191\n",
      "Epoch 7 Batch 4600 Loss 1.3578 Accuracy 0.3192\n",
      "Epoch 7 Batch 4650 Loss 1.3572 Accuracy 0.3192\n",
      "Epoch 7 Batch 4700 Loss 1.3567 Accuracy 0.3193\n",
      "Epoch 7 Batch 4750 Loss 1.3562 Accuracy 0.3195\n",
      "Epoch 7 Batch 4800 Loss 1.3558 Accuracy 0.3196\n",
      "Epoch 7 Batch 4850 Loss 1.3555 Accuracy 0.3197\n",
      "Epoch 7 Batch 4900 Loss 1.3553 Accuracy 0.3198\n",
      "Epoch 7 Batch 4950 Loss 1.3552 Accuracy 0.3198\n",
      "Epoch 7 Batch 5000 Loss 1.3550 Accuracy 0.3199\n",
      "Epoch 7 Batch 5050 Loss 1.3546 Accuracy 0.3201\n",
      "Epoch 7 Batch 5100 Loss 1.3541 Accuracy 0.3202\n",
      "Epoch 7 Batch 5150 Loss 1.3538 Accuracy 0.3203\n",
      "Epoch 7 Batch 5200 Loss 1.3535 Accuracy 0.3204\n",
      "Epoch 7 Batch 5250 Loss 1.3535 Accuracy 0.3206\n",
      "Epoch 7 Batch 5300 Loss 1.3531 Accuracy 0.3207\n",
      "Epoch 7 Batch 5350 Loss 1.3529 Accuracy 0.3208\n",
      "Epoch 7 Batch 5400 Loss 1.3526 Accuracy 0.3210\n",
      "Epoch 7 Batch 5450 Loss 1.3524 Accuracy 0.3211\n",
      "Epoch 7 Batch 5500 Loss 1.3521 Accuracy 0.3213\n",
      "Epoch 7 Batch 5550 Loss 1.3518 Accuracy 0.3214\n",
      "Epoch 7 Batch 5600 Loss 1.3516 Accuracy 0.3216\n",
      "Epoch 7 Batch 5650 Loss 1.3512 Accuracy 0.3217\n",
      "Epoch 7 Batch 5700 Loss 1.3508 Accuracy 0.3218\n",
      "Epoch 7 Batch 5750 Loss 1.3504 Accuracy 0.3220\n",
      "Epoch 7 Batch 5800 Loss 1.3503 Accuracy 0.3221\n",
      "Epoch 7 Batch 5850 Loss 1.3498 Accuracy 0.3222\n",
      "Epoch 7 Batch 5900 Loss 1.3497 Accuracy 0.3224\n",
      "Epoch 7 Batch 5950 Loss 1.3493 Accuracy 0.3225\n",
      "Epoch 7 Batch 6000 Loss 1.3491 Accuracy 0.3226\n",
      "Epoch 7 Batch 6050 Loss 1.3491 Accuracy 0.3227\n",
      "Epoch 7 Batch 6100 Loss 1.3488 Accuracy 0.3228\n",
      "Epoch 7 Batch 6150 Loss 1.3485 Accuracy 0.3229\n",
      "Epoch 7 Batch 6200 Loss 1.3485 Accuracy 0.3230\n",
      "Epoch 7 Batch 6250 Loss 1.3482 Accuracy 0.3231\n",
      "Epoch 7 Batch 6300 Loss 1.3480 Accuracy 0.3232\n",
      "Epoch 7 Batch 6350 Loss 1.3481 Accuracy 0.3234\n",
      "Epoch 7 Batch 6400 Loss 1.3480 Accuracy 0.3234\n",
      "Epoch 7 Batch 6450 Loss 1.3480 Accuracy 0.3236\n",
      "Epoch 7 Batch 6500 Loss 1.3478 Accuracy 0.3236\n",
      "Epoch 7 Batch 6550 Loss 1.3476 Accuracy 0.3237\n",
      "Epoch 7 Batch 6600 Loss 1.3473 Accuracy 0.3238\n",
      "Epoch 7 Batch 6650 Loss 1.3471 Accuracy 0.3239\n",
      "Epoch 7 Batch 6700 Loss 1.3468 Accuracy 0.3240\n",
      "Epoch 7 Batch 6750 Loss 1.3466 Accuracy 0.3241\n",
      "Epoch 7 Batch 6800 Loss 1.3466 Accuracy 0.3242\n",
      "Epoch 7 Batch 6850 Loss 1.3463 Accuracy 0.3242\n",
      "Epoch 7 Batch 6900 Loss 1.3460 Accuracy 0.3243\n",
      "Epoch 7 Batch 6950 Loss 1.3456 Accuracy 0.3244\n",
      "Epoch 7 Batch 7000 Loss 1.3452 Accuracy 0.3244\n",
      "Epoch 7 Batch 7050 Loss 1.3449 Accuracy 0.3245\n",
      "Epoch 7 Batch 7100 Loss 1.3445 Accuracy 0.3246\n",
      "Epoch 7 Batch 7150 Loss 1.3443 Accuracy 0.3247\n",
      "Epoch 7 Batch 7200 Loss 1.3441 Accuracy 0.3248\n",
      "Epoch 7 Batch 7250 Loss 1.3438 Accuracy 0.3249\n",
      "Epoch 7 Batch 7300 Loss 1.3436 Accuracy 0.3250\n",
      "Epoch 7 Batch 7350 Loss 1.3434 Accuracy 0.3250\n",
      "Epoch 7 Batch 7400 Loss 1.3431 Accuracy 0.3251\n",
      "Epoch 7 Batch 7450 Loss 1.3428 Accuracy 0.3252\n",
      "Epoch 7 Batch 7500 Loss 1.3427 Accuracy 0.3253\n",
      "Epoch 7 Batch 7550 Loss 1.3423 Accuracy 0.3254\n",
      "Epoch 7 Batch 7600 Loss 1.3419 Accuracy 0.3255\n",
      "Epoch 7 Batch 7650 Loss 1.3414 Accuracy 0.3255\n",
      "Epoch 7 Batch 7700 Loss 1.3409 Accuracy 0.3255\n",
      "Epoch 7 Batch 7750 Loss 1.3403 Accuracy 0.3255\n",
      "Epoch 7 Batch 7800 Loss 1.3395 Accuracy 0.3256\n",
      "Epoch 7 Batch 7850 Loss 1.3390 Accuracy 0.3256\n",
      "Epoch 7 Batch 7900 Loss 1.3383 Accuracy 0.3256\n",
      "Epoch 7 Batch 7950 Loss 1.3378 Accuracy 0.3257\n",
      "Epoch 7 Batch 8000 Loss 1.3372 Accuracy 0.3257\n",
      "Epoch 7 Batch 8050 Loss 1.3366 Accuracy 0.3257\n",
      "Epoch 7 Batch 8100 Loss 1.3359 Accuracy 0.3257\n",
      "Epoch 7 Batch 8150 Loss 1.3353 Accuracy 0.3258\n",
      "Epoch 7 Batch 8200 Loss 1.3346 Accuracy 0.3258\n",
      "Epoch 7 Batch 8250 Loss 1.3340 Accuracy 0.3258\n",
      "Epoch 7 Batch 8300 Loss 1.3334 Accuracy 0.3258\n",
      "Epoch 7 Batch 8350 Loss 1.3330 Accuracy 0.3259\n",
      "Epoch 7 Batch 8400 Loss 1.3324 Accuracy 0.3259\n",
      "Epoch 7 Batch 8450 Loss 1.3321 Accuracy 0.3259\n",
      "Epoch 7 Batch 8500 Loss 1.3317 Accuracy 0.3260\n",
      "Epoch 7 Batch 8550 Loss 1.3312 Accuracy 0.3260\n",
      "Epoch 7 Batch 8600 Loss 1.3308 Accuracy 0.3260\n",
      "Epoch 7 Batch 8650 Loss 1.3305 Accuracy 0.3261\n",
      "Epoch 7 Batch 8700 Loss 1.3301 Accuracy 0.3261\n",
      "Epoch 7 Batch 8750 Loss 1.3297 Accuracy 0.3262\n",
      "Epoch 7 Batch 8800 Loss 1.3293 Accuracy 0.3262\n",
      "Epoch 7 Batch 8850 Loss 1.3289 Accuracy 0.3263\n",
      "Epoch 7 Batch 8900 Loss 1.3286 Accuracy 0.3263\n",
      "Epoch 7 Batch 8950 Loss 1.3282 Accuracy 0.3264\n",
      "Epoch 7 Batch 9000 Loss 1.3279 Accuracy 0.3264\n",
      "Epoch 7 Batch 9050 Loss 1.3275 Accuracy 0.3264\n",
      "Epoch 7 Batch 9100 Loss 1.3271 Accuracy 0.3265\n",
      "Epoch 7 Batch 9150 Loss 1.3268 Accuracy 0.3265\n",
      "Epoch 7 Batch 9200 Loss 1.3265 Accuracy 0.3265\n",
      "Epoch 7 Batch 9250 Loss 1.3259 Accuracy 0.3266\n",
      "Epoch 7 Batch 9300 Loss 1.3255 Accuracy 0.3266\n",
      "Epoch 7 Batch 9350 Loss 1.3251 Accuracy 0.3267\n",
      "Epoch 7 Batch 9400 Loss 1.3248 Accuracy 0.3267\n",
      "Epoch 7 Batch 9450 Loss 1.3245 Accuracy 0.3268\n",
      "Epoch 7 Batch 9500 Loss 1.3241 Accuracy 0.3269\n",
      "Epoch 7 Batch 9550 Loss 1.3238 Accuracy 0.3270\n",
      "Epoch 7 Batch 9600 Loss 1.3235 Accuracy 0.3270\n",
      "Epoch 7 Batch 9650 Loss 1.3231 Accuracy 0.3271\n",
      "Epoch 7 Batch 9700 Loss 1.3227 Accuracy 0.3272\n",
      "Epoch 7 Batch 9750 Loss 1.3222 Accuracy 0.3272\n",
      "Epoch 7 Batch 9800 Loss 1.3219 Accuracy 0.3273\n",
      "Epoch 7 Batch 9850 Loss 1.3215 Accuracy 0.3273\n",
      "Epoch 7 Batch 9900 Loss 1.3212 Accuracy 0.3274\n",
      "Epoch 7 Batch 9950 Loss 1.3209 Accuracy 0.3275\n",
      "Epoch 7 Batch 10000 Loss 1.3205 Accuracy 0.3275\n",
      "Epoch 7 Batch 10050 Loss 1.3201 Accuracy 0.3276\n",
      "Epoch 7 Batch 10100 Loss 1.3198 Accuracy 0.3276\n",
      "Epoch 7 Batch 10150 Loss 1.3194 Accuracy 0.3276\n",
      "Epoch 7 Batch 10200 Loss 1.3191 Accuracy 0.3277\n",
      "Epoch 7 Batch 10250 Loss 1.3188 Accuracy 0.3277\n",
      "Epoch 7 Batch 10300 Loss 1.3185 Accuracy 0.3278\n",
      "Epoch 7 Batch 10350 Loss 1.3182 Accuracy 0.3278\n",
      "Epoch 7 Batch 10400 Loss 1.3179 Accuracy 0.3278\n",
      "Epoch 7 Batch 10450 Loss 1.3175 Accuracy 0.3279\n",
      "Epoch 7 Batch 10500 Loss 1.3173 Accuracy 0.3279\n",
      "Epoch 7 Batch 10550 Loss 1.3170 Accuracy 0.3280\n",
      "Epoch 7 Batch 10600 Loss 1.3167 Accuracy 0.3280\n",
      "Epoch 7 Batch 10650 Loss 1.3163 Accuracy 0.3280\n",
      "Epoch 7 Batch 10700 Loss 1.3160 Accuracy 0.3281\n",
      "Epoch 7 Batch 10750 Loss 1.3157 Accuracy 0.3281\n",
      "Epoch 7 Batch 10800 Loss 1.3154 Accuracy 0.3282\n",
      "Epoch 7 Batch 10850 Loss 1.3149 Accuracy 0.3282\n",
      "Epoch 7 Batch 10900 Loss 1.3146 Accuracy 0.3283\n",
      "Epoch 7 Batch 10950 Loss 1.3142 Accuracy 0.3283\n",
      "Epoch 7 Batch 11000 Loss 1.3139 Accuracy 0.3284\n",
      "Epoch 7 Batch 11050 Loss 1.3136 Accuracy 0.3284\n",
      "Epoch 7 Batch 11100 Loss 1.3133 Accuracy 0.3285\n",
      "Epoch 7 Batch 11150 Loss 1.3130 Accuracy 0.3286\n",
      "Epoch 7 Batch 11200 Loss 1.3126 Accuracy 0.3286\n",
      "Epoch 7 Batch 11250 Loss 1.3123 Accuracy 0.3287\n",
      "Epoch 7 Batch 11300 Loss 1.3120 Accuracy 0.3287\n",
      "Epoch 7 Batch 11350 Loss 1.3118 Accuracy 0.3288\n",
      "Epoch 7 Batch 11400 Loss 1.3115 Accuracy 0.3288\n",
      "Epoch 7 Batch 11450 Loss 1.3112 Accuracy 0.3289\n",
      "Epoch 7 Batch 11500 Loss 1.3109 Accuracy 0.3290\n",
      "Epoch 7 Batch 11550 Loss 1.3105 Accuracy 0.3290\n",
      "Epoch 7 Batch 11600 Loss 1.3101 Accuracy 0.3290\n",
      "Epoch 7 Batch 11650 Loss 1.3097 Accuracy 0.3290\n",
      "Epoch 7 Batch 11700 Loss 1.3093 Accuracy 0.3291\n",
      "Epoch 7 Batch 11750 Loss 1.3090 Accuracy 0.3291\n",
      "Epoch 7 Batch 11800 Loss 1.3085 Accuracy 0.3291\n",
      "Epoch 7 Batch 11850 Loss 1.3081 Accuracy 0.3292\n",
      "Epoch 7 Batch 11900 Loss 1.3077 Accuracy 0.3292\n",
      "Epoch 7 Batch 11950 Loss 1.3074 Accuracy 0.3293\n",
      "Epoch 7 Batch 12000 Loss 1.3070 Accuracy 0.3293\n",
      "Epoch 7 Batch 12050 Loss 1.3065 Accuracy 0.3293\n",
      "Epoch 7 Batch 12100 Loss 1.3061 Accuracy 0.3294\n",
      "Epoch 7 Batch 12150 Loss 1.3056 Accuracy 0.3294\n",
      "Epoch 7 Batch 12200 Loss 1.3053 Accuracy 0.3294\n",
      "Epoch 7 Batch 12250 Loss 1.3049 Accuracy 0.3295\n",
      "Epoch 7 Batch 12300 Loss 1.3048 Accuracy 0.3295\n",
      "Epoch 7 Batch 12350 Loss 1.3044 Accuracy 0.3296\n",
      "Epoch 7 Batch 12400 Loss 1.3041 Accuracy 0.3296\n",
      "Epoch 7 Batch 12450 Loss 1.3037 Accuracy 0.3297\n",
      "Epoch 7 Batch 12500 Loss 1.3034 Accuracy 0.3297\n",
      "Epoch 7 Batch 12550 Loss 1.3031 Accuracy 0.3297\n",
      "Epoch 7 Batch 12600 Loss 1.3028 Accuracy 0.3298\n",
      "Epoch 7 Batch 12650 Loss 1.3025 Accuracy 0.3298\n",
      "Epoch 7 Batch 12700 Loss 1.3023 Accuracy 0.3299\n",
      "Epoch 7 Batch 12750 Loss 1.3020 Accuracy 0.3300\n",
      "Epoch 7 Batch 12800 Loss 1.3018 Accuracy 0.3300\n",
      "Epoch 7 Batch 12850 Loss 1.3015 Accuracy 0.3301\n",
      "Epoch 7 Batch 12900 Loss 1.3013 Accuracy 0.3302\n",
      "Epoch 7 Batch 12950 Loss 1.3009 Accuracy 0.3302\n",
      "Epoch 7 Batch 13000 Loss 1.3006 Accuracy 0.3303\n",
      "Epoch 7 Batch 13050 Loss 1.3003 Accuracy 0.3303\n",
      "Epoch 7 Batch 13100 Loss 1.3000 Accuracy 0.3304\n",
      "Epoch 7 Batch 13150 Loss 1.2996 Accuracy 0.3305\n",
      "Epoch 7 Batch 13200 Loss 1.2993 Accuracy 0.3305\n",
      "Epoch 7 Batch 13250 Loss 1.2990 Accuracy 0.3306\n",
      "Epoch 7 Batch 13300 Loss 1.2988 Accuracy 0.3306\n",
      "Epoch 7 Batch 13350 Loss 1.2985 Accuracy 0.3307\n",
      "Epoch 7 Batch 13400 Loss 1.2982 Accuracy 0.3308\n",
      "Epoch 7 Batch 13450 Loss 1.2980 Accuracy 0.3308\n",
      "Epoch 7 Batch 13500 Loss 1.2977 Accuracy 0.3309\n",
      "Epoch 7 Batch 13550 Loss 1.2974 Accuracy 0.3310\n",
      "Epoch 7 Batch 13600 Loss 1.2972 Accuracy 0.3310\n",
      "Epoch 7 Batch 13650 Loss 1.2970 Accuracy 0.3311\n",
      "Epoch 7 Batch 13700 Loss 1.2967 Accuracy 0.3312\n",
      "Epoch 7 Batch 13750 Loss 1.2965 Accuracy 0.3312\n",
      "Epoch 7 Batch 13800 Loss 1.2962 Accuracy 0.3313\n",
      "Epoch 7 Batch 13850 Loss 1.2960 Accuracy 0.3314\n",
      "Epoch 7 Batch 13900 Loss 1.2958 Accuracy 0.3315\n",
      "Epoch 7 Batch 13950 Loss 1.2955 Accuracy 0.3315\n",
      "Epoch 7 Batch 14000 Loss 1.2952 Accuracy 0.3316\n",
      "Epoch 7 Batch 14050 Loss 1.2950 Accuracy 0.3317\n",
      "Epoch 7 Batch 14100 Loss 1.2947 Accuracy 0.3317\n",
      "Epoch 7 Batch 14150 Loss 1.2945 Accuracy 0.3318\n",
      "Epoch 7 Batch 14200 Loss 1.2943 Accuracy 0.3319\n",
      "Epoch 7 Batch 14250 Loss 1.2940 Accuracy 0.3319\n",
      "Epoch 7 Batch 14300 Loss 1.2938 Accuracy 0.3320\n",
      "Epoch 7 Batch 14350 Loss 1.2935 Accuracy 0.3321\n",
      "Epoch 7 Batch 14400 Loss 1.2933 Accuracy 0.3321\n",
      "Epoch 7 Batch 14450 Loss 1.2930 Accuracy 0.3322\n",
      "Epoch 7 Batch 14500 Loss 1.2928 Accuracy 0.3323\n",
      "Epoch 7 Batch 14550 Loss 1.2925 Accuracy 0.3323\n",
      "Epoch 7 Batch 14600 Loss 1.2923 Accuracy 0.3324\n",
      "Epoch 7 Batch 14650 Loss 1.2921 Accuracy 0.3325\n",
      "Epoch 7 Batch 14700 Loss 1.2919 Accuracy 0.3325\n",
      "Epoch 7 Batch 14750 Loss 1.2918 Accuracy 0.3325\n",
      "Epoch 7 Batch 14800 Loss 1.2919 Accuracy 0.3325\n",
      "Epoch 7 Batch 14850 Loss 1.2920 Accuracy 0.3325\n",
      "Epoch 7 Batch 14900 Loss 1.2921 Accuracy 0.3325\n",
      "Epoch 7 Batch 14950 Loss 1.2922 Accuracy 0.3324\n",
      "Epoch 7 Batch 15000 Loss 1.2925 Accuracy 0.3324\n",
      "Epoch 7 Batch 15050 Loss 1.2928 Accuracy 0.3323\n",
      "Epoch 7 Batch 15100 Loss 1.2931 Accuracy 0.3323\n",
      "Epoch 7 Batch 15150 Loss 1.2935 Accuracy 0.3322\n",
      "Epoch 7 Batch 15200 Loss 1.2938 Accuracy 0.3322\n",
      "Epoch 7 Batch 15250 Loss 1.2941 Accuracy 0.3321\n",
      "Epoch 7 Batch 15300 Loss 1.2945 Accuracy 0.3321\n",
      "Epoch 7 Batch 15350 Loss 1.2949 Accuracy 0.3320\n",
      "Epoch 7 Batch 15400 Loss 1.2953 Accuracy 0.3320\n",
      "Epoch 7 Batch 15450 Loss 1.2957 Accuracy 0.3319\n",
      "Epoch 7 Batch 15500 Loss 1.2959 Accuracy 0.3319\n",
      "Epoch 7 Batch 15550 Loss 1.2963 Accuracy 0.3318\n",
      "Epoch 7 Batch 15600 Loss 1.2966 Accuracy 0.3318\n",
      "Epoch 7 Batch 15650 Loss 1.2969 Accuracy 0.3317\n",
      "Epoch 7 Batch 15700 Loss 1.2972 Accuracy 0.3316\n",
      "Epoch 7 Batch 15750 Loss 1.2976 Accuracy 0.3316\n",
      "Epoch 7 Batch 15800 Loss 1.2979 Accuracy 0.3315\n",
      "Epoch 7 Batch 15850 Loss 1.2982 Accuracy 0.3314\n",
      "Epoch 7 Batch 15900 Loss 1.2985 Accuracy 0.3314\n",
      "Epoch 7 Batch 15950 Loss 1.2988 Accuracy 0.3313\n",
      "Epoch 7 Batch 16000 Loss 1.2991 Accuracy 0.3312\n",
      "Epoch 7 Batch 16050 Loss 1.2994 Accuracy 0.3312\n",
      "Epoch 7 Batch 16100 Loss 1.2997 Accuracy 0.3311\n",
      "Epoch 7 Batch 16150 Loss 1.3000 Accuracy 0.3310\n",
      "Epoch 7 Batch 16200 Loss 1.3003 Accuracy 0.3310\n",
      "Epoch 7 Batch 16250 Loss 1.3005 Accuracy 0.3309\n",
      "Epoch 7 Batch 16300 Loss 1.3008 Accuracy 0.3309\n",
      "Epoch 7 Batch 16350 Loss 1.3011 Accuracy 0.3308\n",
      "Epoch 7 Batch 16400 Loss 1.3013 Accuracy 0.3307\n",
      "Epoch 7 Batch 16450 Loss 1.3016 Accuracy 0.3307\n",
      "Epoch 7 Batch 16500 Loss 1.3018 Accuracy 0.3306\n",
      "Epoch 7 Batch 16550 Loss 1.3020 Accuracy 0.3306\n",
      "Epoch 7 Batch 16600 Loss 1.3023 Accuracy 0.3305\n",
      "Epoch 7 Batch 16650 Loss 1.3026 Accuracy 0.3305\n",
      "Epoch 7 Batch 16700 Loss 1.3027 Accuracy 0.3305\n",
      "Epoch 7 Batch 16750 Loss 1.3030 Accuracy 0.3304\n",
      "Epoch 7 Batch 16800 Loss 1.3033 Accuracy 0.3304\n",
      "Epoch 7 Batch 16850 Loss 1.3035 Accuracy 0.3303\n",
      "Epoch 7 Batch 16900 Loss 1.3036 Accuracy 0.3303\n",
      "Epoch 7 Batch 16950 Loss 1.3039 Accuracy 0.3302\n",
      "Epoch 7 Batch 17000 Loss 1.3040 Accuracy 0.3302\n",
      "Epoch 7 Batch 17050 Loss 1.3043 Accuracy 0.3302\n",
      "Epoch 7 Batch 17100 Loss 1.3045 Accuracy 0.3301\n",
      "Epoch 7 Batch 17150 Loss 1.3046 Accuracy 0.3301\n",
      "Epoch 7 Batch 17200 Loss 1.3048 Accuracy 0.3301\n",
      "Epoch 7 Batch 17250 Loss 1.3050 Accuracy 0.3300\n",
      "Epoch 7 Batch 17300 Loss 1.3052 Accuracy 0.3300\n",
      "Epoch 7 Batch 17350 Loss 1.3053 Accuracy 0.3300\n",
      "Epoch 7 Batch 17400 Loss 1.3055 Accuracy 0.3300\n",
      "Epoch 7 Batch 17450 Loss 1.3058 Accuracy 0.3299\n",
      "Epoch 7 Batch 17500 Loss 1.3059 Accuracy 0.3299\n",
      "Epoch 7 Batch 17550 Loss 1.3061 Accuracy 0.3299\n",
      "Epoch 7 Batch 17600 Loss 1.3063 Accuracy 0.3298\n",
      "Epoch 7 Batch 17650 Loss 1.3066 Accuracy 0.3298\n",
      "Epoch 7 Batch 17700 Loss 1.3067 Accuracy 0.3297\n",
      "Epoch 7 Batch 17750 Loss 1.3069 Accuracy 0.3297\n",
      "Epoch 7 Batch 17800 Loss 1.3072 Accuracy 0.3297\n",
      "Epoch 7 Batch 17850 Loss 1.3074 Accuracy 0.3296\n",
      "Epoch 7 Batch 17900 Loss 1.3077 Accuracy 0.3296\n",
      "Epoch 7 Batch 17950 Loss 1.3079 Accuracy 0.3295\n",
      "Epoch 7 Batch 18000 Loss 1.3082 Accuracy 0.3295\n",
      "Epoch 7 Batch 18050 Loss 1.3084 Accuracy 0.3294\n",
      "Epoch 7 Batch 18100 Loss 1.3087 Accuracy 0.3294\n",
      "Epoch 7 Batch 18150 Loss 1.3089 Accuracy 0.3294\n",
      "Epoch 7 Batch 18200 Loss 1.3092 Accuracy 0.3293\n",
      "Epoch 7 Batch 18250 Loss 1.3095 Accuracy 0.3293\n",
      "Epoch 7 Batch 18300 Loss 1.3096 Accuracy 0.3293\n",
      "Epoch 7 Batch 18350 Loss 1.3098 Accuracy 0.3292\n",
      "Epoch 7 Batch 18400 Loss 1.3100 Accuracy 0.3292\n",
      "Epoch 7 Batch 18450 Loss 1.3102 Accuracy 0.3291\n",
      "Epoch 7 Batch 18500 Loss 1.3103 Accuracy 0.3291\n",
      "Epoch 7 Batch 18550 Loss 1.3105 Accuracy 0.3291\n",
      "Epoch 7 Batch 18600 Loss 1.3106 Accuracy 0.3290\n",
      "Epoch 7 Batch 18650 Loss 1.3108 Accuracy 0.3289\n",
      "Epoch 7 Batch 18700 Loss 1.3109 Accuracy 0.3289\n",
      "Epoch 7 Batch 18750 Loss 1.3111 Accuracy 0.3289\n",
      "Epoch 7 Batch 18800 Loss 1.3113 Accuracy 0.3288\n",
      "Epoch 7 Batch 18850 Loss 1.3114 Accuracy 0.3288\n",
      "Epoch 7 Batch 18900 Loss 1.3115 Accuracy 0.3287\n",
      "Epoch 7 Batch 18950 Loss 1.3116 Accuracy 0.3287\n",
      "Epoch 7 Batch 19000 Loss 1.3118 Accuracy 0.3286\n",
      "Epoch 7 Batch 19050 Loss 1.3120 Accuracy 0.3286\n",
      "Epoch 7 Batch 19100 Loss 1.3121 Accuracy 0.3285\n",
      "Epoch 7 Batch 19150 Loss 1.3122 Accuracy 0.3285\n",
      "Epoch 7 Batch 19200 Loss 1.3123 Accuracy 0.3285\n",
      "Epoch 7 Batch 19250 Loss 1.3124 Accuracy 0.3284\n",
      "Epoch 7 Batch 19300 Loss 1.3125 Accuracy 0.3284\n",
      "Epoch 7 Batch 19350 Loss 1.3125 Accuracy 0.3284\n",
      "Saving checkpoint for epoch 7 at ./tradutor_chek/ckpt-7\n",
      "Time taken for 1 epoch 15710.837144613266 secs\n",
      "\n",
      "Start or epoch 8\n",
      "Epoch 8 Batch 0 Loss 1.3341 Accuracy 0.3117\n",
      "Epoch 8 Batch 50 Loss 1.3859 Accuracy 0.3127\n",
      "Epoch 8 Batch 100 Loss 1.3842 Accuracy 0.3118\n",
      "Epoch 8 Batch 150 Loss 1.3842 Accuracy 0.3109\n",
      "Epoch 8 Batch 200 Loss 1.3782 Accuracy 0.3113\n",
      "Epoch 8 Batch 250 Loss 1.3758 Accuracy 0.3117\n",
      "Epoch 8 Batch 300 Loss 1.3781 Accuracy 0.3122\n",
      "Epoch 8 Batch 350 Loss 1.3781 Accuracy 0.3131\n",
      "Epoch 8 Batch 400 Loss 1.3754 Accuracy 0.3135\n",
      "Epoch 8 Batch 450 Loss 1.3742 Accuracy 0.3138\n",
      "Epoch 8 Batch 500 Loss 1.3762 Accuracy 0.3143\n",
      "Epoch 8 Batch 550 Loss 1.3768 Accuracy 0.3141\n",
      "Epoch 8 Batch 600 Loss 1.3775 Accuracy 0.3144\n",
      "Epoch 8 Batch 650 Loss 1.3789 Accuracy 0.3146\n",
      "Epoch 8 Batch 700 Loss 1.3792 Accuracy 0.3146\n",
      "Epoch 8 Batch 750 Loss 1.3810 Accuracy 0.3145\n",
      "Epoch 8 Batch 800 Loss 1.3807 Accuracy 0.3144\n",
      "Epoch 8 Batch 850 Loss 1.3798 Accuracy 0.3143\n",
      "Epoch 8 Batch 900 Loss 1.3797 Accuracy 0.3143\n",
      "Epoch 8 Batch 950 Loss 1.3780 Accuracy 0.3143\n",
      "Epoch 8 Batch 1000 Loss 1.3771 Accuracy 0.3143\n",
      "Epoch 8 Batch 1050 Loss 1.3766 Accuracy 0.3145\n",
      "Epoch 8 Batch 1100 Loss 1.3755 Accuracy 0.3146\n",
      "Epoch 8 Batch 1150 Loss 1.3754 Accuracy 0.3148\n",
      "Epoch 8 Batch 1200 Loss 1.3749 Accuracy 0.3148\n",
      "Epoch 8 Batch 1250 Loss 1.3748 Accuracy 0.3150\n",
      "Epoch 8 Batch 1300 Loss 1.3744 Accuracy 0.3152\n",
      "Epoch 8 Batch 1350 Loss 1.3738 Accuracy 0.3152\n",
      "Epoch 8 Batch 1400 Loss 1.3737 Accuracy 0.3152\n",
      "Epoch 8 Batch 1450 Loss 1.3731 Accuracy 0.3153\n",
      "Epoch 8 Batch 1500 Loss 1.3722 Accuracy 0.3153\n",
      "Epoch 8 Batch 1550 Loss 1.3727 Accuracy 0.3153\n",
      "Epoch 8 Batch 1600 Loss 1.3721 Accuracy 0.3154\n",
      "Epoch 8 Batch 1650 Loss 1.3727 Accuracy 0.3156\n",
      "Epoch 8 Batch 1700 Loss 1.3718 Accuracy 0.3157\n",
      "Epoch 8 Batch 1750 Loss 1.3718 Accuracy 0.3158\n",
      "Epoch 8 Batch 1800 Loss 1.3712 Accuracy 0.3159\n",
      "Epoch 8 Batch 1850 Loss 1.3714 Accuracy 0.3159\n",
      "Epoch 8 Batch 1900 Loss 1.3710 Accuracy 0.3160\n",
      "Epoch 8 Batch 1950 Loss 1.3699 Accuracy 0.3160\n",
      "Epoch 8 Batch 2000 Loss 1.3696 Accuracy 0.3161\n",
      "Epoch 8 Batch 2050 Loss 1.3688 Accuracy 0.3161\n",
      "Epoch 8 Batch 2100 Loss 1.3679 Accuracy 0.3161\n",
      "Epoch 8 Batch 2150 Loss 1.3672 Accuracy 0.3161\n",
      "Epoch 8 Batch 2200 Loss 1.3672 Accuracy 0.3161\n",
      "Epoch 8 Batch 2250 Loss 1.3670 Accuracy 0.3162\n",
      "Epoch 8 Batch 2300 Loss 1.3672 Accuracy 0.3163\n",
      "Epoch 8 Batch 2350 Loss 1.3670 Accuracy 0.3163\n",
      "Epoch 8 Batch 2400 Loss 1.3669 Accuracy 0.3163\n",
      "Epoch 8 Batch 2450 Loss 1.3674 Accuracy 0.3164\n",
      "Epoch 8 Batch 2500 Loss 1.3672 Accuracy 0.3165\n",
      "Epoch 8 Batch 2550 Loss 1.3670 Accuracy 0.3166\n",
      "Epoch 8 Batch 2600 Loss 1.3667 Accuracy 0.3167\n",
      "Epoch 8 Batch 2650 Loss 1.3668 Accuracy 0.3169\n",
      "Epoch 8 Batch 2700 Loss 1.3668 Accuracy 0.3171\n",
      "Epoch 8 Batch 2750 Loss 1.3660 Accuracy 0.3173\n",
      "Epoch 8 Batch 2800 Loss 1.3657 Accuracy 0.3175\n",
      "Epoch 8 Batch 2850 Loss 1.3651 Accuracy 0.3177\n",
      "Epoch 8 Batch 2900 Loss 1.3647 Accuracy 0.3179\n",
      "Epoch 8 Batch 2950 Loss 1.3638 Accuracy 0.3181\n",
      "Epoch 8 Batch 3000 Loss 1.3633 Accuracy 0.3183\n",
      "Epoch 8 Batch 3050 Loss 1.3623 Accuracy 0.3185\n",
      "Epoch 8 Batch 3100 Loss 1.3617 Accuracy 0.3186\n",
      "Epoch 8 Batch 3150 Loss 1.3607 Accuracy 0.3187\n",
      "Epoch 8 Batch 3200 Loss 1.3599 Accuracy 0.3189\n",
      "Epoch 8 Batch 3250 Loss 1.3588 Accuracy 0.3190\n",
      "Epoch 8 Batch 3300 Loss 1.3577 Accuracy 0.3191\n",
      "Epoch 8 Batch 3350 Loss 1.3567 Accuracy 0.3193\n",
      "Epoch 8 Batch 3400 Loss 1.3558 Accuracy 0.3193\n",
      "Epoch 8 Batch 3450 Loss 1.3546 Accuracy 0.3194\n",
      "Epoch 8 Batch 3500 Loss 1.3534 Accuracy 0.3195\n",
      "Epoch 8 Batch 3550 Loss 1.3525 Accuracy 0.3197\n",
      "Epoch 8 Batch 3600 Loss 1.3513 Accuracy 0.3197\n",
      "Epoch 8 Batch 3650 Loss 1.3501 Accuracy 0.3198\n",
      "Epoch 8 Batch 3700 Loss 1.3491 Accuracy 0.3199\n",
      "Epoch 8 Batch 3750 Loss 1.3480 Accuracy 0.3201\n",
      "Epoch 8 Batch 3800 Loss 1.3469 Accuracy 0.3201\n",
      "Epoch 8 Batch 3850 Loss 1.3459 Accuracy 0.3203\n",
      "Epoch 8 Batch 3900 Loss 1.3449 Accuracy 0.3203\n",
      "Epoch 8 Batch 3950 Loss 1.3446 Accuracy 0.3205\n",
      "Epoch 8 Batch 4000 Loss 1.3439 Accuracy 0.3205\n",
      "Epoch 8 Batch 4050 Loss 1.3435 Accuracy 0.3206\n",
      "Epoch 8 Batch 4100 Loss 1.3430 Accuracy 0.3208\n",
      "Epoch 8 Batch 4150 Loss 1.3427 Accuracy 0.3209\n",
      "Epoch 8 Batch 4200 Loss 1.3424 Accuracy 0.3211\n",
      "Epoch 8 Batch 4250 Loss 1.3423 Accuracy 0.3212\n",
      "Epoch 8 Batch 4300 Loss 1.3418 Accuracy 0.3213\n",
      "Epoch 8 Batch 4350 Loss 1.3413 Accuracy 0.3215\n",
      "Epoch 8 Batch 4400 Loss 1.3410 Accuracy 0.3216\n",
      "Epoch 8 Batch 4450 Loss 1.3408 Accuracy 0.3217\n",
      "Epoch 8 Batch 4500 Loss 1.3404 Accuracy 0.3219\n",
      "Epoch 8 Batch 4550 Loss 1.3399 Accuracy 0.3220\n",
      "Epoch 8 Batch 4600 Loss 1.3392 Accuracy 0.3222\n",
      "Epoch 8 Batch 4650 Loss 1.3387 Accuracy 0.3223\n",
      "Epoch 8 Batch 4700 Loss 1.3381 Accuracy 0.3224\n",
      "Epoch 8 Batch 4750 Loss 1.3375 Accuracy 0.3224\n",
      "Epoch 8 Batch 4800 Loss 1.3371 Accuracy 0.3225\n",
      "Epoch 8 Batch 4850 Loss 1.3368 Accuracy 0.3226\n",
      "Epoch 8 Batch 4900 Loss 1.3366 Accuracy 0.3227\n",
      "Epoch 8 Batch 4950 Loss 1.3363 Accuracy 0.3228\n",
      "Epoch 8 Batch 5000 Loss 1.3359 Accuracy 0.3229\n",
      "Epoch 8 Batch 5050 Loss 1.3357 Accuracy 0.3230\n",
      "Epoch 8 Batch 5100 Loss 1.3353 Accuracy 0.3231\n",
      "Epoch 8 Batch 5150 Loss 1.3351 Accuracy 0.3232\n",
      "Epoch 8 Batch 5200 Loss 1.3347 Accuracy 0.3233\n",
      "Epoch 8 Batch 5250 Loss 1.3345 Accuracy 0.3235\n",
      "Epoch 8 Batch 5300 Loss 1.3341 Accuracy 0.3236\n",
      "Epoch 8 Batch 5350 Loss 1.3338 Accuracy 0.3238\n",
      "Epoch 8 Batch 5400 Loss 1.3335 Accuracy 0.3240\n",
      "Epoch 8 Batch 5450 Loss 1.3331 Accuracy 0.3241\n",
      "Epoch 8 Batch 5500 Loss 1.3326 Accuracy 0.3243\n",
      "Epoch 8 Batch 5550 Loss 1.3324 Accuracy 0.3244\n",
      "Epoch 8 Batch 5600 Loss 1.3320 Accuracy 0.3246\n",
      "Epoch 8 Batch 5650 Loss 1.3317 Accuracy 0.3247\n",
      "Epoch 8 Batch 5700 Loss 1.3312 Accuracy 0.3249\n",
      "Epoch 8 Batch 5750 Loss 1.3310 Accuracy 0.3250\n",
      "Epoch 8 Batch 5800 Loss 1.3309 Accuracy 0.3252\n",
      "Epoch 8 Batch 5850 Loss 1.3308 Accuracy 0.3253\n",
      "Epoch 8 Batch 5900 Loss 1.3305 Accuracy 0.3254\n",
      "Epoch 8 Batch 5950 Loss 1.3302 Accuracy 0.3255\n",
      "Epoch 8 Batch 6000 Loss 1.3300 Accuracy 0.3256\n",
      "Epoch 8 Batch 6050 Loss 1.3299 Accuracy 0.3257\n",
      "Epoch 8 Batch 6100 Loss 1.3298 Accuracy 0.3258\n",
      "Epoch 8 Batch 6150 Loss 1.3295 Accuracy 0.3259\n",
      "Epoch 8 Batch 6200 Loss 1.3293 Accuracy 0.3260\n",
      "Epoch 8 Batch 6250 Loss 1.3292 Accuracy 0.3261\n",
      "Epoch 8 Batch 6300 Loss 1.3291 Accuracy 0.3262\n",
      "Epoch 8 Batch 6350 Loss 1.3289 Accuracy 0.3263\n",
      "Epoch 8 Batch 6400 Loss 1.3286 Accuracy 0.3264\n",
      "Epoch 8 Batch 6450 Loss 1.3285 Accuracy 0.3265\n",
      "Epoch 8 Batch 6500 Loss 1.3284 Accuracy 0.3267\n",
      "Epoch 8 Batch 6550 Loss 1.3284 Accuracy 0.3268\n",
      "Epoch 8 Batch 6600 Loss 1.3282 Accuracy 0.3269\n",
      "Epoch 8 Batch 6650 Loss 1.3281 Accuracy 0.3270\n",
      "Epoch 8 Batch 6700 Loss 1.3278 Accuracy 0.3271\n",
      "Epoch 8 Batch 6750 Loss 1.3274 Accuracy 0.3271\n",
      "Epoch 8 Batch 6800 Loss 1.3272 Accuracy 0.3272\n",
      "Epoch 8 Batch 6850 Loss 1.3270 Accuracy 0.3273\n",
      "Epoch 8 Batch 6900 Loss 1.3268 Accuracy 0.3274\n",
      "Epoch 8 Batch 6950 Loss 1.3266 Accuracy 0.3275\n",
      "Epoch 8 Batch 7000 Loss 1.3262 Accuracy 0.3276\n",
      "Epoch 8 Batch 7050 Loss 1.3259 Accuracy 0.3276\n",
      "Epoch 8 Batch 7100 Loss 1.3256 Accuracy 0.3277\n",
      "Epoch 8 Batch 7150 Loss 1.3254 Accuracy 0.3278\n",
      "Epoch 8 Batch 7200 Loss 1.3252 Accuracy 0.3279\n",
      "Epoch 8 Batch 7250 Loss 1.3250 Accuracy 0.3280\n",
      "Epoch 8 Batch 7300 Loss 1.3248 Accuracy 0.3281\n",
      "Epoch 8 Batch 7350 Loss 1.3243 Accuracy 0.3281\n",
      "Epoch 8 Batch 7400 Loss 1.3240 Accuracy 0.3282\n",
      "Epoch 8 Batch 7450 Loss 1.3237 Accuracy 0.3283\n",
      "Epoch 8 Batch 7500 Loss 1.3232 Accuracy 0.3283\n",
      "Epoch 8 Batch 7550 Loss 1.3227 Accuracy 0.3284\n",
      "Epoch 8 Batch 7600 Loss 1.3223 Accuracy 0.3285\n",
      "Epoch 8 Batch 7650 Loss 1.3216 Accuracy 0.3285\n",
      "Epoch 8 Batch 7700 Loss 1.3212 Accuracy 0.3286\n",
      "Epoch 8 Batch 7750 Loss 1.3207 Accuracy 0.3286\n",
      "Epoch 8 Batch 7800 Loss 1.3202 Accuracy 0.3287\n",
      "Epoch 8 Batch 7850 Loss 1.3198 Accuracy 0.3287\n",
      "Epoch 8 Batch 7900 Loss 1.3192 Accuracy 0.3288\n",
      "Epoch 8 Batch 7950 Loss 1.3186 Accuracy 0.3288\n",
      "Epoch 8 Batch 8000 Loss 1.3179 Accuracy 0.3288\n",
      "Epoch 8 Batch 8050 Loss 1.3174 Accuracy 0.3288\n",
      "Epoch 8 Batch 8100 Loss 1.3168 Accuracy 0.3289\n",
      "Epoch 8 Batch 8150 Loss 1.3163 Accuracy 0.3289\n",
      "Epoch 8 Batch 8200 Loss 1.3158 Accuracy 0.3289\n",
      "Epoch 8 Batch 8250 Loss 1.3151 Accuracy 0.3289\n",
      "Epoch 8 Batch 8300 Loss 1.3145 Accuracy 0.3290\n",
      "Epoch 8 Batch 8350 Loss 1.3141 Accuracy 0.3290\n",
      "Epoch 8 Batch 8400 Loss 1.3135 Accuracy 0.3290\n",
      "Epoch 8 Batch 8450 Loss 1.3130 Accuracy 0.3290\n",
      "Epoch 8 Batch 8500 Loss 1.3127 Accuracy 0.3291\n",
      "Epoch 8 Batch 8550 Loss 1.3124 Accuracy 0.3291\n",
      "Epoch 8 Batch 8600 Loss 1.3119 Accuracy 0.3292\n",
      "Epoch 8 Batch 8650 Loss 1.3115 Accuracy 0.3292\n",
      "Epoch 8 Batch 8700 Loss 1.3110 Accuracy 0.3292\n",
      "Epoch 8 Batch 8750 Loss 1.3105 Accuracy 0.3293\n",
      "Epoch 8 Batch 8800 Loss 1.3101 Accuracy 0.3293\n",
      "Epoch 8 Batch 8850 Loss 1.3098 Accuracy 0.3293\n",
      "Epoch 8 Batch 8900 Loss 1.3095 Accuracy 0.3294\n",
      "Epoch 8 Batch 8950 Loss 1.3092 Accuracy 0.3294\n",
      "Epoch 8 Batch 9000 Loss 1.3090 Accuracy 0.3295\n",
      "Epoch 8 Batch 9050 Loss 1.3086 Accuracy 0.3295\n",
      "Epoch 8 Batch 9100 Loss 1.3082 Accuracy 0.3296\n",
      "Epoch 8 Batch 9150 Loss 1.3079 Accuracy 0.3296\n",
      "Epoch 8 Batch 9200 Loss 1.3075 Accuracy 0.3297\n",
      "Epoch 8 Batch 9250 Loss 1.3071 Accuracy 0.3297\n",
      "Epoch 8 Batch 9300 Loss 1.3066 Accuracy 0.3297\n",
      "Epoch 8 Batch 9350 Loss 1.3061 Accuracy 0.3298\n",
      "Epoch 8 Batch 9400 Loss 1.3057 Accuracy 0.3298\n",
      "Epoch 8 Batch 9450 Loss 1.3054 Accuracy 0.3299\n",
      "Epoch 8 Batch 9500 Loss 1.3051 Accuracy 0.3300\n",
      "Epoch 8 Batch 9550 Loss 1.3048 Accuracy 0.3300\n",
      "Epoch 8 Batch 9600 Loss 1.3044 Accuracy 0.3301\n",
      "Epoch 8 Batch 9650 Loss 1.3039 Accuracy 0.3301\n",
      "Epoch 8 Batch 9700 Loss 1.3035 Accuracy 0.3302\n",
      "Epoch 8 Batch 9750 Loss 1.3031 Accuracy 0.3303\n",
      "Epoch 8 Batch 9800 Loss 1.3029 Accuracy 0.3304\n",
      "Epoch 8 Batch 9850 Loss 1.3025 Accuracy 0.3305\n",
      "Epoch 8 Batch 9900 Loss 1.3022 Accuracy 0.3305\n",
      "Epoch 8 Batch 9950 Loss 1.3018 Accuracy 0.3306\n",
      "Epoch 8 Batch 10000 Loss 1.3014 Accuracy 0.3306\n",
      "Epoch 8 Batch 10050 Loss 1.3010 Accuracy 0.3307\n",
      "Epoch 8 Batch 10100 Loss 1.3007 Accuracy 0.3307\n",
      "Epoch 8 Batch 10150 Loss 1.3004 Accuracy 0.3308\n",
      "Epoch 8 Batch 10200 Loss 1.3002 Accuracy 0.3308\n",
      "Epoch 8 Batch 10250 Loss 1.2999 Accuracy 0.3309\n",
      "Epoch 8 Batch 10300 Loss 1.2997 Accuracy 0.3309\n",
      "Epoch 8 Batch 10350 Loss 1.2993 Accuracy 0.3309\n",
      "Epoch 8 Batch 10400 Loss 1.2991 Accuracy 0.3310\n",
      "Epoch 8 Batch 10450 Loss 1.2986 Accuracy 0.3310\n",
      "Epoch 8 Batch 10500 Loss 1.2982 Accuracy 0.3310\n",
      "Epoch 8 Batch 10550 Loss 1.2979 Accuracy 0.3311\n",
      "Epoch 8 Batch 10600 Loss 1.2976 Accuracy 0.3311\n",
      "Epoch 8 Batch 10650 Loss 1.2972 Accuracy 0.3311\n",
      "Epoch 8 Batch 10700 Loss 1.2969 Accuracy 0.3312\n",
      "Epoch 8 Batch 10750 Loss 1.2965 Accuracy 0.3312\n",
      "Epoch 8 Batch 10800 Loss 1.2963 Accuracy 0.3313\n",
      "Epoch 8 Batch 10850 Loss 1.2960 Accuracy 0.3313\n",
      "Epoch 8 Batch 10900 Loss 1.2956 Accuracy 0.3313\n",
      "Epoch 8 Batch 10950 Loss 1.2953 Accuracy 0.3314\n",
      "Epoch 8 Batch 11000 Loss 1.2950 Accuracy 0.3315\n",
      "Epoch 8 Batch 11050 Loss 1.2946 Accuracy 0.3315\n",
      "Epoch 8 Batch 11100 Loss 1.2943 Accuracy 0.3316\n",
      "Epoch 8 Batch 11150 Loss 1.2941 Accuracy 0.3316\n",
      "Epoch 8 Batch 11200 Loss 1.2939 Accuracy 0.3317\n",
      "Epoch 8 Batch 11250 Loss 1.2936 Accuracy 0.3318\n",
      "Epoch 8 Batch 11300 Loss 1.2932 Accuracy 0.3318\n",
      "Epoch 8 Batch 11350 Loss 1.2928 Accuracy 0.3319\n",
      "Epoch 8 Batch 11400 Loss 1.2925 Accuracy 0.3319\n",
      "Epoch 8 Batch 11450 Loss 1.2921 Accuracy 0.3319\n",
      "Epoch 8 Batch 11500 Loss 1.2918 Accuracy 0.3320\n",
      "Epoch 8 Batch 11550 Loss 1.2914 Accuracy 0.3320\n",
      "Epoch 8 Batch 11600 Loss 1.2910 Accuracy 0.3321\n",
      "Epoch 8 Batch 11650 Loss 1.2906 Accuracy 0.3321\n",
      "Epoch 8 Batch 11700 Loss 1.2901 Accuracy 0.3321\n",
      "Epoch 8 Batch 11750 Loss 1.2898 Accuracy 0.3321\n",
      "Epoch 8 Batch 11800 Loss 1.2893 Accuracy 0.3322\n",
      "Epoch 8 Batch 11850 Loss 1.2889 Accuracy 0.3322\n",
      "Epoch 8 Batch 11900 Loss 1.2886 Accuracy 0.3323\n",
      "Epoch 8 Batch 11950 Loss 1.2883 Accuracy 0.3323\n",
      "Epoch 8 Batch 12000 Loss 1.2879 Accuracy 0.3324\n",
      "Epoch 8 Batch 12050 Loss 1.2875 Accuracy 0.3324\n",
      "Epoch 8 Batch 12100 Loss 1.2871 Accuracy 0.3324\n",
      "Epoch 8 Batch 12150 Loss 1.2866 Accuracy 0.3325\n",
      "Epoch 8 Batch 12200 Loss 1.2863 Accuracy 0.3325\n",
      "Epoch 8 Batch 12250 Loss 1.2860 Accuracy 0.3325\n",
      "Epoch 8 Batch 12300 Loss 1.2857 Accuracy 0.3326\n",
      "Epoch 8 Batch 12350 Loss 1.2853 Accuracy 0.3326\n",
      "Epoch 8 Batch 12400 Loss 1.2850 Accuracy 0.3327\n",
      "Epoch 8 Batch 12450 Loss 1.2847 Accuracy 0.3327\n",
      "Epoch 8 Batch 12500 Loss 1.2845 Accuracy 0.3327\n",
      "Epoch 8 Batch 12550 Loss 1.2841 Accuracy 0.3328\n",
      "Epoch 8 Batch 12600 Loss 1.2838 Accuracy 0.3328\n",
      "Epoch 8 Batch 12650 Loss 1.2834 Accuracy 0.3329\n",
      "Epoch 8 Batch 12700 Loss 1.2831 Accuracy 0.3329\n",
      "Epoch 8 Batch 12750 Loss 1.2828 Accuracy 0.3330\n",
      "Epoch 8 Batch 12800 Loss 1.2826 Accuracy 0.3331\n",
      "Epoch 8 Batch 12850 Loss 1.2823 Accuracy 0.3331\n",
      "Epoch 8 Batch 12900 Loss 1.2820 Accuracy 0.3332\n",
      "Epoch 8 Batch 12950 Loss 1.2817 Accuracy 0.3333\n",
      "Epoch 8 Batch 13000 Loss 1.2814 Accuracy 0.3333\n",
      "Epoch 8 Batch 13050 Loss 1.2812 Accuracy 0.3334\n",
      "Epoch 8 Batch 13100 Loss 1.2810 Accuracy 0.3334\n",
      "Epoch 8 Batch 13150 Loss 1.2807 Accuracy 0.3335\n",
      "Epoch 8 Batch 13200 Loss 1.2805 Accuracy 0.3336\n",
      "Epoch 8 Batch 13250 Loss 1.2802 Accuracy 0.3336\n",
      "Epoch 8 Batch 13300 Loss 1.2800 Accuracy 0.3337\n",
      "Epoch 8 Batch 13350 Loss 1.2798 Accuracy 0.3338\n",
      "Epoch 8 Batch 13400 Loss 1.2794 Accuracy 0.3338\n",
      "Epoch 8 Batch 13450 Loss 1.2791 Accuracy 0.3339\n",
      "Epoch 8 Batch 13500 Loss 1.2789 Accuracy 0.3340\n",
      "Epoch 8 Batch 13550 Loss 1.2786 Accuracy 0.3340\n",
      "Epoch 8 Batch 13600 Loss 1.2784 Accuracy 0.3341\n",
      "Epoch 8 Batch 13650 Loss 1.2782 Accuracy 0.3342\n",
      "Epoch 8 Batch 13700 Loss 1.2779 Accuracy 0.3342\n",
      "Epoch 8 Batch 13750 Loss 1.2776 Accuracy 0.3343\n",
      "Epoch 8 Batch 13800 Loss 1.2774 Accuracy 0.3344\n",
      "Epoch 8 Batch 13850 Loss 1.2771 Accuracy 0.3344\n",
      "Epoch 8 Batch 13900 Loss 1.2769 Accuracy 0.3345\n",
      "Epoch 8 Batch 13950 Loss 1.2766 Accuracy 0.3346\n",
      "Epoch 8 Batch 14000 Loss 1.2763 Accuracy 0.3346\n",
      "Epoch 8 Batch 14050 Loss 1.2761 Accuracy 0.3347\n",
      "Epoch 8 Batch 14100 Loss 1.2758 Accuracy 0.3347\n",
      "Epoch 8 Batch 14150 Loss 1.2756 Accuracy 0.3348\n",
      "Epoch 8 Batch 14200 Loss 1.2754 Accuracy 0.3349\n",
      "Epoch 8 Batch 14250 Loss 1.2751 Accuracy 0.3350\n",
      "Epoch 8 Batch 14300 Loss 1.2749 Accuracy 0.3350\n",
      "Epoch 8 Batch 14350 Loss 1.2747 Accuracy 0.3351\n",
      "Epoch 8 Batch 14400 Loss 1.2744 Accuracy 0.3351\n",
      "Epoch 8 Batch 14450 Loss 1.2742 Accuracy 0.3352\n",
      "Epoch 8 Batch 14500 Loss 1.2739 Accuracy 0.3353\n",
      "Epoch 8 Batch 14550 Loss 1.2737 Accuracy 0.3353\n",
      "Epoch 8 Batch 14600 Loss 1.2735 Accuracy 0.3354\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_14880\\349417505.py\u001B[0m in \u001B[0;36m?\u001B[1;34m()\u001B[0m\n\u001B[0;32m     11\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mGradientTape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtape\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m             \u001B[0mpredictions\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtransformer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menc_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdec_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m             \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mloss_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdec_outputs_real\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpredictions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 15\u001B[1;33m         \u001B[0mgradients\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtape\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgradient\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransformer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrainable_variables\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     16\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapply_gradients\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgradients\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransformer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrainable_variables\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m         \u001B[0mtrain_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001B[0m\n\u001B[0;32m   1109\u001B[0m               output_gradients))\n\u001B[0;32m   1110\u001B[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001B[0;32m   1111\u001B[0m                           for x in output_gradients]\n\u001B[0;32m   1112\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1113\u001B[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001B[0m\u001B[0;32m   1114\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_tape\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1115\u001B[0m         \u001B[0mflat_targets\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1116\u001B[0m         \u001B[0mflat_sources\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001B[0m\n\u001B[0;32m     63\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     64\u001B[0m     raise ValueError(\n\u001B[0;32m     65\u001B[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001B[0;32m     66\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 67\u001B[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001B[0m\u001B[0;32m     68\u001B[0m       \u001B[0mtape\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_tape\u001B[0m\u001B[1;33m,\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m       \u001B[0mtarget\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     70\u001B[0m       \u001B[0msources\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001B[0m\n\u001B[0;32m    156\u001B[0m     \u001B[0mgradient_name_scope\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"gradient_tape/\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    157\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mforward_pass_name_scope\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    158\u001B[0m       \u001B[0mgradient_name_scope\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mforward_pass_name_scope\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\"/\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    159\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname_scope\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgradient_name_scope\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 160\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mgrad_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmock_op\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0mout_grads\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    161\u001B[0m   \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    162\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mgrad_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmock_op\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0mout_grads\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(op, grad)\u001B[0m\n\u001B[0;32m   1365\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mgen_math_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmul\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgen_math_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmul\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgrad\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1366\u001B[0m   \u001B[1;32massert\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbase_dtype\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbase_dtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\" vs. \"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1367\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1368\u001B[0m   (sx, rx, must_reduce_x), (sy, ry, must_reduce_y) = (\n\u001B[1;32m-> 1369\u001B[1;33m       SmartBroadcastGradientArgs(x, y, grad))\n\u001B[0m\u001B[0;32m   1370\u001B[0m   \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmath_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconj\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1371\u001B[0m   \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmath_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconj\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1372\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mskip_input_indices\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mskip_input_indices\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(x, y, grad)\u001B[0m\n\u001B[0;32m     87\u001B[0m   if context.executing_eagerly() or not (\n\u001B[0;32m     88\u001B[0m       \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     89\u001B[0m       and isinstance(grad, ops.Tensor)):\n\u001B[0;32m     90\u001B[0m     \u001B[0msx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0marray_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 91\u001B[1;33m     \u001B[0msy\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0marray_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     92\u001B[0m     \u001B[0mrx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mry\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgen_array_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbroadcast_gradient_args\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msy\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     93\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0msx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0msy\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mry\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     94\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    153\u001B[0m       \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    154\u001B[0m     \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 155\u001B[1;33m       \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1180\u001B[0m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdispatch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mop_dispatch_handler\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1181\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mresult\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mOpDispatcher\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mNOT_SUPPORTED\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1182\u001B[0m           \u001B[1;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1183\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1184\u001B[1;33m           \u001B[1;32mraise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(input, name, out_type)\u001B[0m\n\u001B[0;32m    652\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    653\u001B[0m   \u001B[0mReturns\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    654\u001B[0m     \u001B[0mA\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;31m`\u001B[0m \u001B[0mof\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mout_type\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    655\u001B[0m   \"\"\"\n\u001B[1;32m--> 656\u001B[1;33m   \u001B[1;32mreturn\u001B[0m \u001B[0mshape_internal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout_type\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mout_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(input, name, optimize, out_type)\u001B[0m\n\u001B[0;32m    693\u001B[0m                 input_shape)\n\u001B[0;32m    694\u001B[0m           \u001B[1;32mreturn\u001B[0m \u001B[0mconstant\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_shape\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_list\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    695\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mout_type\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    696\u001B[0m         \u001B[0mout_type\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdtypes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mint32\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 697\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mgen_array_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mout_type\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mout_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(input, out_type, name)\u001B[0m\n\u001B[0;32m   9357\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   9358\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   9359\u001B[0m       \u001B[0m_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraise_from_not_ok_status\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   9360\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_FallbackException\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 9361\u001B[1;33m       \u001B[1;32mpass\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   9362\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   9363\u001B[0m       return shape_eager_fallback(\n\u001B[0;32m   9364\u001B[0m           input, out_type=out_type, name=name, ctx=_ctx)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print('Start or epoch {}'.format(epoch + 1))\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
    "        dec_inputs = targets[:, :-1]\n",
    "        dec_outputs_real = targets[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
    "            loss = loss_function(dec_outputs_real, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(dec_outputs_real, predictions)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print('Saving checkpoint for epoch {} at {}'.format(epoch + 1, ckpt_save_path))\n",
    "    print('Time taken for 1 epoch {} secs\\n'.format(time.time() - start)) "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-08-06T02:36:59.868824400Z"
    }
   },
   "id": "126edd5dbe6e569f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "avaliação"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70fb226b1a8cfe29"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "text = 'you are smart'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T12:48:24.036868600Z",
     "start_time": "2023-08-07T12:48:24.028756400Z"
    }
   },
   "id": "7daae6b2a34d1ba3"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "def evaluete(inp_sentence):\n",
    "    inp_sentence = [vocab_size_en - 2] + tokenize_en.encode(inp_sentence) + [vocab_size_en-1]\n",
    "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
    "    output = tf.expand_dims([vocab_size_pt - 2], axis=0)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        predicoes = transformer(enc_input, output, False)\n",
    "        predict = predicoes[:, -1:, :]\n",
    "        predict_id = tf.cast(tf.argmax(predict, axis=-1), tf.int32)\n",
    "        \n",
    "        if predict_id == vocab_size_pt -1:\n",
    "            return tf.squeeze(output, axis=0)\n",
    "        \n",
    "        output = tf.concat([output, predict_id], axis=1)\n",
    "        \n",
    "    return tf.squeeze(output, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T12:48:24.093160700Z",
     "start_time": "2023-08-07T12:48:24.045385800Z"
    }
   },
   "id": "8633d041e1568525"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "def tradutor(sentence):\n",
    "    output = evaluete(sentence).numpy()\n",
    "    predict_sentence = tokenize_pt.decode([i for i in output if i < vocab_size_pt - 2])\n",
    "    print(f'Input: {sentence}')\n",
    "    print(f'Predição: {predict_sentence}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T12:48:24.094163400Z",
     "start_time": "2023-08-07T12:48:24.068586600Z"
    }
   },
   "id": "736dfc72395d45f5"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: this is a powerful tool\n",
      "Predição: Também é um instrumento poderoso.\n"
     ]
    }
   ],
   "source": [
    "tradutor('this is a powerful tool')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T12:48:27.019660800Z",
     "start_time": "2023-08-07T12:48:24.084627Z"
    }
   },
   "id": "b8c23fed5e3d261c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
